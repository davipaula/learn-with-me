WEBVTT
Kind: captions
Language: en

00:00:12.802 --> 00:00:17.373
I'm here, because I've spent
far too many nights lying awake,

00:00:17.397 --> 00:00:19.883
worrying and wondering
who wins in the end.

00:00:19.907 --> 00:00:22.357
Is it humans or is it robots?

00:00:22.918 --> 00:00:24.641
You see, as a technology strategist,

00:00:24.665 --> 00:00:26.569
my job involves behavior change:

00:00:26.593 --> 00:00:29.689
understanding why and how
people adopt new technologies.

00:00:30.101 --> 00:00:32.474
And that means I'm really frustrated

00:00:32.498 --> 00:00:35.958
that I know I won't live to see
how this all ends up.

00:00:36.577 --> 00:00:39.850
And in fact, if the youngest person
watching this is 14

00:00:39.874 --> 00:00:42.687
and the oldest, a robust 99,

00:00:42.711 --> 00:00:43.863
then together,

00:00:43.887 --> 00:00:48.061
our collective consciousnesses
span just 185 years.

00:00:48.593 --> 00:00:51.959
That is a myopic pinprick of time

00:00:51.983 --> 00:00:55.475
when you think of the evolution
and the story of life on this planet.

00:00:55.912 --> 00:00:57.737
Turns out we're all in the cheap seats

00:00:57.761 --> 00:01:00.753
and none of us will live to see
how it all pans out.

00:01:00.777 --> 00:01:03.125
So at my company,
we wanted a way around this.

00:01:03.149 --> 00:01:05.887
We wanted to see if there was
a way to cantilever out,

00:01:05.911 --> 00:01:08.919
beyond our fixed temporal vantage point,

00:01:08.943 --> 00:01:11.213
to get a sense of how it all shakes up.

00:01:11.237 --> 00:01:15.214
And to do this, we conducted a study
amongst 1,200 Americans

00:01:15.238 --> 00:01:17.388
representative of the US census,

00:01:17.412 --> 00:01:19.936
in which we asked a battery
of attitudinal questions

00:01:19.960 --> 00:01:21.761
around robotics and AI

00:01:21.785 --> 00:01:25.158
and also captured behavioral ones
around technology adoption.

00:01:26.011 --> 00:01:27.296
We had a big study

00:01:27.320 --> 00:01:30.297
so that we could analyze
differences in gender and generations,

00:01:30.321 --> 00:01:32.289
between religious and political beliefs,

00:01:32.313 --> 00:01:35.082
even job function and personality trait.

00:01:35.106 --> 00:01:38.609
It is a fascinating,
time-bound time capsule

00:01:38.633 --> 00:01:40.299
of our human frailty

00:01:40.323 --> 00:01:42.909
in this predawn of the robotic era.

00:01:43.220 --> 00:01:45.498
And I have five minutes
to tell you about it.

00:01:46.260 --> 00:01:48.839
The first thing you should know
is that we brainstormed

00:01:48.863 --> 00:01:55.090
a list of scenarios
of current and potential AI robotics.

00:01:55.693 --> 00:01:58.235
They ran the spectrum from the mundane,

00:01:58.259 --> 00:02:00.235
so, a robot house cleaner, anyone?

00:02:00.259 --> 00:02:01.561
Through to the mischievous,

00:02:01.585 --> 00:02:04.680
the idea of a robot pet sitter,
or maybe a robot lawyer,

00:02:04.704 --> 00:02:06.237
or maybe a sex partner.

00:02:06.553 --> 00:02:09.474
Through to the downright macabre,
the idea of being a cyborg,

00:02:09.498 --> 00:02:10.791
blending human and robot,

00:02:10.815 --> 00:02:14.696
or uploading your brain
so it could live on after your death.

00:02:15.045 --> 00:02:18.942
And we plotted people's comfort levels
with these various scenarios.

00:02:18.966 --> 00:02:20.696
There were actually 31 in the study,

00:02:20.720 --> 00:02:23.934
but for ease, I'm going to show you
just a few of them here.

00:02:24.355 --> 00:02:27.188
The first thing you'll notice,
of course, is the sea of red.

00:02:27.212 --> 00:02:30.021
America is very uncomfortable
with this stuff.

00:02:30.901 --> 00:02:33.736
That's why we call it
the discomfort index,

00:02:33.760 --> 00:02:35.180
not the comfort index.

00:02:35.204 --> 00:02:38.895
There were only two things
the majority of America is OK with.

00:02:38.919 --> 00:02:41.793
And that's the idea
of a robot AI house cleaner

00:02:41.817 --> 00:02:43.809
and a robot AI package deliverer,

00:02:43.833 --> 00:02:46.459
so Dyson and Amazon, you guys should talk.

00:02:46.848 --> 00:02:48.396
There's an opportunity there.

00:02:48.420 --> 00:02:52.380
It seems we're ready to off-load
our chores to our robot friends.

00:02:52.404 --> 00:02:55.419
We're kind of definitely on the fence
when it comes to services,

00:02:55.443 --> 00:02:59.213
so robot AI lawyer
or a financial adviser, maybe.

00:02:59.237 --> 00:03:02.022
But we're firmly closed
to the idea of robot care,

00:03:02.046 --> 00:03:04.485
whether it be a nurse,
a doctor, child care.

00:03:04.509 --> 00:03:05.862
So from this, you'd go,

00:03:05.886 --> 00:03:07.339
"It's OK, Lucy, you know what?

00:03:07.363 --> 00:03:10.418
Go back to sleep, stop worrying,
the humans win in the end."

00:03:10.442 --> 00:03:11.664
But actually not so fast.

00:03:11.688 --> 00:03:13.410
If you look at my data very closely,

00:03:13.434 --> 00:03:15.680
you can see we're more
vulnerable than we think.

00:03:15.704 --> 00:03:16.989
AI has a branding problem.

00:03:17.013 --> 00:03:19.170
So of those folks who said

00:03:19.194 --> 00:03:22.337
that they would absolutely reject
the idea of a personal assistant,

00:03:22.361 --> 00:03:25.247
45 percent of them had, in fact,
one in their pockets,

00:03:25.271 --> 00:03:28.874
in terms of a device
with Alexa, Google or Siri.

00:03:28.898 --> 00:03:31.970
One in five of those who were against
the idea of AI matchmaking

00:03:31.994 --> 00:03:34.649
had of course, you guessed it,
done online dating.

00:03:34.673 --> 00:03:36.962
And 80 percent of those of us
who refuse the idea

00:03:36.986 --> 00:03:39.442
of boarding an autonomous plane
with a pilot backup

00:03:39.466 --> 00:03:42.021
had in fact, just like me
to get here to Vancouver,

00:03:42.045 --> 00:03:43.212
flown commercial.

00:03:43.236 --> 00:03:45.291
Lest you think everybody
was scared, though,

00:03:45.315 --> 00:03:47.450
here are the marvelous folk in the middle.

00:03:47.474 --> 00:03:48.721
These are the neutrals.

00:03:48.745 --> 00:03:50.387
These are people for whom you say,

00:03:50.411 --> 00:03:51.680
"OK, robot friend,"

00:03:51.704 --> 00:03:54.639
and they're like,
"Hm, robot friend. Maybe."

00:03:54.663 --> 00:03:56.284
Or, "AI pet,"

00:03:56.308 --> 00:03:58.370
and they go, "Never say never."

00:03:58.843 --> 00:04:01.509
And as any decent
political operative knows,

00:04:01.533 --> 00:04:04.255
flipping the ambivalent middle
can change the game.

00:04:04.644 --> 00:04:06.963
Another reason I know
we're vulnerable is men --

00:04:06.987 --> 00:04:09.563
I'm sorry, but men,
you are twice as likely than women

00:04:09.587 --> 00:04:12.998
to believe that getting
into an autonomous car is a good idea,

00:04:13.022 --> 00:04:15.839
that uploading your brain
for posterity is fun,

00:04:15.863 --> 00:04:19.585
and two and a half times more likely
to believe that becoming a cyborg is cool,

00:04:19.609 --> 00:04:21.288
and for this, I blame Hollywood.

00:04:21.312 --> 00:04:22.608
(Laughter)

00:04:22.632 --> 00:04:25.228
And this is where I want you
to look around the theater

00:04:25.252 --> 00:04:28.609
and know that one in four men
are OK with the idea of sex with a robot.

00:04:28.633 --> 00:04:31.770
That goes up to 44 percent
of millennial men

00:04:31.794 --> 00:04:33.388
compared to just one in 10 women,

00:04:33.412 --> 00:04:36.801
which I think puts a whole new twist
on the complaint of mechanical sex.

00:04:36.825 --> 00:04:38.793
(Laughter)

00:04:38.817 --> 00:04:41.246
Even more astounding
than that though, to be honest,

00:04:41.270 --> 00:04:42.840
is this behavioral difference.

00:04:42.864 --> 00:04:47.077
So here we have people who have
a device with a voice assistant in it,

00:04:47.101 --> 00:04:49.847
so a smart speaker,
a home hub or a smart phone,

00:04:49.871 --> 00:04:51.593
versus those who don't.

00:04:51.617 --> 00:04:53.117
And you can see from this graph

00:04:53.141 --> 00:04:56.864
that the Trojan horse
is already in our living room.

00:04:56.888 --> 00:04:58.975
And as these devices proliferate

00:04:58.999 --> 00:05:02.216
and our collective defenses soften,

00:05:02.240 --> 00:05:04.237
we all see how it can end.

00:05:04.261 --> 00:05:06.650
In fact, this may be as good
a time as any to admit

00:05:06.674 --> 00:05:09.080
I did take my Alexa Dot
on vacation with me.

00:05:10.192 --> 00:05:12.343
Final finding I have time for
is generational.

00:05:12.367 --> 00:05:14.922
So look at the difference
just three generations make.

00:05:14.946 --> 00:05:18.101
This is the leap from silent
to boomer to millennial.

00:05:18.125 --> 00:05:21.370
And what's more fascinating than this
is if you extrapolate this out,

00:05:21.394 --> 00:05:22.609
the same rate of change,

00:05:22.633 --> 00:05:23.823
just the same pace,

00:05:23.847 --> 00:05:26.682
not the accelerated one
I actually believe will be the case,

00:05:26.706 --> 00:05:27.881
the same pace,

00:05:27.905 --> 00:05:29.602
then it is eight generations away

00:05:29.626 --> 00:05:31.966
when we hear every single American

00:05:31.990 --> 00:05:35.371
thinking the majority
of these things here are normal.

00:05:35.395 --> 00:05:38.434
So the year 2222 is an astounding place

00:05:38.458 --> 00:05:40.752
where everything here is mainstream.

00:05:40.776 --> 00:05:42.721
And lest you needed any more convincing,

00:05:42.745 --> 00:05:45.553
here is the generation's
"excitement level with AI."

00:05:45.577 --> 00:05:48.917
So not surprisingly,
the youngest of us are more excited.

00:05:49.331 --> 00:05:53.307
But, and possibly the most
paradoxical finding of my career,

00:05:53.331 --> 00:05:56.331
when I asked these people my 3am question,

00:05:56.355 --> 00:05:57.822
"Who wins in the end?"

00:05:58.188 --> 00:05:59.338
Guess what.

00:05:59.736 --> 00:06:01.974
The more excited you are
about AI and robotics,

00:06:01.998 --> 00:06:04.683
the more likely you are to say
it's the robots.

00:06:05.347 --> 00:06:08.879
And I don't think we need a neural net
running pattern-recognition software

00:06:08.903 --> 00:06:10.608
to see where this is all headed.

00:06:10.632 --> 00:06:13.337
We are the proverbial frogs
in boiling water.

00:06:13.361 --> 00:06:18.514
So if the robots at TED2222
are watching this for posterity,

00:06:18.538 --> 00:06:21.514
could you send a cyborg, dig me up
and tell me if I was right?

00:06:21.538 --> 00:06:22.703
(Laughter)

00:06:22.727 --> 00:06:23.904
Thank you.

00:06:23.928 --> 00:06:25.547
(Applause)

