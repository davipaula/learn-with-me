WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:07.000
Translator: Ivana Korom
Reviewer: Joanna Pietrulewicz

00:00:12.875 --> 00:00:16.643
How many decisions
have been made about you today,

00:00:16.667 --> 00:00:19.268
or this week or this year,

00:00:19.292 --> 00:00:21.250
by artificial intelligence?

00:00:22.958 --> 00:00:24.643
I build AI for a living

00:00:24.667 --> 00:00:27.684
so, full disclosure, I'm kind of a nerd.

00:00:27.708 --> 00:00:30.101
And because I'm kind of a nerd,

00:00:30.125 --> 00:00:32.476
wherever some new news story comes out

00:00:32.500 --> 00:00:35.934
about artificial intelligence
stealing all our jobs,

00:00:35.958 --> 00:00:40.143
or robots getting citizenship
of an actual country,

00:00:40.167 --> 00:00:43.309
I'm the person my friends
and followers message

00:00:43.333 --> 00:00:44.875
freaking out about the future.

00:00:45.833 --> 00:00:47.934
We see this everywhere.

00:00:47.958 --> 00:00:52.851
This media panic that
our robot overlords are taking over.

00:00:52.875 --> 00:00:54.792
We could blame Hollywood for that.

00:00:56.125 --> 00:01:00.250
But in reality, that's not the problem
we should be focusing on.

00:01:01.250 --> 00:01:04.893
There is a more pressing danger,
a bigger risk with AI,

00:01:04.917 --> 00:01:06.500
that we need to fix first.

00:01:07.417 --> 00:01:09.726
So we are back to this question:

00:01:09.750 --> 00:01:14.458
How many decisions
have been made about you today by AI?

00:01:15.792 --> 00:01:17.768
And how many of these

00:01:17.792 --> 00:01:22.292
were based on your gender,
your race or your background?

00:01:24.500 --> 00:01:27.268
Algorithms are being used all the time

00:01:27.292 --> 00:01:31.125
to make decisions about who we are
and what we want.

00:01:32.208 --> 00:01:35.851
Some of the women in this room
will know what I'm talking about

00:01:35.875 --> 00:01:39.643
if you've been made to sit through
those pregnancy test adverts on YouTube

00:01:39.667 --> 00:01:41.726
like 1,000 times.

00:01:41.750 --> 00:01:44.601
Or you've scrolled past adverts
of fertility clinics

00:01:44.625 --> 00:01:46.667
on your Facebook feed.

00:01:47.625 --> 00:01:50.018
Or in my case, Indian marriage bureaus.

00:01:50.042 --> 00:01:51.309
(Laughter)

00:01:51.333 --> 00:01:54.309
But AI isn't just being used
to make decisions

00:01:54.333 --> 00:01:56.934
about what products we want to buy

00:01:56.958 --> 00:01:59.458
or which show we want to binge watch next.

00:02:01.042 --> 00:02:06.226
I wonder how you'd feel about someone
who thought things like this:

00:02:06.250 --> 00:02:08.184
"A black or Latino person

00:02:08.208 --> 00:02:12.333
is less likely than a white person
to pay off their loan on time."

00:02:13.542 --> 00:02:16.351
"A person called John
makes a better programmer

00:02:16.375 --> 00:02:18.042
than a person called Mary."

00:02:19.250 --> 00:02:24.333
"A black man is more likely to be
a repeat offender than a white man."

00:02:26.958 --> 00:02:28.226
You're probably thinking,

00:02:28.250 --> 00:02:32.000
"Wow, that sounds like a pretty sexist,
racist person," right?

00:02:33.000 --> 00:02:37.851
These are some real decisions
that AI has made very recently,

00:02:37.875 --> 00:02:40.809
based on the biases
it has learned from us,

00:02:40.833 --> 00:02:42.083
from the humans.

00:02:43.750 --> 00:02:48.559
AI is being used to help decide
whether or not you get that job interview;

00:02:48.583 --> 00:02:50.976
how much you pay for your car insurance;

00:02:51.000 --> 00:02:52.893
how good your credit score is;

00:02:52.917 --> 00:02:56.042
and even what rating you get
in your annual performance review.

00:02:57.083 --> 00:03:00.226
But these decisions
are all being filtered through

00:03:00.250 --> 00:03:06.125
its assumptions about our identity,
our race, our gender, our age.

00:03:08.250 --> 00:03:10.518
How is that happening?

00:03:10.542 --> 00:03:14.059
Now, imagine an AI is helping
a hiring manager

00:03:14.083 --> 00:03:16.934
find the next tech leader in the company.

00:03:16.958 --> 00:03:20.059
So far, the manager
has been hiring mostly men.

00:03:20.083 --> 00:03:24.833
So the AI learns men are more likely
to be programmers than women.

00:03:25.542 --> 00:03:28.434
And it's a very short leap from there to:

00:03:28.458 --> 00:03:30.500
men make better programmers than women.

00:03:31.417 --> 00:03:35.143
We have reinforced
our own bias into the AI.

00:03:35.167 --> 00:03:38.792
And now, it's screening out
female candidates.

00:03:40.917 --> 00:03:43.934
Hang on, if a human
hiring manager did that,

00:03:43.958 --> 00:03:46.309
we'd be outraged, we wouldn't allow it.

00:03:46.333 --> 00:03:49.809
This kind of gender
discrimination is not OK.

00:03:49.833 --> 00:03:54.351
And yet somehow,
AI has become above the law,

00:03:54.375 --> 00:03:56.458
because a machine made the decision.

00:03:57.833 --> 00:03:59.351
That's not it.

00:03:59.375 --> 00:04:04.250
We are also reinforcing our bias
in how we interact with AI.

00:04:04.917 --> 00:04:10.893
How often do you use a voice assistant
like Siri, Alexa or even Cortana?

00:04:10.917 --> 00:04:13.476
They all have two things in common:

00:04:13.500 --> 00:04:16.601
one, they can never get my name right,

00:04:16.625 --> 00:04:19.292
and second, they are all female.

00:04:20.417 --> 00:04:23.184
They are designed to be
our obedient servants,

00:04:23.208 --> 00:04:26.458
turning your lights on and off,
ordering your shopping.

00:04:27.125 --> 00:04:30.434
You get male AIs too,
but they tend to be more high-powered,

00:04:30.458 --> 00:04:33.517
like IBM Watson,
making business decisions,

00:04:33.541 --> 00:04:37.333
Salesforce Einstein
or ROSS, the robot lawyer.

00:04:38.208 --> 00:04:42.268
So poor robots, even they suffer
from sexism in the workplace.

00:04:42.292 --> 00:04:43.417
(Laughter)

00:04:44.542 --> 00:04:47.393
Think about how these two things combine

00:04:47.417 --> 00:04:52.726
and affect a kid growing up
in today's world around AI.

00:04:52.750 --> 00:04:55.684
So they're doing some research
for a school project

00:04:55.708 --> 00:04:58.726
and they Google images of CEO.

00:04:58.750 --> 00:05:01.643
The algorithm shows them
results of mostly men.

00:05:01.667 --> 00:05:04.226
And now, they Google personal assistant.

00:05:04.250 --> 00:05:07.684
As you can guess,
it shows them mostly females.

00:05:07.708 --> 00:05:11.309
And then they want to put on some music,
and maybe order some food,

00:05:11.333 --> 00:05:17.917
and now, they are barking orders
at an obedient female voice assistant.

00:05:19.542 --> 00:05:24.851
Some of our brightest minds
are creating this technology today.

00:05:24.875 --> 00:05:29.059
Technology that they could have created
in any way they wanted.

00:05:29.083 --> 00:05:34.768
And yet, they have chosen to create it
in the style of 1950s "Mad Man" secretary.

00:05:34.792 --> 00:05:36.292
Yay!

00:05:36.958 --> 00:05:38.268
But OK, don't worry,

00:05:38.292 --> 00:05:40.351
this is not going to end
with me telling you

00:05:40.375 --> 00:05:43.852
that we are all heading towards
sexist, racist machines running the world.

00:05:44.792 --> 00:05:50.583
The good news about AI
is that it is entirely within our control.

00:05:51.333 --> 00:05:55.333
We get to teach the right values,
the right ethics to AI.

00:05:56.167 --> 00:05:58.351
So there are three things we can do.

00:05:58.375 --> 00:06:01.726
One, we can be aware of our own biases

00:06:01.750 --> 00:06:04.476
and the bias in machines around us.

00:06:04.500 --> 00:06:09.018
Two, we can make sure that diverse teams
are building this technology.

00:06:09.042 --> 00:06:13.958
And three, we have to give it
diverse experiences to learn from.

00:06:14.875 --> 00:06:18.184
I can talk about the first two
from personal experience.

00:06:18.208 --> 00:06:19.643
When you work in technology

00:06:19.667 --> 00:06:23.059
and you don't look like
a Mark Zuckerberg or Elon Musk,

00:06:23.083 --> 00:06:26.833
your life is a little bit difficult,
your ability gets questioned.

00:06:27.875 --> 00:06:29.268
Here's just one example.

00:06:29.292 --> 00:06:33.018
Like most developers,
I often join online tech forums

00:06:33.042 --> 00:06:36.268
and share my knowledge to help others.

00:06:36.292 --> 00:06:37.601
And I've found,

00:06:37.625 --> 00:06:41.601
when I log on as myself,
with my own photo, my own name,

00:06:41.625 --> 00:06:46.226
I tend to get questions
or comments like this:

00:06:46.250 --> 00:06:49.250
"What makes you think
you're qualified to talk about AI?"

00:06:50.458 --> 00:06:53.934
"What makes you think
you know about machine learning?"

00:06:53.958 --> 00:06:57.393
So, as you do, I made a new profile,

00:06:57.417 --> 00:07:02.268
and this time, instead of my own picture,
I chose a cat with a jet pack on it.

00:07:02.292 --> 00:07:04.750
And I chose a name
that did not reveal my gender.

00:07:05.917 --> 00:07:08.643
You can probably guess
where this is going, right?

00:07:08.667 --> 00:07:15.059
So, this time, I didn't get any of those
patronizing comments about my ability

00:07:15.083 --> 00:07:18.417
and I was able to actually
get some work done.

00:07:19.500 --> 00:07:21.351
And it sucks, guys.

00:07:21.375 --> 00:07:23.851
I've been building robots since I was 15,

00:07:23.875 --> 00:07:26.143
I have a few degrees in computer science,

00:07:26.167 --> 00:07:28.601
and yet, I had to hide my gender

00:07:28.625 --> 00:07:30.875
in order for my work
to be taken seriously.

00:07:31.875 --> 00:07:33.768
So, what's going on here?

00:07:33.792 --> 00:07:37.000
Are men just better
at technology than women?

00:07:37.917 --> 00:07:39.476
Another study found

00:07:39.500 --> 00:07:44.434
that when women coders on one platform
hid their gender, like myself,

00:07:44.458 --> 00:07:47.708
their code was accepted
four percent more than men.

00:07:48.542 --> 00:07:51.458
So this is not about the talent.

00:07:51.958 --> 00:07:54.851
This is about an elitism in AI

00:07:54.875 --> 00:07:57.667
that says a programmer
needs to look like a certain person.

00:07:59.375 --> 00:08:02.476
What we really need to do
to make AI better

00:08:02.500 --> 00:08:05.542
is bring people
from all kinds of backgrounds.

00:08:06.542 --> 00:08:09.101
We need people who can
write and tell stories

00:08:09.125 --> 00:08:11.292
to help us create personalities of AI.

00:08:12.208 --> 00:08:14.250
We need people who can solve problems.

00:08:15.125 --> 00:08:18.893
We need people
who face different challenges

00:08:18.917 --> 00:08:24.268
and we need people who can tell us
what are the real issues that need fixing

00:08:24.292 --> 00:08:27.333
and help us find ways
that technology can actually fix it.

00:08:29.833 --> 00:08:33.559
Because, when people
from diverse backgrounds come together,

00:08:33.583 --> 00:08:35.726
when we build things in the right way,

00:08:35.750 --> 00:08:37.792
the possibilities are limitless.

00:08:38.750 --> 00:08:42.059
And that's what I want to end
by talking to you about.

00:08:42.083 --> 00:08:46.308
Less racist robots, less machines
that are going to take our jobs --

00:08:46.332 --> 00:08:49.457
and more about what technology
can actually achieve.

00:08:50.292 --> 00:08:53.726
So, yes, some of the energy
in the world of AI,

00:08:53.750 --> 00:08:55.143
in the world of technology

00:08:55.167 --> 00:08:59.434
is going to be about
what ads you see on your stream.

00:08:59.458 --> 00:09:04.667
But a lot of it is going towards
making the world so much better.

00:09:05.500 --> 00:09:09.268
Think about a pregnant woman
in the Democratic Republic of Congo,

00:09:09.292 --> 00:09:13.476
who has to walk 17 hours
to her nearest rural prenatal clinic

00:09:13.500 --> 00:09:15.351
to get a checkup.

00:09:15.375 --> 00:09:18.292
What if she could get diagnosis
on her phone, instead?

00:09:19.750 --> 00:09:21.559
Or think about what AI could do

00:09:21.583 --> 00:09:24.309
for those one in three women
in South Africa

00:09:24.333 --> 00:09:26.458
who face domestic violence.

00:09:27.083 --> 00:09:29.809
If it wasn't safe to talk out loud,

00:09:29.833 --> 00:09:32.309
they could get an AI service
to raise alarm,

00:09:32.333 --> 00:09:34.792
get financial and legal advice.

00:09:35.958 --> 00:09:40.976
These are all real examples of projects
that people, including myself,

00:09:41.000 --> 00:09:43.500
are working on right now, using AI.

00:09:45.542 --> 00:09:49.143
So, I'm sure in the next couple of days
there will be yet another news story

00:09:49.167 --> 00:09:51.851
about the existential risk,

00:09:51.875 --> 00:09:54.309
robots taking over
and coming for your jobs.

00:09:54.333 --> 00:09:55.351
(Laughter)

00:09:55.375 --> 00:09:57.684
And when something like that happens,

00:09:57.708 --> 00:10:01.309
I know I'll get the same messages
worrying about the future.

00:10:01.333 --> 00:10:05.000
But I feel incredibly positive
about this technology.

00:10:07.458 --> 00:10:13.417
This is our chance to remake the world
into a much more equal place.

00:10:14.458 --> 00:10:18.458
But to do that, we need to build it
the right way from the get go.

00:10:19.667 --> 00:10:24.750
We need people of different genders,
races, sexualities and backgrounds.

00:10:26.458 --> 00:10:28.934
We need women to be the makers

00:10:28.958 --> 00:10:31.958
and not just the machines
who do the makers' bidding.

00:10:33.875 --> 00:10:37.643
We need to think very carefully
what we teach machines,

00:10:37.667 --> 00:10:39.309
what data we give them,

00:10:39.333 --> 00:10:42.458
so they don't just repeat
our own past mistakes.

00:10:44.125 --> 00:10:47.667
So I hope I leave you
thinking about two things.

00:10:48.542 --> 00:10:53.101
First, I hope you leave
thinking about bias today.

00:10:53.125 --> 00:10:56.309
And that the next time
you scroll past an advert

00:10:56.333 --> 00:10:59.143
that assumes you are interested
in fertility clinics

00:10:59.167 --> 00:11:02.018
or online betting websites,

00:11:02.042 --> 00:11:04.059
that you think and remember

00:11:04.083 --> 00:11:08.708
that the same technology is assuming
that a black man will reoffend.

00:11:09.833 --> 00:11:14.000
Or that a woman is more likely
to be a personal assistant than a CEO.

00:11:14.958 --> 00:11:18.667
And I hope that reminds you
that we need to do something about it.

00:11:20.917 --> 00:11:22.768
And second,

00:11:22.792 --> 00:11:24.684
I hope you think about the fact

00:11:24.708 --> 00:11:26.684
that you don't need to look a certain way

00:11:26.708 --> 00:11:30.559
or have a certain background
in engineering or technology

00:11:30.583 --> 00:11:31.851
to create AI,

00:11:31.875 --> 00:11:34.750
which is going to be
a phenomenal force for our future.

00:11:36.166 --> 00:11:38.309
You don't need to look
like a Mark Zuckerberg,

00:11:38.333 --> 00:11:39.583
you can look like me.

00:11:41.250 --> 00:11:44.143
And it is up to all of us in this room

00:11:44.167 --> 00:11:46.893
to convince the governments
and the corporations

00:11:46.917 --> 00:11:49.809
to build AI technology for everyone,

00:11:49.833 --> 00:11:52.226
including the edge cases.

00:11:52.250 --> 00:11:54.309
And for us all to get education

00:11:54.333 --> 00:11:56.708
about this phenomenal
technology in the future.

00:11:58.167 --> 00:12:00.184
Because if we do that,

00:12:00.208 --> 00:12:05.101
then we've only just scratched the surface
of what we can achieve with AI.

00:12:05.125 --> 00:12:06.393
Thank you.

00:12:06.417 --> 00:12:09.125
(Applause)

