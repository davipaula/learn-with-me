WEBVTT
Kind: captions
Language: en

00:00:12.857 --> 00:00:14.336
This is a graph

00:00:14.360 --> 00:00:18.019
that represents the economic history
of human civilization.

00:00:18.043 --> 00:00:20.443
[World GDP per capita
over the last 200,000 years]

00:00:23.757 --> 00:00:25.760
There's not much going on, is there.

00:00:26.751 --> 00:00:29.098
For the vast majority of human history,

00:00:29.835 --> 00:00:33.883
pretty much everyone lived
on the equivalent of one dollar per day,

00:00:33.907 --> 00:00:35.193
and not much changed.

00:00:36.757 --> 00:00:39.534
But then, something
extraordinary happened:

00:00:40.677 --> 00:00:43.488
the Scientific and Industrial Revolutions.

00:00:43.512 --> 00:00:46.297
And the basically flat graph you just saw

00:00:46.321 --> 00:00:48.996
transforms into this.

00:00:50.612 --> 00:00:55.247
What this graph means is that,
in terms of power to change the world,

00:00:55.271 --> 00:00:58.709
we live in an unprecedented time
in human history,

00:00:58.733 --> 00:01:02.677
and I believe our ethical understanding
hasn't yet caught up with this fact.

00:01:03.716 --> 00:01:05.700
The Scientific and Industrial Revolutions

00:01:05.724 --> 00:01:08.633
transformed both
our understanding of the world

00:01:08.657 --> 00:01:10.326
and our ability to alter it.

00:01:11.505 --> 00:01:15.172
What we need is an ethical revolution

00:01:15.196 --> 00:01:16.744
so that we can work out

00:01:16.768 --> 00:01:19.920
how do we use this tremendous
bounty of resources

00:01:19.944 --> 00:01:21.339
to improve the world.

00:01:22.249 --> 00:01:23.840
For the last 10 years,

00:01:23.864 --> 00:01:27.697
my colleagues and I have developed
a philosophy and research program

00:01:27.721 --> 00:01:29.556
that we call effective altruism.

00:01:30.366 --> 00:01:33.961
It tries to respond
to these radical changes in our world,

00:01:33.985 --> 00:01:38.461
uses evidence and careful reasoning
to try to answer this question:

00:01:40.173 --> 00:01:42.451
How can we do the most good?

00:01:44.265 --> 00:01:47.486
Now, there are many issues
you've got to address

00:01:47.510 --> 00:01:49.773
if you want to tackle this problem:

00:01:49.797 --> 00:01:51.828
whether to do good through your charity

00:01:51.852 --> 00:01:54.004
or your career
or your political engagement,

00:01:54.028 --> 00:01:56.423
what programs to focus on,
who to work with.

00:01:57.624 --> 00:01:59.100
But what I want to talk about

00:01:59.124 --> 00:02:01.996
is what I think is the most
fundamental problem.

00:02:02.020 --> 00:02:04.713
Of all the many problems
that the world faces,

00:02:05.962 --> 00:02:08.621
which should we be focused
on trying to solve first?

00:02:10.668 --> 00:02:14.136
Now, I'm going to give you a framework
for thinking about this question,

00:02:14.160 --> 00:02:16.096
and the framework is very simple.

00:02:16.842 --> 00:02:18.541
A problem's higher priority,

00:02:19.416 --> 00:02:23.479
the bigger, the more easily solvable
and the more neglected it is.

00:02:24.694 --> 00:02:26.336
Bigger is better,

00:02:26.360 --> 00:02:29.201
because we've got more to gain
if we do solve the problem.

00:02:30.221 --> 00:02:31.790
More easily solvable is better

00:02:31.814 --> 00:02:34.638
because I can solve the problem
with less time or money.

00:02:35.737 --> 00:02:37.800
And most subtly,

00:02:38.681 --> 00:02:41.530
more neglected is better,
because of diminishing returns.

00:02:42.285 --> 00:02:45.999
The more resources that have already been
invested into solving a problem,

00:02:46.023 --> 00:02:48.928
the harder it will be
to make additional progress.

00:02:50.560 --> 00:02:54.619
Now, the key thing that I want
to leave with you is this framework,

00:02:54.643 --> 00:02:56.627
so that you can think for yourself

00:02:56.651 --> 00:02:58.972
what are the highest global priorities.

00:02:59.954 --> 00:03:02.646
But I and others
in the effective altruism community

00:03:02.670 --> 00:03:08.549
have converged on three moral issues
that we believe are unusually important,

00:03:08.573 --> 00:03:10.755
score unusually well in this framework.

00:03:11.151 --> 00:03:13.964
First is global health.

00:03:13.988 --> 00:03:16.399
This is supersolvable.

00:03:16.423 --> 00:03:19.820
We have an amazing track record
in global health.

00:03:19.844 --> 00:03:25.264
Rates of death from measles,
malaria, diarrheal disease

00:03:25.288 --> 00:03:27.534
are down by over 70 percent.

00:03:29.534 --> 00:03:32.323
And in 1980, we eradicated smallpox.

00:03:33.815 --> 00:03:37.482
I estimate we thereby saved
over 60 million lives.

00:03:37.506 --> 00:03:40.570
That's more lives saved
than if we'd achieved world peace

00:03:40.594 --> 00:03:42.291
in that same time period.

00:03:43.893 --> 00:03:46.218
On our current best estimates,

00:03:46.242 --> 00:03:50.370
we can save a life by distributing
long-lasting insecticide-treated bed nets

00:03:50.394 --> 00:03:52.297
for just a few thousand dollars.

00:03:52.911 --> 00:03:54.578
This is an amazing opportunity.

00:03:55.594 --> 00:03:58.109
The second big priority
is factory farming.

00:03:58.681 --> 00:04:00.244
This is superneglected.

00:04:00.768 --> 00:04:04.911
There are 50 billion land animals
used every year for food,

00:04:05.625 --> 00:04:08.173
and the vast majority of them
are factory farmed,

00:04:08.197 --> 00:04:10.577
living in conditions
of horrific suffering.

00:04:10.601 --> 00:04:13.752
They're probably among
the worst-off creatures on this planet,

00:04:13.776 --> 00:04:16.634
and in many cases, we could
significantly improve their lives

00:04:16.658 --> 00:04:18.260
for just pennies per animal.

00:04:19.123 --> 00:04:21.205
Yet this is hugely neglected.

00:04:21.229 --> 00:04:25.039
There are 3,000 times
more animals in factory farms

00:04:25.063 --> 00:04:26.624
than there are stray pets,

00:04:28.600 --> 00:04:32.973
but yet, factory farming gets one fiftieth
of the philanthropic funding.

00:04:34.211 --> 00:04:36.339
That means additional
resources in this area

00:04:36.363 --> 00:04:38.513
could have a truly transformative impact.

00:04:39.458 --> 00:04:42.443
Now the third area is the one
that I want to focus on the most,

00:04:42.467 --> 00:04:45.451
and that's the category
of existential risks:

00:04:45.475 --> 00:04:49.348
events like a nuclear war
or a global pandemic

00:04:50.824 --> 00:04:53.435
that could permanently derail civilization

00:04:54.156 --> 00:04:56.592
or even lead to the extinction
of the human race.

00:04:57.882 --> 00:05:00.422
Let me explain why I think
this is such a big priority

00:05:00.446 --> 00:05:01.783
in terms of this framework.

00:05:02.992 --> 00:05:04.404
First, size.

00:05:05.341 --> 00:05:09.230
How bad would it be if there were
a truly existential catastrophe?

00:05:10.920 --> 00:05:17.262
Well, it would involve the deaths
of all seven billion people on this planet

00:05:17.286 --> 00:05:20.405
and that means you
and everyone you know and love.

00:05:21.214 --> 00:05:23.793
That's just a tragedy
of unimaginable size.

00:05:25.684 --> 00:05:27.660
But then, what's more,

00:05:27.684 --> 00:05:31.289
it would also mean the curtailment
of humanity's future potential,

00:05:31.313 --> 00:05:34.265
and I believe that humanity's
potential is vast.

00:05:35.551 --> 00:05:39.002
The human race has been around
for about 200,000 years,

00:05:39.026 --> 00:05:41.909
and if she lives as long
as a typical mammalian species,

00:05:41.933 --> 00:05:44.231
she would last
for about two million years.

00:05:46.884 --> 00:05:49.575
If the human race
were a single individual,

00:05:49.599 --> 00:05:52.018
she would be just 10 years old today.

00:05:53.526 --> 00:05:57.692
And what's more, the human race
isn't a typical mammalian species.

00:05:58.950 --> 00:06:00.856
There's no reason why, if we're careful,

00:06:00.880 --> 00:06:03.085
we should die off
after only two million years.

00:06:03.839 --> 00:06:07.879
The earth will remain habitable
for 500 million years to come.

00:06:08.696 --> 00:06:10.640
And if someday, we took to the stars,

00:06:11.640 --> 00:06:14.156
the civilization could continue
for billions more.

00:06:16.193 --> 00:06:18.581
So I think the future
is going to be really big,

00:06:19.669 --> 00:06:21.471
but is it going to be good?

00:06:21.495 --> 00:06:24.312
Is the human race
even really worth preserving?

00:06:26.540 --> 00:06:30.469
Well, we hear all the time about
how things have been getting worse,

00:06:31.459 --> 00:06:34.152
but I think that when
we take the long run,

00:06:34.176 --> 00:06:36.207
things have been getting radically better.

00:06:37.453 --> 00:06:39.747
Here, for example,
is life expectancy over time.

00:06:40.892 --> 00:06:43.915
Here's the proportion of people
not living in extreme poverty.

00:06:45.106 --> 00:06:49.201
Here's the number of countries over time
that have decriminalized homosexuality.

00:06:50.848 --> 00:06:54.117
Here's the number of countries over time
that have become democratic.

00:06:55.015 --> 00:06:59.634
Then, when we look to the future,
there could be so much more to gain again.

00:06:59.658 --> 00:07:00.886
We'll be so much richer,

00:07:00.910 --> 00:07:04.505
we can solve so many problems
that are intractable today.

00:07:05.389 --> 00:07:09.834
So if this is kind of a graph
of how humanity has progressed

00:07:09.858 --> 00:07:12.748
in terms of total human
flourishing over time,

00:07:12.772 --> 00:07:16.127
well, this is what we would expect
future progress to look like.

00:07:16.881 --> 00:07:18.031
It's vast.

00:07:18.953 --> 00:07:20.151
Here, for example,

00:07:20.175 --> 00:07:23.921
is where we would expect no one
to live in extreme poverty.

00:07:25.930 --> 00:07:29.132
Here is where we would expect
everyone to be better off

00:07:29.156 --> 00:07:31.009
than the richest person alive today.

00:07:32.081 --> 00:07:35.273
Perhaps here is where we would discover
the fundamental natural laws

00:07:35.297 --> 00:07:36.565
that govern our world.

00:07:37.516 --> 00:07:41.221
Perhaps here is where we discover
an entirely new form of art,

00:07:41.245 --> 00:07:44.285
a form of music we currently lack
the ears to hear.

00:07:45.072 --> 00:07:47.294
And this is just
the next few thousand years.

00:07:47.827 --> 00:07:50.032
Once we think past that,

00:07:50.056 --> 00:07:54.223
well, we can't even imagine the heights
that human accomplishment might reach.

00:07:54.247 --> 00:07:57.287
So the future could be very big
and it could be very good,

00:07:57.311 --> 00:07:59.397
but are there ways
we could lose this value?

00:08:00.366 --> 00:08:02.192
And sadly, I think there are.

00:08:02.216 --> 00:08:06.269
The last two centuries brought
tremendous technological progress,

00:08:06.293 --> 00:08:08.915
but they also brought
the global risks of nuclear war

00:08:08.939 --> 00:08:11.096
and the possibility
of extreme climate change.

00:08:11.725 --> 00:08:13.492
When we look to the coming centuries,

00:08:13.516 --> 00:08:16.163
we should expect to see
the same pattern again.

00:08:16.187 --> 00:08:19.543
And we can see some radically
powerful technologies on the horizon.

00:08:20.132 --> 00:08:22.981
Synthetic biology might give us
the power to create viruses

00:08:23.005 --> 00:08:26.052
of unprecedented
contagiousness and lethality.

00:08:27.131 --> 00:08:31.774
Geoengineering might give us the power
to dramatically alter the earth's climate.

00:08:31.798 --> 00:08:35.997
Artificial intelligence might give us
the power to create intelligent agents

00:08:36.021 --> 00:08:38.163
with abilities greater than our own.

00:08:40.222 --> 00:08:44.110
Now, I'm not saying that any
of these risks are particularly likely,

00:08:44.134 --> 00:08:45.778
but when there's so much at stake,

00:08:45.802 --> 00:08:48.769
even small probabilities
matter a great deal.

00:08:49.568 --> 00:08:52.569
Imagine if you're getting on a plane
and you're kind of nervous,

00:08:52.593 --> 00:08:56.037
and the pilot reassures you by saying,

00:08:56.061 --> 00:09:00.695
"There's only a one-in-a-thousand
chance of crashing. Don't worry."

00:09:02.157 --> 00:09:03.711
Would you feel reassured?

00:09:04.509 --> 00:09:08.597
For these reasons, I think that preserving
the future of humanity

00:09:08.621 --> 00:09:11.605
is among the most important problems
that we currently face.

00:09:12.546 --> 00:09:14.696
But let's keep using this framework.

00:09:14.720 --> 00:09:16.030
Is this problem neglected?

00:09:18.085 --> 00:09:20.367
And I think the answer is yes,

00:09:20.391 --> 00:09:23.716
and that's because problems
that affect future generations

00:09:23.740 --> 00:09:25.391
are often hugely neglected.

00:09:26.930 --> 00:09:28.336
Why?

00:09:28.360 --> 00:09:31.838
Because future people
don't participate in markets today.

00:09:31.862 --> 00:09:33.384
They don't have a vote.

00:09:33.931 --> 00:09:36.604
It's not like there's a lobby
representing the interests

00:09:36.628 --> 00:09:38.651
of those born in 2300 AD.

00:09:40.313 --> 00:09:43.555
They don't get to influence
the decisions we make today.

00:09:43.995 --> 00:09:45.186
They're voiceless.

00:09:46.490 --> 00:09:49.935
And that means we still spend
a paltry amount on these issues:

00:09:49.959 --> 00:09:51.758
nuclear nonproliferation,

00:09:51.782 --> 00:09:54.112
geoengineering, biorisk,

00:09:55.414 --> 00:09:57.056
artificial intelligence safety.

00:09:57.923 --> 00:10:00.797
All of these receive
only a few tens of millions of dollars

00:10:00.821 --> 00:10:02.748
of philanthropic funding every year.

00:10:04.044 --> 00:10:07.973
That's tiny compared
to the 390 billion dollars

00:10:08.790 --> 00:10:11.051
that's spent on US philanthropy in total.

00:10:13.885 --> 00:10:16.369
The final aspect of our framework then:

00:10:17.083 --> 00:10:18.273
Is this solvable?

00:10:19.289 --> 00:10:20.578
I believe it is.

00:10:21.014 --> 00:10:24.061
You can contribute with your money,

00:10:24.085 --> 00:10:26.729
your career or your political engagement.

00:10:28.225 --> 00:10:30.400
With your money,
you can support organizations

00:10:30.424 --> 00:10:31.726
that focus on these risks,

00:10:31.750 --> 00:10:34.305
like the Nuclear Threat Initiative,

00:10:34.329 --> 00:10:37.989
which campaigns to take nuclear weapons
off hair-trigger alert,

00:10:38.013 --> 00:10:41.584
or the Blue Ribbon Panel, which
develops policy to minimize the damage

00:10:41.608 --> 00:10:43.703
from natural and man-made pandemics,

00:10:45.158 --> 00:10:48.418
or the Center for Human-Compatible AI,
which does technical research

00:10:48.442 --> 00:10:51.189
to ensure that AI systems
are safe and reliable.

00:10:52.652 --> 00:10:54.168
With your political engagement,

00:10:54.192 --> 00:10:57.288
you can vote for candidates
that care about these risks,

00:10:57.312 --> 00:10:59.898
and you can support
greater international cooperation.

00:11:01.767 --> 00:11:05.309
And then with your career,
there is so much that you can do.

00:11:05.333 --> 00:11:09.005
Of course, we need scientists
and policymakers and organization leaders,

00:11:09.865 --> 00:11:11.017
but just as importantly,

00:11:11.041 --> 00:11:15.158
we also need accountants
and managers and assistants

00:11:16.691 --> 00:11:20.445
to work in these organizations
that are tackling these problems.

00:11:20.469 --> 00:11:23.961
Now, the research program
of effective altruism

00:11:25.191 --> 00:11:26.635
is still in its infancy,

00:11:27.262 --> 00:11:29.786
and there's still a huge amount
that we don't know.

00:11:31.173 --> 00:11:33.516
But even with what we've learned so far,

00:11:34.748 --> 00:11:36.931
we can see that by thinking carefully

00:11:37.494 --> 00:11:42.367
and by focusing on those problems
that are big, solvable and neglected,

00:11:43.152 --> 00:11:45.860
we can make a truly tremendous
difference to the world

00:11:45.884 --> 00:11:47.515
for thousands of years to come.

00:11:47.963 --> 00:11:49.114
Thank you.

00:11:49.138 --> 00:11:53.698
(Applause)

