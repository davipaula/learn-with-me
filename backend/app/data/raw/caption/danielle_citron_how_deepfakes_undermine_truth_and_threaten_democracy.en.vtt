WEBVTT
Kind: captions
Language: en

00:00:12.535 --> 00:00:15.302
[This talk contains mature content]

00:00:17.762 --> 00:00:20.754
Rana Ayyub is a journalist in India

00:00:20.778 --> 00:00:23.380
whose work has exposed
government corruption

00:00:24.411 --> 00:00:26.568
and human rights violations.

00:00:26.990 --> 00:00:28.157
And over the years,

00:00:28.181 --> 00:00:31.484
she's gotten used to vitriol
and controversy around her work.

00:00:32.149 --> 00:00:37.258
But none of it could have prepared her
for what she faced in April 2018.

00:00:38.125 --> 00:00:41.776
She was sitting in a cafÃ© with a friend
when she first saw it:

00:00:41.800 --> 00:00:46.743
a two-minute, 20-second video
of her engaged in a sex act.

00:00:47.188 --> 00:00:49.537
And she couldn't believe her eyes.

00:00:49.561 --> 00:00:51.834
She had never made a sex video.

00:00:52.506 --> 00:00:55.971
But unfortunately, thousands
upon thousands of people

00:00:55.995 --> 00:00:57.661
would believe it was her.

00:00:58.673 --> 00:01:01.617
I interviewed Ms. Ayyub
about three months ago,

00:01:01.641 --> 00:01:04.145
in connection with my book
on sexual privacy.

00:01:04.681 --> 00:01:07.879
I'm a law professor, lawyer
and civil rights advocate.

00:01:08.204 --> 00:01:12.815
So it's incredibly frustrating
knowing that right now,

00:01:12.839 --> 00:01:15.077
law could do very little to help her.

00:01:15.458 --> 00:01:17.005
And as we talked,

00:01:17.029 --> 00:01:21.541
she explained that she should have seen
the fake sex video coming.

00:01:22.038 --> 00:01:27.634
She said, "After all, sex is so often used
to demean and to shame women,

00:01:27.658 --> 00:01:30.086
especially minority women,

00:01:30.110 --> 00:01:34.422
and especially minority women
who dare to challenge powerful men,"

00:01:34.446 --> 00:01:35.979
as she had in her work.

00:01:37.191 --> 00:01:41.167
The fake sex video went viral in 48 hours.

00:01:42.064 --> 00:01:47.371
All of her online accounts were flooded
with screenshots of the video,

00:01:47.395 --> 00:01:50.022
with graphic rape and death threats

00:01:50.046 --> 00:01:52.579
and with slurs about her Muslim faith.

00:01:53.426 --> 00:01:57.990
Online posts suggested that
she was "available" for sex.

00:01:58.014 --> 00:01:59.624
And she was doxed,

00:01:59.648 --> 00:02:02.426
which means that her home address
and her cell phone number

00:02:02.450 --> 00:02:04.196
were spread across the internet.

00:02:04.879 --> 00:02:08.963
The video was shared
more than 40,000 times.

00:02:09.760 --> 00:02:13.696
Now, when someone is targeted
with this kind of cybermob attack,

00:02:13.720 --> 00:02:15.783
the harm is profound.

00:02:16.482 --> 00:02:19.521
Rana Ayyub's life was turned upside down.

00:02:20.211 --> 00:02:23.545
For weeks, she could hardly eat or speak.

00:02:23.919 --> 00:02:27.608
She stopped writing and closed
all of her social media accounts,

00:02:27.632 --> 00:02:30.790
which is, you know, a tough thing to do
when you're a journalist.

00:02:31.188 --> 00:02:34.672
And she was afraid to go outside
her family's home.

00:02:34.696 --> 00:02:37.718
What if the posters
made good on their threats?

00:02:38.395 --> 00:02:42.760
The UN Council on Human Rights
confirmed that she wasn't being crazy.

00:02:42.784 --> 00:02:47.421
It issued a public statement saying
that they were worried about her safety.

00:02:48.776 --> 00:02:53.005
What Rana Ayyub faced was a deepfake:

00:02:53.029 --> 00:02:55.569
machine-learning technology

00:02:55.593 --> 00:02:59.704
that manipulates or fabricates
audio and video recordings

00:02:59.728 --> 00:03:02.451
to show people doing and saying things

00:03:02.475 --> 00:03:04.341
that they never did or said.

00:03:04.807 --> 00:03:08.168
Deepfakes appear authentic
and realistic, but they're not;

00:03:08.192 --> 00:03:09.964
they're total falsehoods.

00:03:11.228 --> 00:03:15.022
Although the technology
is still developing in its sophistication,

00:03:15.046 --> 00:03:16.660
it is widely available.

00:03:17.371 --> 00:03:20.443
Now, the most recent attention
to deepfakes arose,

00:03:20.467 --> 00:03:22.628
as so many things do online,

00:03:22.652 --> 00:03:23.907
with pornography.

00:03:24.498 --> 00:03:26.609
In early 2018,

00:03:26.633 --> 00:03:29.101
someone posted a tool on Reddit

00:03:29.125 --> 00:03:33.537
to allow users to insert faces
into porn videos.

00:03:33.561 --> 00:03:37.001
And what followed was a cascade
of fake porn videos

00:03:37.025 --> 00:03:39.822
featuring people's favorite
female celebrities.

00:03:40.712 --> 00:03:44.189
And today, you can go on YouTube
and pull up countless tutorials

00:03:44.213 --> 00:03:46.499
with step-by-step instructions

00:03:46.523 --> 00:03:49.686
on how to make a deepfake
on your desktop application.

00:03:50.260 --> 00:03:53.966
And soon we may be even able
to make them on our cell phones.

00:03:55.072 --> 00:04:00.454
Now, it's the interaction
of some of our most basic human frailties

00:04:00.478 --> 00:04:02.160
and network tools

00:04:02.184 --> 00:04:04.850
that can turn deepfakes into weapons.

00:04:04.874 --> 00:04:06.074
So let me explain.

00:04:06.875 --> 00:04:11.441
As human beings, we have
a visceral reaction to audio and video.

00:04:11.860 --> 00:04:13.348
We believe they're true,

00:04:13.372 --> 00:04:15.450
on the notion that
of course you can believe

00:04:15.474 --> 00:04:17.952
what your eyes and ears are telling you.

00:04:18.476 --> 00:04:20.175
And it's that mechanism

00:04:20.199 --> 00:04:23.897
that might undermine our shared
sense of reality.

00:04:23.921 --> 00:04:27.068
Although we believe deepfakes
to be true, they're not.

00:04:27.604 --> 00:04:31.761
And we're attracted
to the salacious, the provocative.

00:04:32.365 --> 00:04:35.412
We tend to believe
and to share information

00:04:35.436 --> 00:04:37.459
that's negative and novel.

00:04:37.809 --> 00:04:42.828
And researchers have found that online
hoaxes spread 10 times faster

00:04:42.852 --> 00:04:44.479
than accurate stories.

00:04:46.015 --> 00:04:50.395
Now, we're also drawn to information

00:04:50.419 --> 00:04:52.311
that aligns with our viewpoints.

00:04:52.950 --> 00:04:56.511
Psychologists call that tendency
"confirmation bias."

00:04:57.300 --> 00:05:01.687
And social media platforms
supercharge that tendency,

00:05:01.711 --> 00:05:05.592
by allowing us to instantly
and widely share information

00:05:05.616 --> 00:05:07.408
that accords with our viewpoints.

00:05:08.735 --> 00:05:14.303
Now, deepfakes have the potential to cause
grave individual and societal harm.

00:05:15.204 --> 00:05:17.228
So, imagine a deepfake

00:05:17.252 --> 00:05:21.434
that shows American soldiers
in Afganistan burning a Koran.

00:05:22.807 --> 00:05:25.831
You can imagine that that deepfake
would provoke violence

00:05:25.855 --> 00:05:27.388
against those soldiers.

00:05:27.847 --> 00:05:30.720
And what if the very next day

00:05:30.744 --> 00:05:32.998
there's another deepfake that drops,

00:05:33.022 --> 00:05:36.339
that shows a well-known imam
based in London

00:05:36.363 --> 00:05:38.830
praising the attack on those soldiers?

00:05:39.617 --> 00:05:42.780
We might see violence and civil unrest,

00:05:42.804 --> 00:05:46.053
not only in Afganistan
and the United Kingdom,

00:05:46.077 --> 00:05:47.592
but across the globe.

00:05:48.251 --> 00:05:49.409
And you might say to me,

00:05:49.433 --> 00:05:51.680
"Come on, Danielle, that's far-fetched."

00:05:51.704 --> 00:05:52.854
But it's not.

00:05:53.293 --> 00:05:55.484
We've seen falsehoods spread

00:05:55.508 --> 00:05:58.230
on WhatsApp and other
online message services

00:05:58.254 --> 00:06:01.015
lead to violence
against ethnic minorities.

00:06:01.039 --> 00:06:02.926
And that was just text --

00:06:02.950 --> 00:06:04.974
imagine if it were video.

00:06:06.593 --> 00:06:11.950
Now, deepfakes have the potential
to corrode the trust that we have

00:06:11.974 --> 00:06:13.966
in democratic institutions.

00:06:15.006 --> 00:06:17.673
So, imagine the night before an election.

00:06:17.996 --> 00:06:21.234
There's a deepfake showing
one of the major party candidates

00:06:21.258 --> 00:06:22.408
gravely sick.

00:06:23.202 --> 00:06:25.535
The deepfake could tip the election

00:06:25.559 --> 00:06:28.934
and shake our sense
that elections are legitimate.

00:06:30.515 --> 00:06:33.841
Imagine if the night before
an initial public offering

00:06:33.865 --> 00:06:36.198
of a major global bank,

00:06:36.222 --> 00:06:39.371
there was a deepfake
showing the bank's CEO

00:06:39.395 --> 00:06:42.092
drunkenly spouting conspiracy theories.

00:06:42.887 --> 00:06:45.934
The deepfake could tank the IPO,

00:06:45.958 --> 00:06:50.073
and worse, shake our sense
that financial markets are stable.

00:06:51.385 --> 00:06:58.374
So deepfakes can exploit and magnify
the deep distrust that we already have

00:06:58.398 --> 00:07:02.612
in politicians, business leaders
and other influential leaders.

00:07:02.945 --> 00:07:06.229
They find an audience
primed to believe them.

00:07:07.287 --> 00:07:10.052
And the pursuit of truth
is on the line as well.

00:07:11.077 --> 00:07:14.641
Technologists expect
that with advances in AI,

00:07:14.665 --> 00:07:18.347
soon it may be difficult if not impossible

00:07:18.371 --> 00:07:22.140
to tell the difference between
a real video and a fake one.

00:07:23.022 --> 00:07:28.363
So how can the truth emerge
in a deepfake-ridden marketplace of ideas?

00:07:28.752 --> 00:07:32.172
Will we just proceed along
the path of least resistance

00:07:32.196 --> 00:07:34.633
and believe what we want to believe,

00:07:34.657 --> 00:07:35.807
truth be damned?

00:07:36.831 --> 00:07:40.006
And not only might we believe the fakery,

00:07:40.030 --> 00:07:43.356
we might start disbelieving the truth.

00:07:43.887 --> 00:07:47.966
We've already seen people invoke
the phenomenon of deepfakes

00:07:47.990 --> 00:07:51.910
to cast doubt on real evidence
of their wrongdoing.

00:07:51.934 --> 00:07:57.903
We've heard politicians say of audio
of their disturbing comments,

00:07:57.927 --> 00:07:59.673
"Come on, that's fake news.

00:07:59.697 --> 00:08:03.617
You can't believe what your eyes
and ears are telling you."

00:08:04.402 --> 00:08:06.133
And it's that risk

00:08:06.157 --> 00:08:11.593
that professor Robert Chesney and I
call the "liar's dividend":

00:08:11.617 --> 00:08:14.974
the risk that liars will invoke deepfakes

00:08:14.998 --> 00:08:17.903
to escape accountability
for their wrongdoing.

00:08:18.963 --> 00:08:22.034
So we've got our work cut out for us,
there's no doubt about it.

00:08:22.606 --> 00:08:25.931
And we're going to need
a proactive solution

00:08:25.955 --> 00:08:29.466
from tech companies, from lawmakers,

00:08:29.490 --> 00:08:31.474
law enforcers and the media.

00:08:32.093 --> 00:08:36.109
And we're going to need
a healthy dose of societal resilience.

00:08:37.506 --> 00:08:41.402
So now, we're right now engaged
in a very public conversation

00:08:41.426 --> 00:08:44.339
about the responsibility
of tech companies.

00:08:44.926 --> 00:08:47.958
And my advice to social media platforms

00:08:47.982 --> 00:08:51.855
has been to change their terms of service
and community guidelines

00:08:51.879 --> 00:08:54.215
to ban deepfakes that cause harm.

00:08:54.712 --> 00:08:58.672
That determination,
that's going to require human judgment,

00:08:58.696 --> 00:09:00.267
and it's expensive.

00:09:00.673 --> 00:09:02.958
But we need human beings

00:09:02.982 --> 00:09:06.855
to look at the content
and context of a deepfake

00:09:06.879 --> 00:09:10.561
to figure out if it is
a harmful impersonation

00:09:10.585 --> 00:09:14.967
or instead, if it's valuable
satire, art or education.

00:09:16.118 --> 00:09:17.613
So now, what about the law?

00:09:18.666 --> 00:09:21.015
Law is our educator.

00:09:21.515 --> 00:09:25.553
It teaches us about
what's harmful and what's wrong.

00:09:25.577 --> 00:09:30.132
And it shapes behavior it deters
by punishing perpetrators

00:09:30.156 --> 00:09:32.423
and securing remedies for victims.

00:09:33.148 --> 00:09:37.428
Right now, law is not up to
the challenge of deepfakes.

00:09:38.116 --> 00:09:39.506
Across the globe,

00:09:39.530 --> 00:09:41.974
we lack well-tailored laws

00:09:41.998 --> 00:09:45.568
that would be designed to tackle
digital impersonations

00:09:45.592 --> 00:09:47.823
that invade sexual privacy,

00:09:47.847 --> 00:09:49.234
that damage reputations

00:09:49.258 --> 00:09:51.209
and that cause emotional distress.

00:09:51.725 --> 00:09:55.598
What happened to Rana Ayyub
is increasingly commonplace.

00:09:56.074 --> 00:09:58.288
Yet, when she went
to law enforcement in Delhi,

00:09:58.312 --> 00:10:00.447
she was told nothing could be done.

00:10:01.101 --> 00:10:04.284
And the sad truth is
that the same would be true

00:10:04.308 --> 00:10:06.574
in the United States and in Europe.

00:10:07.300 --> 00:10:11.656
So we have a legal vacuum
that needs to be filled.

00:10:12.292 --> 00:10:16.384
My colleague Dr. Mary Anne Franks and I
are working with US lawmakers

00:10:16.408 --> 00:10:21.212
to devise legislation that would ban
harmful digital impersonations

00:10:21.236 --> 00:10:23.769
that are tantamount to identity theft.

00:10:24.252 --> 00:10:26.378
And we've seen similar moves

00:10:26.402 --> 00:10:29.703
in Iceland, the UK and Australia.

00:10:30.157 --> 00:10:33.416
But of course, that's just a small piece
of the regulatory puzzle.

00:10:34.911 --> 00:10:38.080
Now, I know law is not a cure-all. Right?

00:10:38.104 --> 00:10:39.704
It's a blunt instrument.

00:10:40.346 --> 00:10:41.885
And we've got to use it wisely.

00:10:42.411 --> 00:10:45.223
It also has some practical impediments.

00:10:45.657 --> 00:10:50.701
You can't leverage law against people
you can't identify and find.

00:10:51.463 --> 00:10:54.749
And if a perpetrator lives
outside the country

00:10:54.773 --> 00:10:56.527
where a victim lives,

00:10:56.551 --> 00:10:58.180
then you may not be able to insist

00:10:58.204 --> 00:11:00.553
that the perpetrator
come into local courts

00:11:00.577 --> 00:11:01.727
to face justice.

00:11:02.236 --> 00:11:06.299
And so we're going to need
a coordinated international response.

00:11:07.819 --> 00:11:11.152
Education has to be part
of our response as well.

00:11:11.803 --> 00:11:15.534
Law enforcers are not
going to enforce laws

00:11:15.558 --> 00:11:17.016
they don't know about

00:11:17.040 --> 00:11:19.636
and proffer problems
they don't understand.

00:11:20.376 --> 00:11:22.567
In my research on cyberstalking,

00:11:22.591 --> 00:11:26.090
I found that law enforcement
lacked the training

00:11:26.114 --> 00:11:28.696
to understand the laws available to them

00:11:28.720 --> 00:11:31.069
and the problem of online abuse.

00:11:31.093 --> 00:11:33.775
And so often they told victims,

00:11:33.799 --> 00:11:37.770
"Just turn your computer off.
Ignore it. It'll go away."

00:11:38.261 --> 00:11:40.727
And we saw that in Rana Ayyub's case.

00:11:41.102 --> 00:11:44.570
She was told, "Come on,
you're making such a big deal about this.

00:11:44.594 --> 00:11:46.337
It's boys being boys."

00:11:47.268 --> 00:11:52.520
And so we need to pair new legislation
with efforts at training.

00:11:54.053 --> 00:11:57.482
And education has to be aimed
on the media as well.

00:11:58.180 --> 00:12:02.440
Journalists need educating
about the phenomenon of deepfakes

00:12:02.464 --> 00:12:05.503
so they don't amplify and spread them.

00:12:06.583 --> 00:12:08.751
And this is the part
where we're all involved.

00:12:08.775 --> 00:12:12.630
Each and every one of us needs educating.

00:12:13.375 --> 00:12:17.050
We click, we share, we like,
and we don't even think about it.

00:12:17.551 --> 00:12:19.098
We need to do better.

00:12:19.726 --> 00:12:22.535
We need far better radar for fakery.

00:12:25.744 --> 00:12:29.585
So as we're working
through these solutions,

00:12:29.609 --> 00:12:32.172
there's going to be
a lot of suffering to go around.

00:12:33.093 --> 00:12:35.839
Rana Ayyub is still wrestling
with the fallout.

00:12:36.669 --> 00:12:40.858
She still doesn't feel free
to express herself on- and offline.

00:12:41.566 --> 00:12:42.931
And as she told me,

00:12:42.955 --> 00:12:48.029
she still feels like there are thousands
of eyes on her naked body,

00:12:48.053 --> 00:12:51.714
even though, intellectually,
she knows it wasn't her body.

00:12:52.371 --> 00:12:54.720
And she has frequent panic attacks,

00:12:54.744 --> 00:12:58.844
especially when someone she doesn't know
tries to take her picture.

00:12:58.868 --> 00:13:02.379
"What if they're going to make
another deepfake?" she thinks to herself.

00:13:03.082 --> 00:13:07.003
And so for the sake of
individuals like Rana Ayyub

00:13:07.027 --> 00:13:09.333
and the sake of our democracy,

00:13:09.357 --> 00:13:11.539
we need to do something right now.

00:13:11.563 --> 00:13:12.714
Thank you.

00:13:12.738 --> 00:13:15.246
(Applause)

