WEBVTT
Kind: captions
Language: en

00:00:12.904 --> 00:00:15.790
Chris Anderson: Help us understand
what machine learning is,

00:00:15.814 --> 00:00:17.868
because that seems to be the key driver

00:00:17.892 --> 00:00:20.629
of so much of the excitement
and also of the concern

00:00:20.653 --> 00:00:22.147
around artificial intelligence.

00:00:22.171 --> 00:00:23.814
How does machine learning work?

00:00:23.838 --> 00:00:27.734
Sebastian Thrun: So, artificial
intelligence and machine learning

00:00:27.758 --> 00:00:29.760
is about 60 years old

00:00:29.784 --> 00:00:34.053
and has not had a great day
in its past until recently.

00:00:34.077 --> 00:00:37.001
And the reason is that today,

00:00:37.025 --> 00:00:40.998
we have reached a scale
of computing and datasets

00:00:41.022 --> 00:00:43.659
that was necessary to make machines smart.

00:00:43.683 --> 00:00:45.434
So here's how it works.

00:00:45.458 --> 00:00:48.955
If you program a computer today,
say, your phone,

00:00:48.979 --> 00:00:51.314
then you hire software engineers

00:00:51.338 --> 00:00:55.192
that write a very,
very long kitchen recipe,

00:00:55.216 --> 00:00:58.348
like, "If the water is too hot,
turn down the temperature.

00:00:58.372 --> 00:01:00.651
If it's too cold, turn up
the temperature."

00:01:00.675 --> 00:01:03.524
The recipes are not just 10 lines long.

00:01:03.548 --> 00:01:06.151
They are millions of lines long.

00:01:06.175 --> 00:01:10.259
A modern cell phone
has 12 million lines of code.

00:01:10.283 --> 00:01:12.929
A browser has five million lines of code.

00:01:12.953 --> 00:01:17.922
And each bug in this recipe
can cause your computer to crash.

00:01:17.946 --> 00:01:21.021
That's why a software engineer
makes so much money.

00:01:21.953 --> 00:01:25.613
The new thing now is that computers
can find their own rules.

00:01:25.637 --> 00:01:29.243
So instead of an expert
deciphering, step by step,

00:01:29.267 --> 00:01:31.415
a rule for every contingency,

00:01:31.439 --> 00:01:34.513
what you do now is you give
the computer examples

00:01:34.537 --> 00:01:36.118
and have it infer its own rules.

00:01:36.142 --> 00:01:40.448
A really good example is AlphaGo,
which recently was won by Google.

00:01:40.472 --> 00:01:44.159
Normally, in game playing,
you would really write down all the rules,

00:01:44.183 --> 00:01:45.968
but in AlphaGo's case,

00:01:45.992 --> 00:01:48.058
the system looked over a million games

00:01:48.082 --> 00:01:50.274
and was able to infer its own rules

00:01:50.298 --> 00:01:53.036
and then beat the world's
residing Go champion.

00:01:53.853 --> 00:01:57.362
That is exciting, because it relieves
the software engineer

00:01:57.386 --> 00:01:59.205
of the need of being super smart,

00:01:59.229 --> 00:02:01.554
and pushes the burden towards the data.

00:02:01.578 --> 00:02:06.112
As I said, the inflection point
where this has become really possible --

00:02:06.136 --> 00:02:08.882
very embarrassing, my thesis
was about machine learning.

00:02:08.906 --> 00:02:11.111
It was completely
insignificant, don't read it,

00:02:11.135 --> 00:02:12.485
because it was 20 years ago

00:02:12.509 --> 00:02:15.416
and back then, the computers
were as big as a cockroach brain.

00:02:15.440 --> 00:02:17.771
Now they are powerful enough
to really emulate

00:02:17.795 --> 00:02:19.871
kind of specialized human thinking.

00:02:19.895 --> 00:02:22.208
And then the computers
take advantage of the fact

00:02:22.232 --> 00:02:24.732
that they can look at
much more data than people can.

00:02:24.756 --> 00:02:27.836
So I'd say AlphaGo looked at
more than a million games.

00:02:27.860 --> 00:02:30.699
No human expert can ever
study a million games.

00:02:30.723 --> 00:02:33.905
Google has looked at over
a hundred billion web pages.

00:02:33.929 --> 00:02:36.579
No person can ever study
a hundred billion web pages.

00:02:36.603 --> 00:02:39.317
So as a result,
the computer can find rules

00:02:39.341 --> 00:02:41.096
that even people can't find.

00:02:41.120 --> 00:02:45.432
CA: So instead of looking ahead
to, "If he does that, I will do that,"

00:02:45.456 --> 00:02:48.528
it's more saying, "Here is what
looks like a winning pattern,

00:02:48.552 --> 00:02:50.631
here is what looks like
a winning pattern."

00:02:50.655 --> 00:02:53.172
ST: Yeah. I mean, think about
how you raise children.

00:02:53.196 --> 00:02:56.840
You don't spend the first 18 years
giving kids a rule for every contingency

00:02:56.864 --> 00:02:59.211
and set them free
and they have this big program.

00:02:59.235 --> 00:03:01.954
They stumble, fall, get up,
they get slapped or spanked,

00:03:01.978 --> 00:03:04.862
and they have a positive experience,
a good grade in school,

00:03:04.886 --> 00:03:06.720
and they figure it out on their own.

00:03:06.744 --> 00:03:08.481
That's happening with computers now,

00:03:08.505 --> 00:03:11.534
which makes computer programming
so much easier all of a sudden.

00:03:11.558 --> 00:03:14.733
Now we don't have to think anymore.
We just give them lots of data.

00:03:14.757 --> 00:03:18.179
CA: And so, this has been key
to the spectacular improvement

00:03:18.203 --> 00:03:21.267
in power of self-driving cars.

00:03:21.291 --> 00:03:23.030
I think you gave me an example.

00:03:23.054 --> 00:03:25.739
Can you explain what's happening here?

00:03:25.763 --> 00:03:29.327
ST: This is a drive of a self-driving car

00:03:29.351 --> 00:03:31.308
that we happened to have at Udacity

00:03:31.332 --> 00:03:33.730
and recently made
into a spin-off called Voyage.

00:03:33.754 --> 00:03:36.328
We have used this thing
called deep learning

00:03:36.352 --> 00:03:37.975
to train a car to drive itself,

00:03:37.999 --> 00:03:40.386
and this is driving
from Mountain View, California,

00:03:40.410 --> 00:03:41.578
to San Francisco

00:03:41.602 --> 00:03:43.861
on El Camino Real on a rainy day,

00:03:43.885 --> 00:03:47.409
with bicyclists and pedestrians
and 133 traffic lights.

00:03:47.433 --> 00:03:50.069
And the novel thing here is,

00:03:50.093 --> 00:03:53.213
many, many moons ago, I started
the Google self-driving car team.

00:03:53.237 --> 00:03:56.418
And back in the day, I hired
the world's best software engineers

00:03:56.442 --> 00:03:58.049
to find the world's best rules.

00:03:58.073 --> 00:03:59.827
This is just trained.

00:03:59.851 --> 00:04:03.187
We drive this road 20 times,

00:04:03.211 --> 00:04:05.658
we put all this data
into the computer brain,

00:04:05.682 --> 00:04:07.764
and after a few hours of processing,

00:04:07.788 --> 00:04:11.714
it comes up with behavior
that often surpasses human agility.

00:04:11.738 --> 00:04:13.755
So it's become really easy to program it.

00:04:13.779 --> 00:04:17.582
This is 100 percent autonomous,
about 33 miles, an hour and a half.

00:04:17.606 --> 00:04:21.236
CA: So, explain it -- on the big part
of this program on the left,

00:04:21.260 --> 00:04:24.517
you're seeing basically what
the computer sees as trucks and cars

00:04:24.541 --> 00:04:27.427
and those dots overtaking it and so forth.

00:04:27.451 --> 00:04:31.213
ST: On the right side, you see the camera
image, which is the main input here,

00:04:31.237 --> 00:04:33.913
and it's used to find lanes,
other cars, traffic lights.

00:04:33.937 --> 00:04:36.426
The vehicle has a radar
to do distance estimation.

00:04:36.450 --> 00:04:39.071
This is very commonly used
in these kind of systems.

00:04:39.095 --> 00:04:41.087
On the left side you see a laser diagram,

00:04:41.111 --> 00:04:44.311
where you see obstacles like trees
and so on depicted by the laser.

00:04:44.335 --> 00:04:47.771
But almost all the interesting work
is centering on the camera image now.

00:04:47.795 --> 00:04:51.271
We're really shifting over from precision
sensors like radars and lasers

00:04:51.295 --> 00:04:53.137
into very cheap, commoditized sensors.

00:04:53.161 --> 00:04:55.148
A camera costs less than eight dollars.

00:04:55.172 --> 00:04:57.965
CA: And that green dot
on the left thing, what is that?

00:04:57.989 --> 00:04:59.360
Is that anything meaningful?

00:04:59.384 --> 00:05:03.052
ST: This is a look-ahead point
for your adaptive cruise control,

00:05:03.076 --> 00:05:05.553
so it helps us understand
how to regulate velocity

00:05:05.577 --> 00:05:08.211
based on how far
the cars in front of you are.

00:05:08.235 --> 00:05:10.951
CA: And so, you've also
got an example, I think,

00:05:10.975 --> 00:05:13.356
of how the actual
learning part takes place.

00:05:13.380 --> 00:05:15.838
Maybe we can see that. Talk about this.

00:05:15.862 --> 00:05:19.505
ST: This is an example where we posed
a challenge to Udacity students

00:05:19.529 --> 00:05:22.660
to take what we call
a self-driving car Nanodegree.

00:05:22.684 --> 00:05:24.179
We gave them this dataset

00:05:24.203 --> 00:05:27.257
and said "Hey, can you guys figure out
how to steer this car?"

00:05:27.281 --> 00:05:28.905
And if you look at the images,

00:05:28.929 --> 00:05:33.002
it's, even for humans, quite impossible
to get the steering right.

00:05:33.026 --> 00:05:36.617
And we ran a competition and said,
"It's a deep learning competition,

00:05:36.641 --> 00:05:37.814
AI competition,"

00:05:37.838 --> 00:05:39.725
and we gave the students 48 hours.

00:05:39.749 --> 00:05:43.921
So if you are a software house
like Google or Facebook,

00:05:43.945 --> 00:05:46.662
something like this costs you
at least six months of work.

00:05:46.686 --> 00:05:48.888
So we figured 48 hours is great.

00:05:48.912 --> 00:05:52.379
And within 48 hours, we got about
100 submissions from students,

00:05:52.403 --> 00:05:55.773
and the top four got it perfectly right.

00:05:55.797 --> 00:05:58.437
It drives better than I could
drive on this imagery,

00:05:58.461 --> 00:05:59.650
using deep learning.

00:05:59.674 --> 00:06:01.473
And again, it's the same methodology.

00:06:01.497 --> 00:06:02.661
It's this magical thing.

00:06:02.685 --> 00:06:04.770
When you give enough data
to a computer now,

00:06:04.794 --> 00:06:06.934
and give enough time
to comprehend the data,

00:06:06.958 --> 00:06:08.403
it finds its own rules.

00:06:09.339 --> 00:06:14.184
CA: And so that has led to the development
of powerful applications

00:06:14.208 --> 00:06:15.733
in all sorts of areas.

00:06:15.757 --> 00:06:18.425
You were talking to me
the other day about cancer.

00:06:18.449 --> 00:06:19.638
Can I show this video?

00:06:19.662 --> 00:06:22.016
ST: Yeah, absolutely, please.
CA: This is cool.

00:06:22.040 --> 00:06:25.574
ST: This is kind of an insight
into what's happening

00:06:25.598 --> 00:06:28.027
in a completely different domain.

00:06:28.051 --> 00:06:31.803
This is augmenting, or competing --

00:06:31.827 --> 00:06:33.576
it's in the eye of the beholder --

00:06:33.600 --> 00:06:37.054
with people who are being paid
400,000 dollars a year,

00:06:37.078 --> 00:06:38.315
dermatologists,

00:06:38.339 --> 00:06:40.322
highly trained specialists.

00:06:40.346 --> 00:06:43.907
It takes more than a decade of training
to be a good dermatologist.

00:06:43.931 --> 00:06:47.127
What you see here is
the machine learning version of it.

00:06:47.151 --> 00:06:48.992
It's called a neural network.

00:06:49.016 --> 00:06:52.758
"Neural networks" is the technical term
for these machine learning algorithms.

00:06:52.782 --> 00:06:54.571
They've been around since the 1980s.

00:06:54.595 --> 00:06:59.235
This one was invented in 1988
by a Facebook Fellow called Yann LeCun,

00:06:59.259 --> 00:07:02.817
and it propagates data stages

00:07:02.841 --> 00:07:05.419
through what you could think of
as the human brain.

00:07:05.443 --> 00:07:08.409
It's not quite the same thing,
but it emulates the same thing.

00:07:08.433 --> 00:07:09.735
It goes stage after stage.

00:07:09.759 --> 00:07:13.396
In the very first stage, it takes
the visual input and extracts edges

00:07:13.420 --> 00:07:16.032
and rods and dots.

00:07:16.056 --> 00:07:19.093
And the next one becomes
more complicated edges

00:07:19.117 --> 00:07:22.308
and shapes like little half-moons.

00:07:22.332 --> 00:07:26.775
And eventually, it's able to build
really complicated concepts.

00:07:26.799 --> 00:07:28.847
Andrew Ng has been able to show

00:07:28.871 --> 00:07:32.351
that it's able to find
cat faces and dog faces

00:07:32.375 --> 00:07:34.036
in vast amounts of images.

00:07:34.060 --> 00:07:36.784
What my student team
at Stanford has shown is that

00:07:36.808 --> 00:07:42.881
if you train it on 129,000 images
of skin conditions,

00:07:42.905 --> 00:07:45.470
including melanoma and carcinomas,

00:07:45.494 --> 00:07:48.795
you can do as good a job

00:07:48.819 --> 00:07:51.016
as the best human dermatologists.

00:07:51.040 --> 00:07:53.589
And to convince ourselves
that this is the case,

00:07:53.613 --> 00:07:57.603
we captured an independent dataset
that we presented to our network

00:07:57.627 --> 00:08:01.969
and to 25 board-certified
Stanford-level dermatologists,

00:08:01.993 --> 00:08:03.665
and compared those.

00:08:03.689 --> 00:08:05.193
And in most cases,

00:08:05.217 --> 00:08:09.092
they were either on par or above
the performance classification accuracy

00:08:09.116 --> 00:08:10.583
of human dermatologists.

00:08:10.607 --> 00:08:12.353
CA: You were telling me an anecdote.

00:08:12.377 --> 00:08:14.334
I think about this image right here.

00:08:14.358 --> 00:08:15.842
What happened here?

00:08:15.866 --> 00:08:19.874
ST: This was last Thursday.
That's a moving piece.

00:08:19.898 --> 00:08:23.498
What we've shown before and we published
in "Nature" earlier this year

00:08:23.522 --> 00:08:26.006
was this idea that we show
dermatologists images

00:08:26.030 --> 00:08:27.569
and our computer program images,

00:08:27.593 --> 00:08:29.220
and count how often they're right.

00:08:29.244 --> 00:08:31.022
But all these images are past images.

00:08:31.046 --> 00:08:34.506
They've all been biopsied to make sure
we had the correct classification.

00:08:34.530 --> 00:08:35.702
This one wasn't.

00:08:35.726 --> 00:08:38.905
This one was actually done at Stanford
by one of our collaborators.

00:08:38.929 --> 00:08:41.243
The story goes that our collaborator,

00:08:41.267 --> 00:08:44.658
who is a world-famous dermatologist,
one of the three best, apparently,

00:08:44.682 --> 00:08:47.617
looked at this mole and said,
"This is not skin cancer."

00:08:47.641 --> 00:08:50.117
And then he had
a second moment, where he said,

00:08:50.141 --> 00:08:52.007
"Well, let me just check with the app."

00:08:52.031 --> 00:08:54.730
So he took out his iPhone
and ran our piece of software,

00:08:54.754 --> 00:08:56.875
our "pocket dermatologist," so to speak,

00:08:56.899 --> 00:08:59.893
and the iPhone said: cancer.

00:08:59.917 --> 00:09:01.223
It said melanoma.

00:09:01.849 --> 00:09:03.082
And then he was confused.

00:09:03.106 --> 00:09:07.657
And he decided, "OK, maybe I trust
the iPhone a little bit more than myself,"

00:09:07.681 --> 00:09:10.416
and he sent it out to the lab
to get it biopsied.

00:09:10.440 --> 00:09:12.909
And it came up as an aggressive melanoma.

00:09:13.545 --> 00:09:16.612
So I think this might be the first time
that we actually found,

00:09:16.636 --> 00:09:19.123
in the practice of using deep learning,

00:09:19.147 --> 00:09:22.519
an actual person whose melanoma
would have gone unclassified,

00:09:22.543 --> 00:09:24.658
had it not been for deep learning.

00:09:24.682 --> 00:09:26.242
CA: I mean, that's incredible.

00:09:26.266 --> 00:09:28.035
(Applause)

00:09:28.059 --> 00:09:31.659
It feels like there'd be an instant demand
for an app like this right now,

00:09:31.683 --> 00:09:33.649
that you might freak out a lot of people.

00:09:33.673 --> 00:09:37.200
Are you thinking of doing this,
making an app that allows self-checking?

00:09:37.224 --> 00:09:42.197
ST: So my in-box is flooded
about cancer apps,

00:09:42.221 --> 00:09:44.524
with heartbreaking stories of people.

00:09:44.548 --> 00:09:47.752
I mean, some people have had
10, 15, 20 melanomas removed,

00:09:47.776 --> 00:09:51.728
and are scared that one
might be overlooked, like this one,

00:09:51.752 --> 00:09:53.493
and also, about, I don't know,

00:09:53.517 --> 00:09:56.249
flying cars and speaker inquiries 
these days, I guess.

00:09:56.273 --> 00:09:59.011
My take is, we need more testing.

00:09:59.449 --> 00:10:01.227
I want to be very careful.

00:10:01.251 --> 00:10:04.917
It's very easy to give a flashy result
and impress a TED audience.

00:10:04.941 --> 00:10:07.568
It's much harder to put
something out that's ethical.

00:10:07.592 --> 00:10:09.986
And if people were to use the app

00:10:10.010 --> 00:10:12.807
and choose not to consult
the assistance of a doctor

00:10:12.831 --> 00:10:14.414
because we get it wrong,

00:10:14.438 --> 00:10:16.091
I would feel really bad about it.

00:10:16.115 --> 00:10:18.040
So we're currently doing clinical tests,

00:10:18.064 --> 00:10:20.862
and if these clinical tests commence
and our data holds up,

00:10:20.886 --> 00:10:23.876
we might be able at some point
to take this kind of technology

00:10:23.900 --> 00:10:25.792
and take it out of the Stanford clinic

00:10:25.816 --> 00:10:27.474
and bring it to the entire world,

00:10:27.498 --> 00:10:29.954
places where Stanford
doctors never, ever set foot.

00:10:30.617 --> 00:10:33.197
CA: And do I hear this right,

00:10:33.221 --> 00:10:35.187
that it seemed like what you were saying,

00:10:35.211 --> 00:10:39.465
because you are working
with this army of Udacity students,

00:10:39.489 --> 00:10:42.710
that in a way, you're applying
a different form of machine learning

00:10:42.734 --> 00:10:44.469
than might take place in a company,

00:10:44.493 --> 00:10:47.977
which is you're combining machine learning
with a form of crowd wisdom.

00:10:48.001 --> 00:10:51.385
Are you saying that sometimes you think
that could actually outperform

00:10:51.409 --> 00:10:53.459
what a company can do,
even a vast company?

00:10:53.483 --> 00:10:56.423
ST: I believe there's now
instances that blow my mind,

00:10:56.447 --> 00:10:58.205
and I'm still trying to understand.

00:10:58.229 --> 00:11:02.166
What Chris is referring to
is these competitions that we run.

00:11:02.190 --> 00:11:04.458
We turn them around in 48 hours,

00:11:04.482 --> 00:11:06.734
and we've been able to build
a self-driving car

00:11:06.758 --> 00:11:10.145
that can drive from Mountain View
to San Francisco on surface streets.

00:11:10.169 --> 00:11:13.753
It's not quite on par with Google
after seven years of Google work,

00:11:13.777 --> 00:11:16.305
but it's getting there.

00:11:16.329 --> 00:11:19.413
And it took us only two engineers
and three months to do this.

00:11:19.437 --> 00:11:22.293
And the reason is, we have
an army of students

00:11:22.317 --> 00:11:24.167
who participate in competitions.

00:11:24.191 --> 00:11:26.411
We're not the only ones
who use crowdsourcing.

00:11:26.435 --> 00:11:28.658
Uber and Didi use crowdsource for driving.

00:11:28.682 --> 00:11:31.441
Airbnb uses crowdsourcing for hotels.

00:11:31.465 --> 00:11:35.472
There's now many examples
where people do bug-finding crowdsourcing

00:11:35.496 --> 00:11:38.300
or protein folding, of all things,
in crowdsourcing.

00:11:38.324 --> 00:11:41.239
But we've been able to build
this car in three months,

00:11:41.263 --> 00:11:44.918
so I am actually rethinking

00:11:44.942 --> 00:11:47.180
how we organize corporations.

00:11:47.204 --> 00:11:51.900
We have a staff of 9,000 people
who are never hired,

00:11:51.924 --> 00:11:53.232
that I never fire.

00:11:53.256 --> 00:11:55.618
They show up to work
and I don't even know.

00:11:55.642 --> 00:11:58.700
Then they submit to me
maybe 9,000 answers.

00:11:58.724 --> 00:12:00.900
I'm not obliged to use any of those.

00:12:00.924 --> 00:12:02.915
I end up -- I pay only the winners,

00:12:02.939 --> 00:12:06.657
so I'm actually very cheapskate here,
which is maybe not the best thing to do.

00:12:06.681 --> 00:12:09.866
But they consider it part
of their education, too, which is nice.

00:12:09.890 --> 00:12:14.091
But these students have been able
to produce amazing deep learning results.

00:12:14.115 --> 00:12:17.976
So yeah, the synthesis of great people
and great machine learning is amazing.

00:12:18.000 --> 00:12:20.814
CA: I mean, Gary Kasparov said on
the first day [of TED2017]

00:12:20.848 --> 00:12:26.260
that the winners of chess, surprisingly,
turned out to be two amateur chess players

00:12:26.284 --> 00:12:31.655
with three mediocre-ish,
mediocre-to-good, computer programs,

00:12:31.679 --> 00:12:34.842
that could outperform one grand master
with one great chess player,

00:12:34.866 --> 00:12:36.609
like it was all part of the process.

00:12:36.633 --> 00:12:39.968
And it almost seems like
you're talking about a much richer version

00:12:39.992 --> 00:12:41.192
of that same idea.

00:12:41.216 --> 00:12:45.073
ST: Yeah, I mean, as you followed
the fantastic panels yesterday morning,

00:12:45.097 --> 00:12:47.091
two sessions about AI,

00:12:47.115 --> 00:12:49.282
robotic overlords and the human response,

00:12:49.306 --> 00:12:51.288
many, many great things were said.

00:12:51.312 --> 00:12:53.999
But one of the concerns is
that we sometimes confuse

00:12:54.023 --> 00:12:58.085
what's actually been done with AI
with this kind of overlord threat,

00:12:58.109 --> 00:13:01.533
where your AI develops
consciousness, right?

00:13:01.557 --> 00:13:04.528
The last thing I want
is for my AI to have consciousness.

00:13:04.552 --> 00:13:06.268
I don't want to come into my kitchen

00:13:06.292 --> 00:13:10.485
and have the refrigerator fall in love
with the dishwasher

00:13:10.509 --> 00:13:12.633
and tell me, because I wasn't nice enough,

00:13:12.657 --> 00:13:14.494
my food is now warm.

00:13:14.518 --> 00:13:17.409
I wouldn't buy these products,
and I don't want them.

00:13:17.825 --> 00:13:19.627
But the truth is, for me,

00:13:19.651 --> 00:13:22.371
AI has always been
an augmentation of people.

00:13:22.893 --> 00:13:24.569
It's been an augmentation of us,

00:13:24.593 --> 00:13:26.050
to make us stronger.

00:13:26.074 --> 00:13:28.905
And I think Kasparov was exactly correct.

00:13:28.929 --> 00:13:32.778
It's been the combination
of human smarts and machine smarts

00:13:32.802 --> 00:13:34.266
that make us stronger.

00:13:34.290 --> 00:13:38.877
The theme of machines making us stronger
is as old as machines are.

00:13:39.567 --> 00:13:43.325
The agricultural revolution took
place because it made steam engines

00:13:43.349 --> 00:13:46.015
and farming equipment
that couldn't farm by itself,

00:13:46.039 --> 00:13:48.161
that never replaced us;
it made us stronger.

00:13:48.185 --> 00:13:51.923
And I believe this new wave of AI
will make us much, much stronger

00:13:51.947 --> 00:13:53.130
as a human race.

00:13:53.765 --> 00:13:55.578
CA: We'll come on to that a bit more,

00:13:55.602 --> 00:13:59.273
but just to continue with the scary part
of this for some people,

00:13:59.297 --> 00:14:02.855
like, what feels like it gets
scary for people is when you have

00:14:02.879 --> 00:14:07.497
a computer that can, one,
rewrite its own code,

00:14:07.521 --> 00:14:11.105
so, it can create
multiple copies of itself,

00:14:11.129 --> 00:14:13.026
try a bunch of different code versions,

00:14:13.050 --> 00:14:14.825
possibly even at random,

00:14:14.849 --> 00:14:18.481
and then check them out and see
if a goal is achieved and improved.

00:14:18.505 --> 00:14:22.146
So, say the goal is to do better
on an intelligence test.

00:14:22.170 --> 00:14:26.064
You know, a computer
that's moderately good at that,

00:14:26.088 --> 00:14:28.597
you could try a million versions of that.

00:14:28.621 --> 00:14:30.711
You might find one that was better,

00:14:30.735 --> 00:14:32.739
and then, you know, repeat.

00:14:32.763 --> 00:14:35.803
And so the concern is that you get
some sort of runaway effect

00:14:35.827 --> 00:14:38.835
where everything is fine
on Thursday evening,

00:14:38.859 --> 00:14:41.195
and you come back into the lab
on Friday morning,

00:14:41.219 --> 00:14:43.668
and because of the speed
of computers and so forth,

00:14:43.692 --> 00:14:45.595
things have gone crazy, and suddenly --

00:14:45.619 --> 00:14:47.639
ST: I would say this is a possibility,

00:14:47.663 --> 00:14:49.579
but it's a very remote possibility.

00:14:49.603 --> 00:14:52.940
So let me just translate
what I heard you say.

00:14:52.964 --> 00:14:55.668
In the AlphaGo case,
we had exactly this thing:

00:14:55.692 --> 00:14:58.007
the computer would play
the game against itself

00:14:58.031 --> 00:14:59.281
and then learn new rules.

00:14:59.305 --> 00:15:02.540
And what machine learning is
is a rewriting of the rules.

00:15:02.564 --> 00:15:04.333
It's the rewriting of code.

00:15:04.357 --> 00:15:07.202
But I think there was
absolutely no concern

00:15:07.226 --> 00:15:09.652
that AlphaGo would take over the world.

00:15:09.676 --> 00:15:11.140
It can't even play chess.

00:15:11.164 --> 00:15:16.311
CA: No, no, no, but now,
these are all very single-domain things.

00:15:16.335 --> 00:15:19.214
But it's possible to imagine.

00:15:19.238 --> 00:15:22.327
I mean, we just saw a computer
that seemed nearly capable

00:15:22.351 --> 00:15:25.006
of passing a university entrance test,

00:15:25.030 --> 00:15:28.718
that can kind of -- it can't read
and understand in the sense that we can,

00:15:28.742 --> 00:15:30.729
but it can certainly absorb all the text

00:15:30.753 --> 00:15:33.652
and maybe see increased
patterns of meaning.

00:15:33.676 --> 00:15:37.370
Isn't there a chance that,
as this broadens out,

00:15:37.394 --> 00:15:39.860
there could be a different
kind of runaway effect?

00:15:39.884 --> 00:15:41.962
ST: That's where
I draw the line, honestly.

00:15:41.986 --> 00:15:44.629
And the chance exists --
I don't want to downplay it --

00:15:44.653 --> 00:15:48.325
but I think it's remote, and it's not
the thing that's on my mind these days,

00:15:48.349 --> 00:15:50.861
because I think the big revolution
is something else.

00:15:50.885 --> 00:15:53.807
Everything successful in AI
to the present date

00:15:53.831 --> 00:15:56.045
has been extremely specialized,

00:15:56.069 --> 00:15:58.558
and it's been thriving on a single idea,

00:15:58.582 --> 00:16:01.321
which is massive amounts of data.

00:16:01.345 --> 00:16:05.492
The reason AlphaGo works so well
is because of massive numbers of Go plays,

00:16:05.516 --> 00:16:08.771
and AlphaGo can't drive a car
or fly a plane.

00:16:08.795 --> 00:16:11.826
The Google self-driving car
or the Udacity self-driving car

00:16:11.850 --> 00:16:15.090
thrives on massive amounts of data,
and it can't do anything else.

00:16:15.114 --> 00:16:16.841
It can't even control a motorcycle.

00:16:16.865 --> 00:16:19.627
It's a very specific,
domain-specific function,

00:16:19.651 --> 00:16:21.558
and the same is true for our cancer app.

00:16:21.582 --> 00:16:24.818
There has been almost no progress
on this thing called "general AI,"

00:16:24.842 --> 00:16:28.842
where you go to an AI and say,
"Hey, invent for me special relativity

00:16:28.866 --> 00:16:30.532
or string theory."

00:16:30.556 --> 00:16:32.487
It's totally in the infancy.

00:16:32.511 --> 00:16:34.638
The reason I want to emphasize this,

00:16:34.662 --> 00:16:38.500
I see the concerns,
and I want to acknowledge them.

00:16:38.524 --> 00:16:41.410
But if I were to think about one thing,

00:16:41.434 --> 00:16:46.997
I would ask myself the question,
"What if we can take anything repetitive

00:16:47.021 --> 00:16:50.494
and make ourselves
100 times as efficient?"

00:16:51.170 --> 00:16:55.419
It so turns out, 300 years ago,
we all worked in agriculture

00:16:55.443 --> 00:16:57.494
and did farming and did repetitive things.

00:16:57.518 --> 00:17:00.074
Today, 75 percent of us work in offices

00:17:00.098 --> 00:17:02.222
and do repetitive things.

00:17:02.246 --> 00:17:04.429
We've become spreadsheet monkeys.

00:17:04.453 --> 00:17:06.507
And not just low-end labor.

00:17:06.531 --> 00:17:09.285
We've become dermatologists
doing repetitive things,

00:17:09.309 --> 00:17:11.058
lawyers doing repetitive things.

00:17:11.082 --> 00:17:14.905
I think we are at the brink
of being able to take an AI,

00:17:14.929 --> 00:17:16.647
look over our shoulders,

00:17:16.671 --> 00:17:20.729
and they make us maybe 10 or 50 times
as effective in these repetitive things.

00:17:20.753 --> 00:17:22.028
That's what is on my mind.

00:17:22.052 --> 00:17:24.502
CA: That sounds super exciting.

00:17:24.526 --> 00:17:28.056
The process of getting there seems
a little terrifying to some people,

00:17:28.080 --> 00:17:31.260
because once a computer
can do this repetitive thing

00:17:31.284 --> 00:17:34.718
much better than the dermatologist

00:17:34.742 --> 00:17:37.972
or than the driver, especially,
is the thing that's talked about

00:17:37.996 --> 00:17:39.286
so much now,

00:17:39.310 --> 00:17:41.268
suddenly millions of jobs go,

00:17:41.292 --> 00:17:43.987
and, you know, the country's in revolution

00:17:44.011 --> 00:17:48.340
before we ever get to the more
glorious aspects of what's possible.

00:17:48.364 --> 00:17:50.881
ST: Yeah, and that's an issue,
and it's a big issue,

00:17:50.905 --> 00:17:55.101
and it was pointed out yesterday morning
by several guest speakers.

00:17:55.125 --> 00:17:57.879
Now, prior to me showing up onstage,

00:17:57.903 --> 00:18:01.642
I confessed I'm a positive,
optimistic person,

00:18:01.666 --> 00:18:04.055
so let me give you an optimistic pitch,

00:18:04.079 --> 00:18:08.874
which is, think of yourself
back 300 years ago.

00:18:08.898 --> 00:18:12.894
Europe just survived 140 years
of continuous war,

00:18:12.918 --> 00:18:14.629
none of you could read or write,

00:18:14.653 --> 00:18:17.598
there were no jobs that you hold today,

00:18:17.622 --> 00:18:21.718
like investment banker
or software engineer or TV anchor.

00:18:21.742 --> 00:18:24.156
We would all be in the fields and farming.

00:18:24.180 --> 00:18:27.753
Now here comes little Sebastian
with a little steam engine in his pocket,

00:18:27.777 --> 00:18:29.325
saying, "Hey guys, look at this.

00:18:29.349 --> 00:18:32.944
It's going to make you 100 times
as strong, so you can do something else."

00:18:32.968 --> 00:18:35.438
And then back in the day,
there was no real stage,

00:18:35.462 --> 00:18:37.988
but Chris and I hang out
with the cows in the stable,

00:18:38.012 --> 00:18:40.112
and he says, "I'm really
concerned about it,

00:18:40.136 --> 00:18:43.788
because I milk my cow every day,
and what if the machine does this for me?"

00:18:43.812 --> 00:18:45.514
The reason why I mention this is,

00:18:46.360 --> 00:18:49.963
we're always good in acknowledging
past progress and the benefit of it,

00:18:49.987 --> 00:18:53.341
like our iPhones or our planes
or electricity or medical supply.

00:18:53.365 --> 00:18:57.610
We all love to live to 80,
which was impossible 300 years ago.

00:18:57.634 --> 00:19:01.790
But we kind of don't apply
the same rules to the future.

00:19:02.621 --> 00:19:05.828
So if I look at my own job as a CEO,

00:19:05.852 --> 00:19:08.992
I would say 90 percent
of my work is repetitive,

00:19:09.016 --> 00:19:10.367
I don't enjoy it,

00:19:10.391 --> 00:19:14.369
I spend about four hours per day
on stupid, repetitive email.

00:19:14.393 --> 00:19:18.034
And I'm burning to have something
that helps me get rid of this.

00:19:18.058 --> 00:19:19.216
Why?

00:19:19.240 --> 00:19:22.243
Because I believe all of us
are insanely creative;

00:19:22.731 --> 00:19:25.925
I think the TED community
more than anybody else.

00:19:25.949 --> 00:19:29.508
But even blue-collar workers;
I think you can go to your hotel maid

00:19:29.532 --> 00:19:31.934
and have a drink with him or her,

00:19:31.958 --> 00:19:34.675
and an hour later,
you find a creative idea.

00:19:34.699 --> 00:19:38.839
What this will empower
is to turn this creativity into action.

00:19:39.265 --> 00:19:42.707
Like, what if you could
build Google in a day?

00:19:43.221 --> 00:19:46.537
What if you could sit over beer
and invent the next Snapchat,

00:19:46.561 --> 00:19:47.726
whatever it is,

00:19:47.750 --> 00:19:49.937
and tomorrow morning it's up and running?

00:19:49.961 --> 00:19:51.734
And that is not science fiction.

00:19:51.758 --> 00:19:53.012
What's going to happen is,

00:19:53.036 --> 00:19:54.903
we are already in history.

00:19:54.927 --> 00:19:58.155
We've unleashed this amazing creativity

00:19:58.179 --> 00:19:59.790
by de-slaving us from farming

00:19:59.814 --> 00:20:03.177
and later, of course, from factory work

00:20:03.201 --> 00:20:06.363
and have invented so many things.

00:20:06.387 --> 00:20:08.565
It's going to be even better,
in my opinion.

00:20:08.589 --> 00:20:10.661
And there's going to be
great side effects.

00:20:10.685 --> 00:20:12.174
One of the side effects will be

00:20:12.198 --> 00:20:16.993
that things like food and medical supply
and education and shelter

00:20:17.017 --> 00:20:18.194
and transportation

00:20:18.218 --> 00:20:20.659
will all become much more
affordable to all of us,

00:20:20.683 --> 00:20:22.005
not just the rich people.

00:20:22.029 --> 00:20:23.211
CA: Hmm.

00:20:23.235 --> 00:20:27.576
So when Martin Ford argued, you know,
that this time it's different

00:20:27.600 --> 00:20:31.053
because the intelligence
that we've used in the past

00:20:31.077 --> 00:20:33.560
to find new ways to be

00:20:33.584 --> 00:20:35.863
will be matched at the same pace

00:20:35.887 --> 00:20:38.178
by computers taking over those things,

00:20:38.202 --> 00:20:41.280
what I hear you saying
is that, not completely,

00:20:41.304 --> 00:20:44.255
because of human creativity.

00:20:44.279 --> 00:20:48.064
Do you think that that's fundamentally
different from the kind of creativity

00:20:48.088 --> 00:20:50.784
that computers can do?

00:20:50.808 --> 00:20:55.242
ST: So, that's my firm
belief as an AI person --

00:20:55.266 --> 00:20:59.069
that I haven't seen
any real progress on creativity

00:20:59.949 --> 00:21:01.356
and out-of-the-box thinking.

00:21:01.380 --> 00:21:05.003
What I see right now -- and this is
really important for people to realize,

00:21:05.027 --> 00:21:07.930
because the word "artificial
intelligence" is so threatening,

00:21:07.954 --> 00:21:10.477
and then we have Steve Spielberg
tossing a movie in,

00:21:10.501 --> 00:21:12.914
where all of a sudden
the computer is our overlord,

00:21:12.938 --> 00:21:14.390
but it's really a technology.

00:21:14.414 --> 00:21:17.396
It's a technology that helps us
do repetitive things.

00:21:17.420 --> 00:21:20.333
And the progress has been
entirely on the repetitive end.

00:21:20.357 --> 00:21:22.585
It's been in legal document discovery.

00:21:22.609 --> 00:21:24.289
It's been contract drafting.

00:21:24.313 --> 00:21:28.536
It's been screening X-rays of your chest.

00:21:28.560 --> 00:21:30.333
And these things are so specialized,

00:21:30.357 --> 00:21:32.748
I don't see the big threat of humanity.

00:21:32.772 --> 00:21:34.566
In fact, we as people --

00:21:34.590 --> 00:21:36.975
I mean, let's face it:
we've become superhuman.

00:21:36.999 --> 00:21:38.763
We've made us superhuman.

00:21:38.787 --> 00:21:41.419
We can swim across
the Atlantic in 11 hours.

00:21:41.443 --> 00:21:43.517
We can take a device out of our pocket

00:21:43.541 --> 00:21:45.688
and shout all the way to Australia,

00:21:45.712 --> 00:21:48.312
and in real time, have that person
shouting back to us.

00:21:48.336 --> 00:21:51.960
That's physically not possible.
We're breaking the rules of physics.

00:21:51.984 --> 00:21:54.927
When this is said and done,
we're going to remember everything

00:21:54.951 --> 00:21:56.164
we've ever said and seen,

00:21:56.188 --> 00:21:57.684
you'll remember every person,

00:21:57.708 --> 00:22:00.334
which is good for me
in my early stages of Alzheimer's.

00:22:00.358 --> 00:22:02.035
Sorry, what was I saying? I forgot.

00:22:02.059 --> 00:22:03.637
CA: (Laughs)

00:22:03.661 --> 00:22:06.738
ST: We will probably have
an IQ of 1,000 or more.

00:22:06.762 --> 00:22:10.187
There will be no more
spelling classes for our kids,

00:22:10.211 --> 00:22:12.297
because there's no spelling issue anymore.

00:22:12.321 --> 00:22:14.153
There's no math issue anymore.

00:22:14.177 --> 00:22:17.687
And I think what really will happen
is that we can be super creative.

00:22:17.711 --> 00:22:19.568
And we are. We are creative.

00:22:19.592 --> 00:22:21.144
That's our secret weapon.

00:22:21.168 --> 00:22:23.321
CA: So the jobs that are getting lost,

00:22:23.345 --> 00:22:25.839
in a way, even though
it's going to be painful,

00:22:25.863 --> 00:22:27.910
humans are capable
of more than those jobs.

00:22:27.934 --> 00:22:29.152
This is the dream.

00:22:29.176 --> 00:22:33.423
The dream is that humans can rise
to just a new level of empowerment

00:22:33.447 --> 00:22:35.104
and discovery.

00:22:35.128 --> 00:22:36.580
That's the dream.

00:22:36.604 --> 00:22:38.247
ST: And think about this:

00:22:38.271 --> 00:22:40.292
if you look at the history of humanity,

00:22:40.316 --> 00:22:43.644
that might be whatever --
60-100,000 years old, give or take --

00:22:43.668 --> 00:22:47.394
almost everything that you cherish
in terms of invention,

00:22:47.418 --> 00:22:49.569
of technology, of things we've built,

00:22:49.593 --> 00:22:52.692
has been invented in the last 150 years.

00:22:53.756 --> 00:22:56.804
If you toss in the book and the wheel,
it's a little bit older.

00:22:56.828 --> 00:22:57.997
Or the axe.

00:22:58.021 --> 00:23:00.811
But your phone, your sneakers,

00:23:00.835 --> 00:23:04.386
these chairs, modern
manufacturing, penicillin --

00:23:04.410 --> 00:23:06.124
the things we cherish.

00:23:06.148 --> 00:23:09.806
Now, that to me means

00:23:09.830 --> 00:23:12.871
the next 150 years will find more things.

00:23:12.895 --> 00:23:17.049
In fact, the pace of invention
has gone up, not gone down, in my opinion.

00:23:17.073 --> 00:23:21.978
I believe only one percent of interesting
things have been invented yet. Right?

00:23:22.002 --> 00:23:23.990
We haven't cured cancer.

00:23:24.014 --> 00:23:27.732
We don't have flying cars -- yet.
Hopefully, I'll change this.

00:23:27.756 --> 00:23:31.013
That used to be an example
people laughed about. (Laughs)

00:23:31.037 --> 00:23:34.029
It's funny, isn't it?
Working secretly on flying cars.

00:23:34.053 --> 00:23:36.736
We don't live twice as long yet. OK?

00:23:36.760 --> 00:23:39.545
We don't have this magic
implant in our brain

00:23:39.569 --> 00:23:41.401
that gives us the information we want.

00:23:41.425 --> 00:23:42.951
And you might be appalled by it,

00:23:42.975 --> 00:23:45.419
but I promise you,
once you have it, you'll love it.

00:23:45.443 --> 00:23:46.609
I hope you will.

00:23:46.633 --> 00:23:48.542
It's a bit scary, I know.

00:23:48.566 --> 00:23:50.820
There are so many things
we haven't invented yet

00:23:50.844 --> 00:23:52.112
that I think we'll invent.

00:23:52.136 --> 00:23:53.442
We have no gravity shields.

00:23:53.466 --> 00:23:56.019
We can't beam ourselves
from one location to another.

00:23:56.043 --> 00:23:57.194
That sounds ridiculous,

00:23:57.218 --> 00:23:58.506
but about 200 years ago,

00:23:58.530 --> 00:24:01.197
experts were of the opinion
that flight wouldn't exist,

00:24:01.221 --> 00:24:02.545
even 120 years ago,

00:24:02.569 --> 00:24:05.151
and if you moved faster
than you could run,

00:24:05.175 --> 00:24:06.695
you would instantly die.

00:24:06.719 --> 00:24:10.288
So who says we are correct today
that you can't beam a person

00:24:10.312 --> 00:24:12.561
from here to Mars?

00:24:12.585 --> 00:24:14.154
CA: Sebastian, thank you so much

00:24:14.178 --> 00:24:16.860
for your incredibly inspiring vision
and your brilliance.

00:24:16.884 --> 00:24:18.207
Thank you, Sebastian Thrun.

00:24:18.231 --> 00:24:20.126
That was fantastic. (Applause)

