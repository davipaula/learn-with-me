WEBVTT
Kind: captions
Language: en

00:00:13.548 --> 00:00:16.064
No matter who you are or where you live,

00:00:16.088 --> 00:00:18.444
I'm guessing that you have
at least one relative

00:00:18.468 --> 00:00:20.802
that likes to forward those emails.

00:00:21.206 --> 00:00:23.106
You know the ones I'm talking about --

00:00:23.130 --> 00:00:25.984
the ones with dubious claims
or conspiracy videos.

00:00:26.315 --> 00:00:28.983
And you've probably
already muted them on Facebook

00:00:29.007 --> 00:00:31.355
for sharing social posts like this one.

00:00:31.379 --> 00:00:32.783
It's an image of a banana

00:00:32.807 --> 00:00:35.474
with a strange red cross
running through the center.

00:00:35.498 --> 00:00:37.637
And the text around it is warning people

00:00:37.661 --> 00:00:39.819
not to eat fruits that look like this,

00:00:39.843 --> 00:00:41.871
suggesting they've been
injected with blood

00:00:41.895 --> 00:00:44.025
contaminated with the HIV virus.

00:00:44.049 --> 00:00:46.652
And the social share message
above it simply says,

00:00:46.676 --> 00:00:48.857
"Please forward to save lives."

00:00:49.672 --> 00:00:52.966
Now, fact-checkers have been debunking
this one for years,

00:00:52.990 --> 00:00:55.799
but it's one of those rumors
that just won't die.

00:00:55.823 --> 00:00:57.094
A zombie rumor.

00:00:57.513 --> 00:00:59.606
And, of course, it's entirely false.

00:01:00.180 --> 00:01:03.139
It might be tempting to laugh
at an example like this, to say,

00:01:03.163 --> 00:01:05.047
"Well, who would believe this, anyway?"

00:01:05.419 --> 00:01:07.045
But the reason it's a zombie rumor

00:01:07.069 --> 00:01:10.958
is because it taps into people's
deepest fears about their own safety

00:01:10.982 --> 00:01:13.157
and that of the people they love.

00:01:13.783 --> 00:01:17.056
And if you spend as enough time
as I have looking at misinformation,

00:01:17.080 --> 00:01:19.500
you know that this is just
one example of many

00:01:19.524 --> 00:01:22.571
that taps into people's deepest
fears and vulnerabilities.

00:01:23.214 --> 00:01:27.583
Every day, across the world,
we see scores of new memes on Instagram

00:01:27.607 --> 00:01:30.646
encouraging parents
not to vaccinate their children.

00:01:30.670 --> 00:01:35.202
We see new videos on YouTube
explaining that climate change is a hoax.

00:01:35.226 --> 00:01:39.528
And across all platforms, we see
endless posts designed to demonize others

00:01:39.552 --> 00:01:43.053
on the basis of their race,
religion or sexuality.

00:01:44.314 --> 00:01:47.344
Welcome to one of the central
challenges of our time.

00:01:47.647 --> 00:01:51.672
How can we maintain an internet
with freedom of expression at the core,

00:01:51.696 --> 00:01:54.999
while also ensuring that the content
that's being disseminated

00:01:55.023 --> 00:01:58.909
doesn't cause irreparable harms
to our democracies, our communities

00:01:58.933 --> 00:02:01.171
and to our physical and mental well-being?

00:02:01.998 --> 00:02:04.085
Because we live in the information age,

00:02:04.109 --> 00:02:07.656
yet the central currency
upon which we all depend -- information --

00:02:07.680 --> 00:02:10.037
is no longer deemed entirely trustworthy

00:02:10.061 --> 00:02:12.389
and, at times, can appear
downright dangerous.

00:02:12.811 --> 00:02:16.748
This is thanks in part to the runaway
growth of social sharing platforms

00:02:16.772 --> 00:02:18.414
that allow us to scroll through,

00:02:18.438 --> 00:02:20.660
where lies and facts sit side by side,

00:02:20.684 --> 00:02:23.755
but with none of the traditional
signals of trustworthiness.

00:02:24.268 --> 00:02:27.887
And goodness -- our language around this
is horribly muddled.

00:02:27.911 --> 00:02:31.014
People are still obsessed
with the phrase "fake news,"

00:02:31.038 --> 00:02:33.569
despite the fact that
it's extraordinarily unhelpful

00:02:33.593 --> 00:02:37.053
and used to describe a number of things
that are actually very different:

00:02:37.077 --> 00:02:40.463
lies, rumors, hoaxes,
conspiracies, propaganda.

00:02:40.911 --> 00:02:43.823
And I really wish
we could stop using a phrase

00:02:43.847 --> 00:02:46.709
that's been co-opted by politicians
right around the world,

00:02:46.733 --> 00:02:48.204
from the left and the right,

00:02:48.228 --> 00:02:51.450
used as a weapon to attack
a free and independent press.

00:02:52.307 --> 00:02:57.009
(Applause)

00:02:57.033 --> 00:03:00.495
Because we need our professional
news media now more than ever.

00:03:00.882 --> 00:03:04.255
And besides, most of this content
doesn't even masquerade as news.

00:03:04.279 --> 00:03:06.921
It's memes, videos, social posts.

00:03:06.945 --> 00:03:10.398
And most of it is not fake;
it's misleading.

00:03:10.422 --> 00:03:13.437
We tend to fixate on what's true or false.

00:03:13.461 --> 00:03:17.493
But the biggest concern is actually
the weaponization of context.

00:03:18.855 --> 00:03:20.823
Because the most effective disinformation

00:03:20.847 --> 00:03:23.895
has always been that
which has a kernel of truth to it.

00:03:23.919 --> 00:03:27.395
Let's take this example
from London, from March 2017,

00:03:27.419 --> 00:03:28.959
a tweet that circulated widely

00:03:28.983 --> 00:03:32.570
in the aftermath of a terrorist incident
on Westminster Bridge.

00:03:32.594 --> 00:03:35.022
This is a genuine image, not fake.

00:03:35.046 --> 00:03:38.215
The woman who appears in the photograph
was interviewed afterwards,

00:03:38.239 --> 00:03:40.648
and she explained that
she was utterly traumatized.

00:03:40.672 --> 00:03:42.410
She was on the phone to a loved one,

00:03:42.434 --> 00:03:45.052
and she wasn't looking
at the victim out of respect.

00:03:45.076 --> 00:03:49.036
But it still was circulated widely
with this Islamophobic framing,

00:03:49.060 --> 00:03:52.106
with multiple hashtags,
including: #BanIslam.

00:03:52.425 --> 00:03:54.823
Now, if you worked at Twitter,
what would you do?

00:03:54.847 --> 00:03:57.409
Would you take that down,
or would you leave it up?

00:03:58.553 --> 00:04:01.982
My gut reaction, my emotional reaction,
is to take this down.

00:04:02.006 --> 00:04:04.148
I hate the framing of this image.

00:04:04.585 --> 00:04:06.973
But freedom of expression
is a human right,

00:04:06.997 --> 00:04:10.222
and if we start taking down speech
that makes us feel uncomfortable,

00:04:10.246 --> 00:04:11.476
we're in trouble.

00:04:11.500 --> 00:04:13.794
And this might look like a clear-cut case,

00:04:13.818 --> 00:04:15.516
but, actually, most speech isn't.

00:04:15.540 --> 00:04:17.976
These lines are incredibly
difficult to draw.

00:04:18.000 --> 00:04:20.281
What's a well-meaning
decision by one person

00:04:20.305 --> 00:04:22.382
is outright censorship to the next.

00:04:22.759 --> 00:04:25.688
What we now know is that
this account, Texas Lone Star,

00:04:25.712 --> 00:04:28.942
was part of a wider Russian
disinformation campaign,

00:04:28.966 --> 00:04:31.117
one that has since been taken down.

00:04:31.141 --> 00:04:32.704
Would that change your view?

00:04:33.322 --> 00:04:34.481
It would mine,

00:04:34.505 --> 00:04:36.806
because now it's a case
of a coordinated campaign

00:04:36.830 --> 00:04:38.045
to sow discord.

00:04:38.069 --> 00:04:40.030
And for those of you who'd like to think

00:04:40.054 --> 00:04:42.885
that artificial intelligence
will solve all of our problems,

00:04:42.909 --> 00:04:45.134
I think we can agree
that we're a long way away

00:04:45.158 --> 00:04:47.745
from AI that's able to make sense
of posts like this.

00:04:48.856 --> 00:04:51.363
So I'd like to explain
three interlocking issues

00:04:51.387 --> 00:04:53.760
that make this so complex

00:04:53.784 --> 00:04:56.906
and then think about some ways
we can consider these challenges.

00:04:57.348 --> 00:05:01.238
First, we just don't have
a rational relationship to information,

00:05:01.262 --> 00:05:02.730
we have an emotional one.

00:05:02.754 --> 00:05:06.548
It's just not true that more facts
will make everything OK,

00:05:06.572 --> 00:05:09.672
because the algorithms that determine
what content we see,

00:05:09.696 --> 00:05:12.823
well, they're designed to reward
our emotional responses.

00:05:12.847 --> 00:05:14.228
And when we're fearful,

00:05:14.252 --> 00:05:17.426
oversimplified narratives,
conspiratorial explanations

00:05:17.450 --> 00:05:20.868
and language that demonizes others
is far more effective.

00:05:21.538 --> 00:05:23.412
And besides, many of these companies,

00:05:23.436 --> 00:05:25.982
their business model
is attached to attention,

00:05:26.006 --> 00:05:29.696
which means these algorithms
will always be skewed towards emotion.

00:05:30.371 --> 00:05:34.669
Second, most of the speech
I'm talking about here is legal.

00:05:35.081 --> 00:05:36.527
It would be a different matter

00:05:36.551 --> 00:05:38.892
if I was talking about
child sexual abuse imagery

00:05:38.916 --> 00:05:40.843
or content that incites violence.

00:05:40.867 --> 00:05:44.137
It can be perfectly legal
to post an outright lie.

00:05:45.130 --> 00:05:49.164
But people keep talking about taking down
"problematic" or "harmful" content,

00:05:49.188 --> 00:05:51.797
but with no clear definition
of what they mean by that,

00:05:51.821 --> 00:05:53.085
including Mark Zuckerberg,

00:05:53.109 --> 00:05:56.521
who recently called for global
regulation to moderate speech.

00:05:56.870 --> 00:05:59.085
And my concern is that
we're seeing governments

00:05:59.109 --> 00:06:00.401
right around the world

00:06:00.425 --> 00:06:03.101
rolling out hasty policy decisions

00:06:03.125 --> 00:06:05.871
that might actually trigger
much more serious consequences

00:06:05.895 --> 00:06:07.609
when it comes to our speech.

00:06:08.006 --> 00:06:11.712
And even if we could decide
which speech to take up or take down,

00:06:11.736 --> 00:06:13.910
we've never had so much speech.

00:06:13.934 --> 00:06:16.065
Every second, millions
of pieces of content

00:06:16.089 --> 00:06:18.196
are uploaded by people
right around the world

00:06:18.220 --> 00:06:19.388
in different languages,

00:06:19.412 --> 00:06:22.180
drawing on thousands
of different cultural contexts.

00:06:22.204 --> 00:06:24.736
We've simply never had
effective mechanisms

00:06:24.760 --> 00:06:26.498
to moderate speech at this scale,

00:06:26.522 --> 00:06:29.323
whether powered by humans
or by technology.

00:06:30.284 --> 00:06:34.228
And third, these companies --
Google, Twitter, Facebook, WhatsApp --

00:06:34.252 --> 00:06:37.093
they're part of a wider
information ecosystem.

00:06:37.117 --> 00:06:40.469
We like to lay all the blame
at their feet, but the truth is,

00:06:40.493 --> 00:06:44.323
the mass media and elected officials
can also play an equal role

00:06:44.347 --> 00:06:47.260
in amplifying rumors and conspiracies
when they want to.

00:06:47.800 --> 00:06:52.744
As can we, when we mindlessly forward
divisive or misleading content

00:06:52.768 --> 00:06:54.053
without trying.

00:06:54.077 --> 00:06:55.877
We're adding to the pollution.

00:06:57.236 --> 00:06:59.854
I know we're all looking for an easy fix.

00:06:59.878 --> 00:07:01.545
But there just isn't one.

00:07:01.950 --> 00:07:06.395
Any solution will have to be rolled out
at a massive scale, internet scale,

00:07:06.419 --> 00:07:09.680
and yes, the platforms,
they're used to operating at that level.

00:07:09.704 --> 00:07:13.176
But can and should we allow them
to fix these problems?

00:07:13.668 --> 00:07:14.900
They're certainly trying.

00:07:14.924 --> 00:07:19.010
But most of us would agree that, actually,
we don't want global corporations

00:07:19.034 --> 00:07:21.366
to be the guardians of truth
and fairness online.

00:07:21.390 --> 00:07:23.927
And I also think the platforms
would agree with that.

00:07:24.257 --> 00:07:27.138
And at the moment,
they're marking their own homework.

00:07:27.162 --> 00:07:28.360
They like to tell us

00:07:28.384 --> 00:07:30.963
that the interventions
they're rolling out are working,

00:07:30.987 --> 00:07:33.527
but because they write
their own transparency reports,

00:07:33.551 --> 00:07:37.169
there's no way for us to independently
verify what's actually happening.

00:07:38.431 --> 00:07:41.773
(Applause)

00:07:41.797 --> 00:07:44.749
And let's also be clear
that most of the changes we see

00:07:44.773 --> 00:07:47.767
only happen after journalists
undertake an investigation

00:07:47.791 --> 00:07:49.402
and find evidence of bias

00:07:49.426 --> 00:07:52.255
or content that breaks
their community guidelines.

00:07:52.815 --> 00:07:57.410
So yes, these companies have to play
a really important role in this process,

00:07:57.434 --> 00:07:58.994
but they can't control it.

00:07:59.855 --> 00:08:01.373
So what about governments?

00:08:01.863 --> 00:08:04.959
Many people believe
that global regulation is our last hope

00:08:04.983 --> 00:08:07.863
in terms of cleaning up
our information ecosystem.

00:08:07.887 --> 00:08:11.053
But what I see are lawmakers
who are struggling to keep up to date

00:08:11.077 --> 00:08:13.418
with the rapid changes in technology.

00:08:13.442 --> 00:08:15.346
And worse, they're working in the dark,

00:08:15.370 --> 00:08:17.191
because they don't have access to data

00:08:17.215 --> 00:08:19.865
to understand what's happening
on these platforms.

00:08:20.260 --> 00:08:23.331
And anyway, which governments
would we trust to do this?

00:08:23.355 --> 00:08:26.125
We need a global response,
not a national one.

00:08:27.419 --> 00:08:29.696
So the missing link is us.

00:08:29.720 --> 00:08:32.843
It's those people who use
these technologies every day.

00:08:33.260 --> 00:08:37.851
Can we design a new infrastructure
to support quality information?

00:08:38.371 --> 00:08:39.601
Well, I believe we can,

00:08:39.625 --> 00:08:42.982
and I've got a few ideas about
what we might be able to actually do.

00:08:43.006 --> 00:08:46.109
So firstly, if we're serious
about bringing the public into this,

00:08:46.133 --> 00:08:48.514
can we take some inspiration
from Wikipedia?

00:08:48.538 --> 00:08:50.362
They've shown us what's possible.

00:08:50.386 --> 00:08:51.537
Yes, it's not perfect,

00:08:51.561 --> 00:08:54.195
but they've demonstrated
that with the right structures,

00:08:54.219 --> 00:08:56.854
with a global outlook
and lots and lots of transparency,

00:08:56.878 --> 00:08:59.974
you can build something
that will earn the trust of most people.

00:08:59.998 --> 00:09:03.160
Because we have to find a way
to tap into the collective wisdom

00:09:03.184 --> 00:09:05.493
and experience of all users.

00:09:05.517 --> 00:09:08.163
This is particularly the case
for women, people of color

00:09:08.187 --> 00:09:09.533
and underrepresented groups.

00:09:09.557 --> 00:09:10.723
Because guess what?

00:09:10.747 --> 00:09:13.482
They are experts when it comes
to hate and disinformation,

00:09:13.506 --> 00:09:17.022
because they have been the targets
of these campaigns for so long.

00:09:17.046 --> 00:09:19.396
And over the years,
they've been raising flags,

00:09:19.420 --> 00:09:21.085
and they haven't been listened to.

00:09:21.109 --> 00:09:22.389
This has got to change.

00:09:22.807 --> 00:09:27.133
So could we build a Wikipedia for trust?

00:09:27.157 --> 00:09:31.346
Could we find a way that users
can actually provide insights?

00:09:31.370 --> 00:09:35.067
They could offer insights around
difficult content-moderation decisions.

00:09:35.091 --> 00:09:36.554
They could provide feedback

00:09:36.578 --> 00:09:39.619
when platforms decide
they want to roll out new changes.

00:09:40.241 --> 00:09:44.403
Second, people's experiences
with the information is personalized.

00:09:44.427 --> 00:09:47.070
My Facebook news feed
is very different to yours.

00:09:47.094 --> 00:09:49.839
Your YouTube recommendations
are very different to mine.

00:09:49.863 --> 00:09:52.355
That makes it impossible for us
to actually examine

00:09:52.379 --> 00:09:54.402
what information people are seeing.

00:09:54.815 --> 00:09:56.204
So could we imagine

00:09:56.228 --> 00:10:01.006
developing some kind of centralized
open repository for anonymized data,

00:10:01.030 --> 00:10:03.894
with privacy and ethical
concerns built in?

00:10:04.220 --> 00:10:05.998
Because imagine what we would learn

00:10:06.022 --> 00:10:09.283
if we built out a global network
of concerned citizens

00:10:09.307 --> 00:10:12.601
who wanted to donate
their social data to science.

00:10:13.141 --> 00:10:14.863
Because we actually know very little

00:10:14.887 --> 00:10:17.768
about the long-term consequences
of hate and disinformation

00:10:17.792 --> 00:10:19.767
on people's attitudes and behaviors.

00:10:20.236 --> 00:10:21.403
And what we do know,

00:10:21.427 --> 00:10:23.620
most of that has been
carried out in the US,

00:10:23.644 --> 00:10:26.025
despite the fact that
this is a global problem.

00:10:26.049 --> 00:10:27.684
We need to work on that, too.

00:10:28.192 --> 00:10:29.342
And third,

00:10:29.366 --> 00:10:31.676
can we find a way to connect the dots?

00:10:31.700 --> 00:10:35.138
No one sector, let alone nonprofit,
start-up or government,

00:10:35.162 --> 00:10:36.584
is going to solve this.

00:10:36.608 --> 00:10:39.172
But there are very smart people
right around the world

00:10:39.196 --> 00:10:40.577
working on these challenges,

00:10:40.601 --> 00:10:44.177
from newsrooms, civil society,
academia, activist groups.

00:10:44.201 --> 00:10:46.099
And you can see some of them here.

00:10:46.123 --> 00:10:49.050
Some are building out indicators
of content credibility.

00:10:49.074 --> 00:10:50.320
Others are fact-checking,

00:10:50.344 --> 00:10:53.935
so that false claims, videos and images
can be down-ranked by the platforms.

00:10:53.959 --> 00:10:56.172
A nonprofit I helped
to found, First Draft,

00:10:56.196 --> 00:10:59.164
is working with normally competitive
newsrooms around the world

00:10:59.188 --> 00:11:02.691
to help them build out investigative,
collaborative programs.

00:11:03.231 --> 00:11:05.540
And Danny Hillis, a software architect,

00:11:05.564 --> 00:11:07.945
is designing a new system
called The Underlay,

00:11:07.969 --> 00:11:10.744
which will be a record
of all public statements of fact

00:11:10.768 --> 00:11:12.097
connected to their sources,

00:11:12.121 --> 00:11:15.775
so that people and algorithms
can better judge what is credible.

00:11:16.800 --> 00:11:20.156
And educators around the world
are testing different techniques

00:11:20.180 --> 00:11:23.664
for finding ways to make people
critical of the content they consume.

00:11:24.633 --> 00:11:27.774
All of these efforts are wonderful,
but they're working in silos,

00:11:27.798 --> 00:11:30.478
and many of them are woefully underfunded.

00:11:30.502 --> 00:11:32.555
There are also hundreds
of very smart people

00:11:32.579 --> 00:11:34.231
working inside these companies,

00:11:34.255 --> 00:11:36.580
but again, these efforts
can feel disjointed,

00:11:36.604 --> 00:11:40.541
because they're actually developing
different solutions to the same problems.

00:11:41.205 --> 00:11:43.474
How can we find a way
to bring people together

00:11:43.498 --> 00:11:46.776
in one physical location
for days or weeks at a time,

00:11:46.800 --> 00:11:49.196
so they can actually tackle
these problems together

00:11:49.220 --> 00:11:51.038
but from their different perspectives?

00:11:51.062 --> 00:11:52.402
So can we do this?

00:11:52.426 --> 00:11:55.665
Can we build out a coordinated,
ambitious response,

00:11:55.689 --> 00:11:59.398
one that matches the scale
and the complexity of the problem?

00:11:59.819 --> 00:12:01.192
I really think we can.

00:12:01.216 --> 00:12:04.175
Together, let's rebuild
our information commons.

00:12:04.819 --> 00:12:06.009
Thank you.

00:12:06.033 --> 00:12:09.761
(Applause)

