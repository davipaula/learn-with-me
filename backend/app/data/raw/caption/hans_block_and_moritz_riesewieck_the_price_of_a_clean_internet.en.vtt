WEBVTT
Kind: captions
Language: en

00:00:12.360 --> 00:00:15.581
[This talk contains mature content]

00:00:16.913 --> 00:00:21.365
Moritz Riesewieck: On March 23, 2013,

00:00:21.389 --> 00:00:25.446
users worldwide
discovered in their news feed

00:00:25.470 --> 00:00:30.533
a video of a young girl
being raped by an older man.

00:00:31.478 --> 00:00:35.334
Before this video
was removed from Facebook,

00:00:35.358 --> 00:00:39.974
it was already shared 16,000 times,

00:00:39.998 --> 00:00:43.640
and it was even liked 4,000 times.

00:00:45.268 --> 00:00:48.931
This video went viral
and infected the net.

00:00:49.873 --> 00:00:52.968
Hans Block: And that was the moment
we asked ourselves

00:00:52.992 --> 00:00:55.770
how could something like this
get on Facebook?

00:00:55.794 --> 00:01:00.196
And at the same time,
why don't we see such content more often?

00:01:00.220 --> 00:01:03.903
After all, there's a lot
of revolting material online,

00:01:03.927 --> 00:01:08.382
but why do we so rarely see such crap
on Facebook, Twitter or Google?

00:01:08.958 --> 00:01:11.199
MR: While image-recognition software

00:01:11.223 --> 00:01:15.506
can identify the outlines
of sexual organs,

00:01:15.530 --> 00:01:20.458
blood or naked skin in images and videos,

00:01:20.482 --> 00:01:26.022
it has immense difficulties
to distinguish pornographic content

00:01:26.046 --> 00:01:30.394
from holiday pictures, Adonis statues

00:01:30.418 --> 00:01:33.116
or breast-cancer screening campaigns.

00:01:33.140 --> 00:01:37.395
It can't distinguish
Romeo and Juliet dying onstage

00:01:37.419 --> 00:01:39.974
from a real knife attack.

00:01:39.998 --> 00:01:45.220
It can't distinguish satire
from propaganda

00:01:45.244 --> 00:01:49.212
or irony from hatred,
and so on and so forth.

00:01:50.077 --> 00:01:54.155
Therefore, humans are needed to decide

00:01:54.179 --> 00:01:58.180
which of the suspicious content
should be deleted,

00:01:58.204 --> 00:01:59.804
and which should remain.

00:02:00.909 --> 00:02:03.720
Humans whom we know almost nothing about,

00:02:03.744 --> 00:02:05.611
because they work in secret.

00:02:06.053 --> 00:02:07.958
They sign nondisclosure agreements,

00:02:07.982 --> 00:02:10.935
which prohibit them
from talking and sharing

00:02:10.959 --> 00:02:14.544
what they see on their screens
and what this work does to them.

00:02:14.887 --> 00:02:19.157
They are forced to use code words
in order to hide who they work for.

00:02:19.585 --> 00:02:22.490
They are monitored
by private security firms

00:02:22.514 --> 00:02:26.024
in order to ensure
that they don't talk to journalists.

00:02:26.048 --> 00:02:29.587
And they are threatened by fines
in case they speak.

00:02:30.421 --> 00:02:34.183
All of this sounds
like a weird crime story,

00:02:34.207 --> 00:02:35.537
but it's true.

00:02:35.561 --> 00:02:37.053
These people exist,

00:02:37.633 --> 00:02:41.814
and they are called content moderators.

00:02:42.942 --> 00:02:46.387
HB: We are the directors of the feature
documentary film "The Cleaners,"

00:02:46.411 --> 00:02:48.371
and we would like to take you

00:02:48.395 --> 00:02:50.982
to a world that many of you
may not know yet.

00:02:51.006 --> 00:02:53.139
Here's a short clip of our film.

00:02:58.639 --> 00:03:02.259
(Music)

00:03:04.400 --> 00:03:08.086
(Video) Moderator: I need to be anonymous,
because we have a contract signed.

00:03:09.784 --> 00:03:13.405
We are not allowed to declare
whom we are working with.

00:03:14.807 --> 00:03:16.570
The reason why I speak to you

00:03:16.594 --> 00:03:20.974
is because the world should know
that we are here.

00:03:22.544 --> 00:03:25.347
There is somebody
who is checking the social media.

00:03:26.317 --> 00:03:29.921
We are doing our best
to make this platform

00:03:29.945 --> 00:03:31.735
safe for all of them.

00:03:42.438 --> 00:03:43.588
Delete.

00:03:44.278 --> 00:03:45.572
Ignore.

00:03:45.596 --> 00:03:46.746
Delete.

00:03:47.279 --> 00:03:48.576
Ignore.

00:03:48.600 --> 00:03:49.751
Delete.

00:03:50.680 --> 00:03:51.831
Ignore.

00:03:51.855 --> 00:03:53.005
Ignore.

00:03:53.625 --> 00:03:54.775
Delete.

00:03:58.030 --> 00:04:00.007
HB: The so-called content moderators

00:04:00.031 --> 00:04:04.031
don't get their paychecks from Facebook,
Twitter or Google themselves,

00:04:04.055 --> 00:04:06.372
but from outsourcing firms
around the world

00:04:06.396 --> 00:04:08.463
in order to keep the wages low.

00:04:08.833 --> 00:04:10.800
Tens of thousands of young people

00:04:10.824 --> 00:04:14.037
looking at everything
we are not supposed to see.

00:04:14.061 --> 00:04:17.609
And we are talking about
decapitations, mutilations,

00:04:17.633 --> 00:04:21.329
executions, necrophilia,
torture, child abuse.

00:04:21.743 --> 00:04:24.017
Thousands of images in one shift --

00:04:24.041 --> 00:04:26.956
ignore, delete, day and night.

00:04:27.393 --> 00:04:30.791
And much of this work is done in Manila,

00:04:30.815 --> 00:04:34.117
where the analog toxic waste
from the Western world

00:04:34.141 --> 00:04:36.749
was transported for years
by container ships,

00:04:36.773 --> 00:04:40.776
now the digital waste is dumped there
via fiber-optic cable.

00:04:40.800 --> 00:04:43.847
And just as the so-called scavengers

00:04:43.871 --> 00:04:47.347
rummage through gigantic tips
on the edge of the city,

00:04:47.371 --> 00:04:52.204
the content moderators click their way
through an endless toxic ocean

00:04:52.228 --> 00:04:56.315
of images and videos and all manner
of intellectual garbage,

00:04:56.339 --> 00:04:58.641
so that we don't have to look at it.

00:04:58.665 --> 00:05:02.205
MR: But unlike the wounds
of the scavengers,

00:05:02.229 --> 00:05:05.680
those of the content moderators
remain invisible.

00:05:06.117 --> 00:05:09.197
Full of shocking and disturbing content,

00:05:09.221 --> 00:05:12.484
these pictures and videos
burrow into their memories

00:05:12.508 --> 00:05:15.953
where, at any time,
they can have unpredictable effects:

00:05:15.977 --> 00:05:19.334
eating disorders, loss of libido,

00:05:19.358 --> 00:05:22.617
anxiety disorders, alcoholism,

00:05:22.641 --> 00:05:25.553
depression, which can even
lead to suicide.

00:05:26.100 --> 00:05:28.545
The pictures and videos infect them,

00:05:28.569 --> 00:05:30.958
and often never let them go again.

00:05:30.982 --> 00:05:35.823
If they are unlucky, they develop
post-traumatic stress disorders,

00:05:35.847 --> 00:05:38.047
like soldiers after war missions.

00:05:39.445 --> 00:05:43.088
In our film, we tell the story
of a young man

00:05:43.112 --> 00:05:48.310
who had to monitor livestreams
of self-mutilations and suicide attempts,

00:05:48.334 --> 00:05:50.009
again and again,

00:05:50.033 --> 00:05:53.099
and who eventually
committed suicide himself.

00:05:53.787 --> 00:05:56.527
It's not an isolated case,
as we've been told.

00:05:57.184 --> 00:06:01.164
This is the price all of us pay

00:06:01.188 --> 00:06:06.515
for our so-called clean
and safe and "healthy"

00:06:06.539 --> 00:06:08.823
environments on social media.

00:06:10.482 --> 00:06:13.077
Never before in the history of mankind

00:06:13.101 --> 00:06:16.433
has it been easier to reach
millions of people around the globe

00:06:16.457 --> 00:06:18.124
in a few seconds.

00:06:18.148 --> 00:06:22.093
What is posted on social media
spreads so quickly,

00:06:22.117 --> 00:06:26.053
becomes viral and excites the minds
of people all around the globe.

00:06:26.450 --> 00:06:28.514
Before it is deleted,

00:06:28.538 --> 00:06:30.471
it is often already too late.

00:06:30.966 --> 00:06:33.196
Millions of people
have already been infected

00:06:33.220 --> 00:06:35.077
with hatred and anger,

00:06:35.101 --> 00:06:37.831
and they either become active online,

00:06:37.855 --> 00:06:40.998
by spreading or amplifying hatred,

00:06:41.022 --> 00:06:44.815
or they take to the streets
and take up arms.

00:06:45.236 --> 00:06:47.776
HB: Therefore, an army
of content moderators

00:06:47.800 --> 00:06:51.695
sit in front of a screen
to avoid new collateral damage.

00:06:52.434 --> 00:06:54.553
And they are deciding,
as soon as possible,

00:06:54.577 --> 00:06:58.672
whether the content
stays on the platform -- ignore;

00:06:58.696 --> 00:07:01.036
or disappears -- delete.

00:07:01.823 --> 00:07:04.450
But not every decision is as clear

00:07:04.474 --> 00:07:07.371
as the decision about a child-abuse video.

00:07:07.395 --> 00:07:10.172
What about controversial content,
ambivalent content,

00:07:10.196 --> 00:07:13.349
uploaded by civil rights activists
or citizen journalists?

00:07:14.048 --> 00:07:17.270
The content moderators
often decide on such cases

00:07:17.294 --> 00:07:20.027
at the same speed as the [clear] cases.

00:07:21.515 --> 00:07:24.174
MR: We will show you a video now,

00:07:24.198 --> 00:07:27.507
and we would like to ask you to decide:

00:07:27.531 --> 00:07:29.221
Would you delete it,

00:07:29.245 --> 00:07:31.046
or would you not delete it?

00:07:31.070 --> 00:07:32.737
(Video) (Air strike sounds)

00:07:33.100 --> 00:07:35.650
(Explosion)

00:07:40.076 --> 00:07:46.029
(People speaking in Arabic)

00:07:46.053 --> 00:07:48.283
MR: Yeah, we did some blurring for you.

00:07:49.196 --> 00:07:52.951
A child would potentially
be dangerously disturbed

00:07:52.975 --> 00:07:55.784
and extremely frightened by such content.

00:07:55.808 --> 00:07:58.105
So, you rather delete it?

00:07:59.610 --> 00:08:04.249
But what if this video could help
investigate the war crimes in Syria?

00:08:04.717 --> 00:08:07.884
What if nobody would have heard
about this air strike,

00:08:07.908 --> 00:08:11.646
because Facebook, YouTube, Twitter
would have decided to take it down?

00:08:12.895 --> 00:08:17.220
Airwars, a nongovernmental
organization based in London,

00:08:17.244 --> 00:08:20.141
tries to find those videos
as quickly as possible

00:08:20.165 --> 00:08:22.725
whenever they are uploaded
to social media,

00:08:22.749 --> 00:08:24.349
in order to archive them.

00:08:24.693 --> 00:08:27.526
Because they know, sooner or later,

00:08:27.550 --> 00:08:30.860
Facebook, YouTube, Twitter
would take such content down.

00:08:31.345 --> 00:08:33.553
People armed with their mobile phones

00:08:33.577 --> 00:08:37.776
can make visible what journalists
often do not have access to.

00:08:37.800 --> 00:08:40.863
Civil rights groups often
do not have any better option

00:08:40.887 --> 00:08:44.688
to quickly make their recordings
accessible to a large audience

00:08:44.712 --> 00:08:47.312
than by uploading them to social media.

00:08:47.950 --> 00:08:52.236
Wasn't this the empowering potential
the World Wide Web should have?

00:08:52.966 --> 00:08:54.926
Weren't these the dreams

00:08:54.950 --> 00:08:59.061
people in its early stages had
about the World Wide Web?

00:08:59.608 --> 00:09:02.403
Can't pictures and videos like these

00:09:02.427 --> 00:09:07.561
persuade people who have become
insensitive to facts

00:09:07.585 --> 00:09:08.735
to rethink?

00:09:09.917 --> 00:09:13.519
HB: But instead, everything
that might be disturbing is deleted.

00:09:13.543 --> 00:09:15.601
And there's a general shift in society.

00:09:15.625 --> 00:09:19.522
Media, for example, more and more often
use trigger warnings

00:09:19.546 --> 00:09:21.339
at the top of articles

00:09:21.363 --> 00:09:24.672
which some people may perceive
as offensive or troubling.

00:09:24.696 --> 00:09:28.610
Or more and more students
at universities in the United States

00:09:28.634 --> 00:09:31.451
demand the banishment of antique classics

00:09:31.475 --> 00:09:34.590
which depict sexual violence or assault
from the curriculum.

00:09:34.991 --> 00:09:37.324
But how far should we go with that?

00:09:37.875 --> 00:09:41.255
Physical integrity is guaranteed
as a human right

00:09:41.279 --> 00:09:43.079
in constitutions worldwide.

00:09:43.422 --> 00:09:47.176
In the Charter of Fundamental Rights
of the European Union,

00:09:47.200 --> 00:09:50.554
this right expressly applies
to mental integrity.

00:09:51.347 --> 00:09:54.005
But even if the potentially
traumatic effect

00:09:54.029 --> 00:09:56.855
of images and videos is hard to predict,

00:09:56.879 --> 00:09:58.836
do we want to become so cautious

00:09:58.860 --> 00:10:02.587
that we risk losing
social awareness of injustice?

00:10:03.203 --> 00:10:04.353
So what to do?

00:10:04.942 --> 00:10:07.934
Mark Zuckerberg recently stated
that in the future,

00:10:07.958 --> 00:10:11.760
the users, we, or almost everybody,

00:10:11.784 --> 00:10:14.045
will decide individually

00:10:14.069 --> 00:10:16.117
what they would like to see
on the platform,

00:10:16.141 --> 00:10:18.172
by personal filter settings.

00:10:18.196 --> 00:10:21.268
So everyone could easily claim
to remain undisturbed

00:10:21.292 --> 00:10:25.031
by images of war
or other violent conflicts, like ...

00:10:25.849 --> 00:10:30.295
MR: I'm the type of guy
who doesn't mind seeing breasts

00:10:30.319 --> 00:10:34.085
and I'm very interested in global warming,

00:10:34.109 --> 00:10:36.673
but I don't like war so much.

00:10:37.109 --> 00:10:38.831
HB: Yeah, I'm more the opposite,

00:10:38.855 --> 00:10:42.908
I have zero interest in naked breasts
or naked bodies at all.

00:10:43.209 --> 00:10:46.120
But why not guns? I like guns, yes.

00:10:46.901 --> 00:10:50.646
MR: Come on, if we don't share
a similar social consciousness,

00:10:50.670 --> 00:10:53.339
how shall we discuss social problems?

00:10:53.363 --> 00:10:55.760
How shall we call people to action?

00:10:55.784 --> 00:10:59.069
Even more isolated bubbles would emerge.

00:10:59.665 --> 00:11:02.896
One of the central questions is:
"How, in the future,

00:11:02.920 --> 00:11:07.823
freedom of expression will be weighed
against the people's need for protection."

00:11:08.441 --> 00:11:09.908
It's a matter of principle.

00:11:10.602 --> 00:11:14.850
Do we want to design
an either open or closed society

00:11:14.874 --> 00:11:16.513
for the digital space?

00:11:17.054 --> 00:11:22.966
At the heart of the matter
is "freedom versus security."

00:11:24.388 --> 00:11:28.872
Facebook has always wanted to be
a "healthy" platform.

00:11:28.896 --> 00:11:32.594
Above all, users should feel
safe and secure.

00:11:32.618 --> 00:11:34.738
It's the same choice of words

00:11:34.762 --> 00:11:37.720
the content moderators
in the Philippines used

00:11:37.744 --> 00:11:39.544
in a lot of our interviews.

00:11:40.188 --> 00:11:42.569
(Video) The world
that we are living in right now,

00:11:42.593 --> 00:11:44.759
I believe, is not really healthy.

00:11:44.783 --> 00:11:46.331
(Music)

00:11:46.355 --> 00:11:49.513
In this world, there is really
an evil who exists.

00:11:49.537 --> 00:11:52.775
(Music)

00:11:52.799 --> 00:11:54.862
We need to watch for it.

00:11:54.886 --> 00:11:56.768
(Music)

00:11:56.792 --> 00:12:00.042
We need to control it -- good or bad.

00:12:00.646 --> 00:12:07.646
(Music)

00:12:10.193 --> 00:12:14.296
[Look up, Young man! --God]

00:12:14.952 --> 00:12:19.230
MR: For the young content moderators
in the strictly Catholic Philippines,

00:12:19.254 --> 00:12:22.047
this is linked to a Christian mission.

00:12:22.833 --> 00:12:25.799
To counter the sins of the world

00:12:25.823 --> 00:12:27.997
which spread across the web.

00:12:28.641 --> 00:12:32.053
"Cleanliness is next to godliness,"

00:12:32.077 --> 00:12:35.511
is a saying everybody
in the Philippines knows.

00:12:36.035 --> 00:12:37.694
HB: And others motivate themselves

00:12:37.718 --> 00:12:41.449
by comparing themselves
with their president, Rodrigo Duterte.

00:12:41.837 --> 00:12:45.328
He has been ruling
the Philippines since 2016,

00:12:45.352 --> 00:12:49.345
and he won the election
with the promise: "I will clean up."

00:12:49.892 --> 00:12:53.209
And what that means is eliminating
all kinds of problems

00:12:53.233 --> 00:12:55.688
by literally killing people on the streets

00:12:55.712 --> 00:12:58.577
who are supposed to be criminals,
whatever that means.

00:12:58.601 --> 00:12:59.871
And since he was elected,

00:12:59.895 --> 00:13:03.331
an estimated 20,000 people
have been killed.

00:13:03.655 --> 00:13:06.156
And one moderator in our film says,

00:13:06.180 --> 00:13:08.235
"What Duterte does on the streets,

00:13:08.259 --> 00:13:09.973
I do for the internet."

00:13:10.934 --> 00:13:14.498
And here they are,
our self-proclaimed superheroes,

00:13:14.522 --> 00:13:17.498
who enforce law and order
in our digital world.

00:13:17.522 --> 00:13:19.903
They clean up,
they polish everything clean,

00:13:19.927 --> 00:13:22.260
they free us from everything evil.

00:13:22.284 --> 00:13:26.013
Tasks formerly reserved
to state authorities

00:13:26.037 --> 00:13:29.712
have been taken over
by college graduates in their early 20s,

00:13:29.736 --> 00:13:32.629
equipped with
three- to five-day training --

00:13:32.653 --> 00:13:34.589
this is the qualification --

00:13:34.613 --> 00:13:37.680
who work on nothing less
than the world's rescue.

00:13:38.756 --> 00:13:42.975
MR: National sovereignties
have been outsourced to private companies,

00:13:42.999 --> 00:13:47.007
and they pass on their
responsibilities to third parties.

00:13:47.031 --> 00:13:50.094
It's an outsourcing of the outsourcing
of the outsourcing,

00:13:50.118 --> 00:13:51.268
which takes place.

00:13:51.618 --> 00:13:53.014
With social networks,

00:13:53.038 --> 00:13:56.053
we are dealing with a completely
new infrastructure,

00:13:56.077 --> 00:13:57.593
with its own mechanisms,

00:13:57.617 --> 00:13:59.196
its own logic of action

00:13:59.220 --> 00:14:04.465
and therefore, also, its own new dangers,

00:14:04.489 --> 00:14:08.514
which had not yet existed
in the predigitalized public sphere.

00:14:08.538 --> 00:14:10.747
HB: When Mark Zuckerberg
was at the US Congress

00:14:10.771 --> 00:14:12.541
or at the European Parliament,

00:14:12.565 --> 00:14:15.200
he was confronted
with all kinds of critics.

00:14:15.224 --> 00:14:17.804
And his reaction was always the same:

00:14:18.501 --> 00:14:19.969
"We will fix that,

00:14:19.993 --> 00:14:22.544
and I will follow up on that
with my team."

00:14:23.167 --> 00:14:26.945
But such a debate shouldn't be held
in back rooms of Facebook,

00:14:26.969 --> 00:14:28.254
Twitter or Google --

00:14:28.278 --> 00:14:33.094
such a debate should be openly discussed
in new, cosmopolitan parliaments,

00:14:33.118 --> 00:14:37.978
in new institutions
that reflect the diversity of people

00:14:38.002 --> 00:14:42.544
contributing to a utopian project
of a global network.

00:14:42.568 --> 00:14:45.945
And while it may seem impossible
to consider the values

00:14:45.969 --> 00:14:48.211
of users worldwide,

00:14:48.235 --> 00:14:49.917
it's worth believing

00:14:49.941 --> 00:14:53.227
that there's more that connects us
than separates us.

00:14:53.624 --> 00:14:57.331
MR: Yeah, at a time
when populism is gaining strength,

00:14:57.355 --> 00:15:00.553
it becomes popular
to justify the symptoms,

00:15:00.577 --> 00:15:01.855
to eradicate them,

00:15:01.879 --> 00:15:03.767
to make them invisible.

00:15:04.919 --> 00:15:08.268
This ideology is spreading worldwide,

00:15:08.292 --> 00:15:11.077
analog as well as digital,

00:15:11.903 --> 00:15:15.395
and it's our duty to stop it

00:15:15.419 --> 00:15:17.045
before it's too late.

00:15:17.665 --> 00:15:21.649
The question of freedom and democracy

00:15:21.673 --> 00:15:24.640
must not only have these two options.

00:15:25.053 --> 00:15:26.219
HB: Delete.

00:15:26.243 --> 00:15:28.390
MR: Or ignore.

00:15:29.300 --> 00:15:30.897
HB: Thank you very much.

00:15:30.921 --> 00:15:36.190
(Applause)

