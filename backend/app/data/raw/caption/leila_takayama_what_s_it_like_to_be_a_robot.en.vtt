WEBVTT
Kind: captions
Language: en

00:00:12.760 --> 00:00:15.416
You only get one chance
to make a first impression,

00:00:15.440 --> 00:00:18.616
and that's true if you're a robot
as well as if you're a person.

00:00:18.640 --> 00:00:21.656
The first time that I met
one of these robots

00:00:21.680 --> 00:00:24.576
was at a place
called Willow Garage in 2008.

00:00:24.600 --> 00:00:27.616
When I went to visit there,
my host walked me into the building

00:00:27.640 --> 00:00:29.216
and we met this little guy.

00:00:29.240 --> 00:00:30.896
He was rolling into the hallway,

00:00:30.920 --> 00:00:32.736
came up to me, sat there,

00:00:32.760 --> 00:00:35.016
stared blankly past me,

00:00:35.040 --> 00:00:36.696
did nothing for a while,

00:00:36.720 --> 00:00:38.656
rapidly spun his head around 180 degrees

00:00:38.680 --> 00:00:40.216
and then ran away.

00:00:40.240 --> 00:00:42.416
And that was not a great first impression.

00:00:42.440 --> 00:00:44.616
The thing that I learned
about robots that day

00:00:44.640 --> 00:00:46.816
is that they kind of do their own thing,

00:00:46.840 --> 00:00:48.976
and they're not totally aware of us.

00:00:49.000 --> 00:00:52.239
And I think as we're experimenting
with these possible robot futures,

00:00:52.263 --> 00:00:54.936
we actually end up learning
a lot more about ourselves

00:00:54.960 --> 00:00:56.616
as opposed to just these machines.

00:00:56.640 --> 00:00:57.976
And what I learned that day

00:00:58.000 --> 00:01:01.416
was that I had pretty high
expectations for this little dude.

00:01:01.440 --> 00:01:04.616
He was not only supposed to be able
to navigate the physical world,

00:01:04.640 --> 00:01:07.296
but also be able
to navigate my social world --

00:01:07.320 --> 00:01:09.496
he's in my space; it's a personal robot.

00:01:09.520 --> 00:01:11.536
wWhy didn't it understand me?

00:01:11.560 --> 00:01:12.816
My host explained to me,

00:01:12.840 --> 00:01:16.016
"Well, the robot is trying
to get from point A to point B,

00:01:16.040 --> 00:01:17.816
and you were an obstacle in his way,

00:01:17.840 --> 00:01:19.856
so he had to replan his path,

00:01:19.880 --> 00:01:21.136
figure out where to go,

00:01:21.160 --> 00:01:22.856
and then get there some other way,"

00:01:22.880 --> 00:01:25.336
which was actually
not a very efficient thing to do.

00:01:25.360 --> 00:01:28.616
If that robot had figured out
that I was a person, not a chair,

00:01:28.640 --> 00:01:30.736
and that I was willing
to get out of its way

00:01:30.760 --> 00:01:32.416
if it was trying to get somewhere,

00:01:32.440 --> 00:01:34.656
then it actually
would have been more efficient

00:01:34.680 --> 00:01:35.936
at getting its job done

00:01:35.960 --> 00:01:38.176
if it had bothered
to notice that I was a human

00:01:38.200 --> 00:01:41.776
and that I have different affordances
than things like chairs and walls do.

00:01:41.800 --> 00:01:45.016
You know, we tend to think of these robots
as being from outer space

00:01:45.040 --> 00:01:47.176
and from the future
and from science fiction,

00:01:47.200 --> 00:01:48.616
and while that could be true,

00:01:48.640 --> 00:01:51.296
I'd actually like to argue
that robots are here today,

00:01:51.320 --> 00:01:54.096
and they live and work
amongst us right now.

00:01:54.120 --> 00:01:56.976
These are two robots that live in my home.

00:01:57.000 --> 00:01:59.496
They vacuum the floors
and they cut the grass

00:01:59.520 --> 00:02:00.736
every single day,

00:02:00.760 --> 00:02:04.136
which is more than I would do
if I actually had time to do these tasks,

00:02:04.160 --> 00:02:06.496
and they probably
do it better than I would, too.

00:02:06.520 --> 00:02:09.016
This one actually takes care of my kitty.

00:02:09.040 --> 00:02:11.616
Every single time
he uses the box, it cleans it,

00:02:11.640 --> 00:02:13.616
which is not something I'm willing to do,

00:02:13.640 --> 00:02:16.216
and it actually makes
his life better as well as mine.

00:02:16.240 --> 00:02:18.656
And while we call these robot products --

00:02:18.680 --> 00:02:21.376
it's a "robot vacuum cleaner,
it's a robot lawnmower,

00:02:21.400 --> 00:02:22.895
it's a robot littler box,"

00:02:22.919 --> 00:02:27.056
I think there's actually a bunch
of other robots hiding in plain sight

00:02:27.080 --> 00:02:28.936
that have just become so darn useful

00:02:28.960 --> 00:02:30.416
and so darn mundane

00:02:30.440 --> 00:02:32.936
that we call them things
like, "dishwasher," right?

00:02:32.960 --> 00:02:34.176
They get new names.

00:02:34.200 --> 00:02:35.896
They don't get called robot anymore

00:02:35.920 --> 00:02:38.336
because they actually
serve a purpose in our lives.

00:02:38.360 --> 00:02:39.856
Similarly, a thermostat, right?

00:02:39.880 --> 00:02:41.656
I know my robotics friends out there

00:02:41.680 --> 00:02:44.016
are probably cringing
at me calling this a robot,

00:02:44.040 --> 00:02:45.296
but it has a goal.

00:02:45.320 --> 00:02:48.216
Its goal is to make my house
66 degrees Fahrenheit,

00:02:48.240 --> 00:02:49.496
and it senses the world.

00:02:49.520 --> 00:02:51.096
It knows it's a little bit cold,

00:02:51.120 --> 00:02:53.736
it makes a plan and then
it acts on the physical world.

00:02:53.760 --> 00:02:55.016
It's robotics.

00:02:55.040 --> 00:02:57.616
Even if it might not
look like Rosie the Robot,

00:02:57.640 --> 00:03:00.576
it's doing something
that's really useful in my life

00:03:00.600 --> 00:03:01.976
so I don't have to take care

00:03:02.000 --> 00:03:04.576
of turning the temperature
up and down myself.

00:03:04.600 --> 00:03:08.416
And I think these systems
live and work amongst us now,

00:03:08.440 --> 00:03:10.776
and not only are these systems
living amongst us

00:03:10.800 --> 00:03:13.456
but you are probably
a robot operator, too.

00:03:13.480 --> 00:03:14.736
When you drive your car,

00:03:14.760 --> 00:03:16.976
it feels like you are operating machinery.

00:03:17.000 --> 00:03:19.816
You are also going
from point A to point B,

00:03:19.840 --> 00:03:22.056
but your car probably has power steering,

00:03:22.080 --> 00:03:24.776
it probably has automatic braking systems,

00:03:24.800 --> 00:03:28.536
it might have an automatic transmission
and maybe even adaptive cruise control.

00:03:28.560 --> 00:03:31.496
And while it might not be
a fully autonomous car,

00:03:31.520 --> 00:03:32.816
it has bits of autonomy,

00:03:32.840 --> 00:03:34.176
and they're so useful

00:03:34.200 --> 00:03:36.016
and they make us drive safer,

00:03:36.040 --> 00:03:39.696
and we just sort of feel
like they're invisible-in-use, right?

00:03:39.720 --> 00:03:41.296
So when you're driving your car,

00:03:41.320 --> 00:03:44.416
you should just feel like
you're going from one place to another.

00:03:44.440 --> 00:03:48.176
It doesn't feel like it's this big thing
that you have to deal with and operate

00:03:48.200 --> 00:03:49.456
and use these controls

00:03:49.480 --> 00:03:51.656
because we spent so long
learning how to drive

00:03:51.680 --> 00:03:54.376
that they've become
extensions of ourselves.

00:03:54.400 --> 00:03:57.096
When you park that car
in that tight little garage space,

00:03:57.120 --> 00:03:58.696
you know where your corners are.

00:03:58.720 --> 00:04:01.976
And when you drive a rental car
that maybe you haven't driven before,

00:04:02.000 --> 00:04:05.056
it takes some time
to get used to your new robot body.

00:04:05.080 --> 00:04:09.056
And this is also true for people
who operate other types of robots,

00:04:09.080 --> 00:04:11.680
so I'd like to share with you
a few stories about that.

00:04:12.240 --> 00:04:14.576
Dealing with the problem
of remote collaboration.

00:04:14.600 --> 00:04:17.176
So, at Willow Garage
I had a coworker named Dallas,

00:04:17.200 --> 00:04:18.776
and Dallas looked like this.

00:04:18.800 --> 00:04:22.856
He worked from his home in Indiana
in our company in California.

00:04:22.880 --> 00:04:25.816
He was a voice in a box
on the table in most of our meetings,

00:04:25.840 --> 00:04:28.055
which was kind of OK
except that, you know,

00:04:28.079 --> 00:04:31.456
if we had a really heated debate
and we didn't like what he was saying,

00:04:31.480 --> 00:04:32.896
we might just hang up on him.

00:04:32.920 --> 00:04:33.935
(Laughter)

00:04:33.959 --> 00:04:36.176
Then we might have a meeting
after that meeting

00:04:36.200 --> 00:04:38.896
and actually make the decisions
in the hallway afterwards

00:04:38.920 --> 00:04:40.336
when he wasn't there anymore.

00:04:40.360 --> 00:04:41.936
So that wasn't so great for him.

00:04:41.960 --> 00:04:43.696
And as a robotics company at Willow,

00:04:43.720 --> 00:04:46.056
we had some extra
robot body parts laying around,

00:04:46.080 --> 00:04:48.576
so Dallas and his buddy Curt
put together this thing,

00:04:48.600 --> 00:04:51.536
which looks kind of
like Skype on a stick on wheels,

00:04:51.560 --> 00:04:53.296
which seems like a techy, silly toy,

00:04:53.320 --> 00:04:56.096
but really it's probably
one of the most powerful tools

00:04:56.120 --> 00:04:58.600
that I've seen ever made
for remote collaboration.

00:04:59.160 --> 00:05:02.656
So now, if I didn't answer
Dallas' email question,

00:05:02.680 --> 00:05:04.896
he could literally roll into my office,

00:05:04.920 --> 00:05:07.496
block my doorway
and ask me the question again --

00:05:07.520 --> 00:05:08.536
(Laughter)

00:05:08.560 --> 00:05:09.776
until I answered it.

00:05:09.800 --> 00:05:12.776
And I'm not going to turn him off, right?
That's kind of rude.

00:05:12.800 --> 00:05:15.496
Not only was it good
for these one-on-one communications,

00:05:15.520 --> 00:05:18.456
but also for just showing up
at the company all-hands meeting.

00:05:18.480 --> 00:05:20.176
Getting your butt in that chair

00:05:20.200 --> 00:05:23.416
and showing people that you're present
and committed to your project

00:05:23.440 --> 00:05:24.696
is a big deal

00:05:24.720 --> 00:05:26.896
and can help remote collaboration a ton.

00:05:26.920 --> 00:05:29.776
We saw this over the period
of months and then years,

00:05:29.800 --> 00:05:31.960
not only at our company
but at others, too.

00:05:32.720 --> 00:05:35.056
The best thing that can happen
with these systems

00:05:35.080 --> 00:05:37.416
is that it starts to feel
like you're just there.

00:05:37.440 --> 00:05:39.136
It's just you, it's just your body,

00:05:39.160 --> 00:05:42.256
and so people actually start
to give these things personal space.

00:05:42.280 --> 00:05:44.256
So when you're having a stand-up meeting,

00:05:44.280 --> 00:05:45.936
people will stand around the space

00:05:45.960 --> 00:05:48.176
just as they would
if you were there in person.

00:05:48.200 --> 00:05:50.776
That's great until
there's breakdowns and it's not.

00:05:50.800 --> 00:05:52.776
People, when they first see these robots,

00:05:52.800 --> 00:05:56.376
are like, "Wow, where's the components?
There must be a camera over there,"

00:05:56.400 --> 00:05:57.976
and they start poking your face.

00:05:58.000 --> 00:06:00.936
"You're talking too softly,
I'm going to turn up your volume,"

00:06:00.960 --> 00:06:03.576
which is like having a coworker
walk up to you and say,

00:06:03.600 --> 00:06:06.496
"You're speaking too softly,
I'm going to turn up your face."

00:06:06.520 --> 00:06:07.776
That's awkward and not OK,

00:06:07.800 --> 00:06:10.416
and so we end up having to build
these new social norms

00:06:10.440 --> 00:06:12.656
around using these systems.

00:06:12.680 --> 00:06:16.096
Similarly, as you start
feeling like it's your body,

00:06:16.120 --> 00:06:19.816
you start noticing things like,
"Oh, my robot is kind of short."

00:06:19.840 --> 00:06:22.496
Dallas would say things to me --
he was six-foot tall --

00:06:22.520 --> 00:06:25.976
and we would take him via robot
to cocktail parties and things like that,

00:06:26.000 --> 00:06:27.216
as you do,

00:06:27.240 --> 00:06:30.576
and the robot was about five-foot-tall,
which is close to my height.

00:06:30.600 --> 00:06:31.816
And he would tell me,

00:06:31.840 --> 00:06:34.376
"You know, people are not
really looking at me.

00:06:34.400 --> 00:06:37.096
I feel like I'm just looking
at this sea of shoulders,

00:06:37.120 --> 00:06:39.096
and it's just -- we need a taller robot."

00:06:39.120 --> 00:06:40.376
And I told him,

00:06:40.400 --> 00:06:41.696
"Um, no.

00:06:41.720 --> 00:06:43.656
You get to walk in my shoes for today.

00:06:43.680 --> 00:06:47.216
You get to see what it's like
to be on the shorter end of the spectrum."

00:06:47.240 --> 00:06:50.616
And he actually ended up building
a lot of empathy for that experience,

00:06:50.640 --> 00:06:51.896
which was kind of great.

00:06:51.920 --> 00:06:53.576
So when he'd come visit in person,

00:06:53.600 --> 00:06:56.016
he no longer stood over me
as he was talking to me,

00:06:56.040 --> 00:06:58.136
he would sit down
and talk to me eye to eye,

00:06:58.160 --> 00:06:59.896
which was kind of a beautiful thing.

00:06:59.920 --> 00:07:02.576
So we actually decided
to look at this in the laboratory

00:07:02.600 --> 00:07:06.256
and see what others kinds of differences
things like robot height would make.

00:07:06.280 --> 00:07:09.136
And so half of the people in our study
used a shorter robot,

00:07:09.160 --> 00:07:11.576
half of the people in our study
used a taller robot

00:07:11.600 --> 00:07:13.856
and we actually found
that the exact same person

00:07:13.880 --> 00:07:17.216
who has the exact same body
and says the exact same things as someone,

00:07:17.240 --> 00:07:19.856
is more persuasive
and perceived as being more credible

00:07:19.880 --> 00:07:21.536
if they're in a taller robot form.

00:07:21.560 --> 00:07:23.376
It makes no rational sense,

00:07:23.400 --> 00:07:25.096
but that's why we study psychology.

00:07:25.120 --> 00:07:27.976
And really, you know,
the way that Cliff Nass would put this

00:07:28.000 --> 00:07:31.016
is that we're having to deal
with these new technologies

00:07:31.040 --> 00:07:33.776
despite the fact
that we have very old brains.

00:07:33.800 --> 00:07:36.776
Human psychology is not changing
at the same speed that tech is

00:07:36.800 --> 00:07:38.616
and so we're always playing catch-up,

00:07:38.640 --> 00:07:40.296
trying to make sense of this world

00:07:40.320 --> 00:07:42.656
where these autonomous things
are running around.

00:07:42.680 --> 00:07:45.416
Usually, things that talk are people,
not machines, right?

00:07:45.440 --> 00:07:50.016
And so we breathe a lot of meaning
into things like just height of a machine,

00:07:50.040 --> 00:07:51.296
not a person,

00:07:51.320 --> 00:07:53.680
and attribute that
to the person using the system.

00:07:55.120 --> 00:07:57.336
You know, this, I think,
is really important

00:07:57.360 --> 00:07:59.096
when you're thinking about robotics.

00:07:59.120 --> 00:08:01.216
It's not so much about reinventing humans,

00:08:01.240 --> 00:08:04.376
it's more about figuring out
how we extend ourselves, right?

00:08:04.400 --> 00:08:07.376
And we end up using things
in ways that are sort of surprising.

00:08:07.400 --> 00:08:11.656
So these guys can't play pool
because the robots don't have arms,

00:08:11.680 --> 00:08:14.016
but they can heckle the guys
who are playing pool

00:08:14.040 --> 00:08:17.216
and that can be an important thing
for team bonding,

00:08:17.240 --> 00:08:18.536
which is kind of neat.

00:08:18.560 --> 00:08:21.056
People who get really good
at operating these systems

00:08:21.080 --> 00:08:23.136
will even do things
like make up new games,

00:08:23.160 --> 00:08:25.296
like robot soccer
in the middle of the night,

00:08:25.320 --> 00:08:26.776
pushing the trash cans around.

00:08:26.800 --> 00:08:28.376
But not everyone's good.

00:08:28.400 --> 00:08:30.896
A lot of people have trouble
operating these systems.

00:08:30.920 --> 00:08:33.176
This is actually a guy
who logged into the robot

00:08:33.200 --> 00:08:35.576
and his eyeball was turned
90 degrees to the left.

00:08:35.600 --> 00:08:36.856
He didn't know that,

00:08:36.880 --> 00:08:39.056
so he ended up just bashing
around the office,

00:08:39.080 --> 00:08:41.696
running into people's desks,
getting super embarrassed,

00:08:41.720 --> 00:08:44.056
laughing about it --
his volume was way too high.

00:08:44.080 --> 00:08:46.216
And this guy here
in the image is telling me,

00:08:46.240 --> 00:08:48.336
"We need a robot mute button."

00:08:48.360 --> 00:08:51.856
And by that what he really meant
was we don't want it to be so disruptive.

00:08:51.880 --> 00:08:53.496
So as a robotics company,

00:08:53.520 --> 00:08:55.976
we added some obstacle
avoidance to the system.

00:08:56.000 --> 00:08:59.056
It got a little laser range finder
that could see the obstacles,

00:08:59.080 --> 00:09:02.216
and if I as a robot operator
try to say, run into a chair,

00:09:02.240 --> 00:09:04.736
it wouldn't let me,
it would just plan a path around,

00:09:04.760 --> 00:09:06.616
which seems like a good idea.

00:09:06.640 --> 00:09:09.816
People did hit fewer obstacles
using that system, obviously,

00:09:09.840 --> 00:09:11.936
but actually, for some of the people,

00:09:11.960 --> 00:09:14.816
it took them a lot longer
to get through our obstacle course,

00:09:14.840 --> 00:09:16.400
and we wanted to know why.

00:09:17.080 --> 00:09:20.136
It turns out that there's
this important human dimension --

00:09:20.160 --> 00:09:22.456
a personality dimension
called locus of control,

00:09:22.480 --> 00:09:25.616
and people who have
a strong internal locus of control,

00:09:25.640 --> 00:09:28.696
they need to be the masters
of their own destiny --

00:09:28.720 --> 00:09:31.816
really don't like giving up control
to an autonomous system --

00:09:31.840 --> 00:09:33.976
so much so that they will
fight the autonomy;

00:09:34.000 --> 00:09:37.096
"If I want to hit that chair,
I'm going to hit that chair."

00:09:37.120 --> 00:09:40.736
And so they would actually suffer
from having that autonomous assistance,

00:09:40.760 --> 00:09:43.336
which is an important thing for us to know

00:09:43.360 --> 00:09:46.656
as we're building increasingly
autonomous, say, cars, right?

00:09:46.680 --> 00:09:50.040
How are different people going
to grapple with that loss of control?

00:09:50.880 --> 00:09:53.576
It's going to be different
depending on human dimensions.

00:09:53.600 --> 00:09:57.096
We can't treat humans
as if we're just one monolithic thing.

00:09:57.120 --> 00:09:59.536
We vary by personality, by culture,

00:09:59.560 --> 00:10:02.016
we even vary by emotional state
moment to moment,

00:10:02.040 --> 00:10:04.016
and being able to design these systems,

00:10:04.040 --> 00:10:06.336
these human-robot interaction systems,

00:10:06.360 --> 00:10:09.096
we need to take into account
the human dimensions,

00:10:09.120 --> 00:10:10.840
not just the technological ones.

00:10:11.640 --> 00:10:15.936
Along with a sense of control
also comes a sense of responsibility.

00:10:15.960 --> 00:10:18.816
And if you were a robot operator
using one of these systems,

00:10:18.840 --> 00:10:20.896
this is what the interface
would look like.

00:10:20.920 --> 00:10:22.856
It looks a little bit like a video game,

00:10:22.880 --> 00:10:25.856
which can be good because
that's very familiar to people,

00:10:25.880 --> 00:10:27.096
but it can also be bad

00:10:27.120 --> 00:10:29.576
because it makes people feel
like it's a video game.

00:10:29.600 --> 00:10:32.456
We had a bunch of kids
over at Stanford play with the system

00:10:32.480 --> 00:10:34.936
and drive the robot
around our office in Menlo Park,

00:10:34.960 --> 00:10:36.896
and the kids started saying things like,

00:10:36.920 --> 00:10:40.096
"10 points if you hit that guy over there.
20 points for that one."

00:10:40.120 --> 00:10:42.136
And they would
chase them down the hallway.

00:10:42.160 --> 00:10:43.176
(Laughter)

00:10:43.200 --> 00:10:45.136
I told them, "Um, those are real people.

00:10:45.160 --> 00:10:48.456
They're actually going to bleed
and feel pain if you hit them."

00:10:48.480 --> 00:10:50.096
And they'd be like, "OK, got it."

00:10:50.120 --> 00:10:52.176
But five minutes later,
they would be like,

00:10:52.200 --> 00:10:55.816
"20 points for that guy over there,
he just looks like he needs to get hit."

00:10:55.840 --> 00:10:57.976
It's a little bit
like "Ender's Game," right?

00:10:58.000 --> 00:10:59.936
There is a real world on that other side

00:10:59.960 --> 00:11:03.376
and I think it's our responsibility
as people designing these interfaces

00:11:03.400 --> 00:11:04.656
to help people remember

00:11:04.680 --> 00:11:06.936
that there's real consequences
to their actions

00:11:06.960 --> 00:11:09.256
and to feel a sense of responsibility

00:11:09.280 --> 00:11:12.560
when they're operating
these increasingly autonomous things.

00:11:13.840 --> 00:11:16.136
These are kind of a great example

00:11:16.160 --> 00:11:19.416
of experimenting with one
possible robotic future,

00:11:19.440 --> 00:11:23.296
and I think it's pretty cool
that we can extend ourselves

00:11:23.320 --> 00:11:25.656
and learn about the ways
that we extend ourselves

00:11:25.680 --> 00:11:26.896
into these machines

00:11:26.920 --> 00:11:29.616
while at the same time
being able to express our humanity

00:11:29.640 --> 00:11:30.856
and our personality.

00:11:30.880 --> 00:11:32.456
We also build empathy for others

00:11:32.480 --> 00:11:35.696
in terms of being
shorter, taller, faster, slower,

00:11:35.720 --> 00:11:37.136
and maybe even armless,

00:11:37.160 --> 00:11:38.496
which is kind of neat.

00:11:38.520 --> 00:11:41.056
We also build empathy
for the robots themselves.

00:11:41.080 --> 00:11:42.736
This is one of my favorite robots.

00:11:42.760 --> 00:11:44.216
It's called the Tweenbot.

00:11:44.240 --> 00:11:46.216
And this guy has a little flag that says,

00:11:46.240 --> 00:11:48.816
"I'm trying to get
to this intersection in Manhattan,"

00:11:48.840 --> 00:11:51.616
and it's cute and rolls
forward, that's it.

00:11:51.640 --> 00:11:55.096
It doesn't know how to build a map,
it doesn't know how to see the world,

00:11:55.120 --> 00:11:56.376
it just asks for help.

00:11:56.400 --> 00:11:57.736
The nice thing about people

00:11:57.760 --> 00:12:00.856
is that it can actually depend
upon the kindness of strangers.

00:12:00.880 --> 00:12:04.776
It did make it across the park
to the other side of Manhattan --

00:12:04.800 --> 00:12:06.056
which is pretty great --

00:12:06.080 --> 00:12:09.536
just because people would pick it up
and point it in the right direction.

00:12:09.560 --> 00:12:10.496
(Laughter)

00:12:10.520 --> 00:12:11.776
And that's great, right?

00:12:11.800 --> 00:12:14.496
We're trying to build
this human-robot world

00:12:14.520 --> 00:12:17.936
in which we can coexist
and collaborate with one another,

00:12:17.960 --> 00:12:21.336
and we don't need to be fully autonomous
and just do things on our own.

00:12:21.360 --> 00:12:22.856
We actually do things together.

00:12:22.880 --> 00:12:24.136
And to make that happen,

00:12:24.160 --> 00:12:27.416
we actually need help from people
like the artists and the designers,

00:12:27.440 --> 00:12:29.296
the policy makers, the legal scholars,

00:12:29.320 --> 00:12:31.536
psychologists, sociologists,
anthropologists --

00:12:31.560 --> 00:12:33.376
we need more perspectives in the room

00:12:33.400 --> 00:12:36.376
if we're going to do the thing
that Stu Card says we should do,

00:12:36.400 --> 00:12:40.336
which is invent the future
that we actually want to live in.

00:12:40.360 --> 00:12:43.016
And I think we can continue to experiment

00:12:43.040 --> 00:12:45.216
with these different
robotic futures together,

00:12:45.240 --> 00:12:49.920
and in doing so, we will end up
learning a lot more about ourselves.

00:12:50.720 --> 00:12:51.936
Thank you.

00:12:51.960 --> 00:12:54.400
(Applause)

