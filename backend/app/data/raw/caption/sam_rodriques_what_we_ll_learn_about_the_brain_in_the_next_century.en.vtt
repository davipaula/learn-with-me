WEBVTT
Kind: captions
Language: en

00:00:13.040 --> 00:00:15.547
I want to tell you guys
something about neuroscience.

00:00:16.040 --> 00:00:17.840
I'm a physicist by training.

00:00:18.230 --> 00:00:20.436
About three years ago, I left physics

00:00:20.460 --> 00:00:22.809
to come and try to understand
how the brain works.

00:00:22.833 --> 00:00:24.307
And this is what I found.

00:00:24.331 --> 00:00:26.395
Lots of people are working on depression.

00:00:26.419 --> 00:00:27.578
And that's really good,

00:00:27.602 --> 00:00:30.323
depression is something
that we really want to understand.

00:00:30.347 --> 00:00:31.514
Here's how you do it:

00:00:31.538 --> 00:00:35.699
you take a jar and you fill it up,
about halfway, with water.

00:00:35.723 --> 00:00:39.905
And then you take a mouse,
and you put the mouse in the jar, OK?

00:00:39.929 --> 00:00:42.279
And the mouse swims around
for a little while

00:00:42.303 --> 00:00:44.691
and then at some point,
the mouse gets tired

00:00:44.715 --> 00:00:46.649
and decides to stop swimming.

00:00:46.673 --> 00:00:49.806
And when it stops swimming,
that's depression.

00:00:50.696 --> 00:00:51.846
OK?

00:00:52.291 --> 00:00:55.671
And I'm from theoretical physics,

00:00:55.695 --> 00:00:59.363
so I'm used to people making
very sophisticated mathematical models

00:00:59.387 --> 00:01:02.268
to precisely describe physical phenomena,

00:01:02.292 --> 00:01:04.744
so when I saw that this
is the model for depression,

00:01:04.768 --> 00:01:07.705
I though to myself, "Oh my God,
we have a lot of work to do."

00:01:07.729 --> 00:01:09.099
(Laughter)

00:01:09.123 --> 00:01:12.074
But this is a kind of general
problem in neuroscience.

00:01:12.377 --> 00:01:14.488
So for example, take emotion.

00:01:14.512 --> 00:01:16.971
Lots of people want to understand emotion.

00:01:17.352 --> 00:01:20.665
But you can't study emotion
in mice or monkeys

00:01:20.689 --> 00:01:21.943
because you can't ask them

00:01:21.967 --> 00:01:24.284
how they're feeling
or what they're experiencing.

00:01:24.308 --> 00:01:26.665
So instead, people who want
to understand emotion,

00:01:26.689 --> 00:01:29.466
typically end up studying
what's called motivated behavior,

00:01:29.490 --> 00:01:33.148
which is code for "what the mouse does
when it really, really wants cheese."

00:01:33.839 --> 00:01:35.514
OK, I could go on and on.

00:01:35.538 --> 00:01:41.854
I mean, the point is, the NIH
spends about 5.5 billion dollars a year

00:01:41.878 --> 00:01:43.410
on neuroscience research.

00:01:43.434 --> 00:01:47.037
And yet there have been almost
no significant improvements in outcomes

00:01:47.061 --> 00:01:50.552
for patients with brain diseases
in the past 40 years.

00:01:51.015 --> 00:01:53.555
And I think a lot of that
is basically due to the fact

00:01:53.579 --> 00:01:57.730
that mice might be OK as a model
for cancer or diabetes,

00:01:57.754 --> 00:02:00.441
but the mouse brain
is just not sophisticated enough

00:02:00.465 --> 00:02:03.640
to reproduce human psychology
or human brain disease.

00:02:04.379 --> 00:02:05.604
OK?

00:02:05.628 --> 00:02:09.262
So if the mouse models are so bad,
why are we still using them?

00:02:10.143 --> 00:02:12.246
Well, it basically boils down to this:

00:02:12.270 --> 00:02:14.826
the brain is made up of neurons

00:02:14.850 --> 00:02:18.297
which are these little cells that send
electrical signals to each other.

00:02:18.680 --> 00:02:20.824
If you want to understand
how the brain works,

00:02:20.848 --> 00:02:24.656
you have to be able to measure
the electrical activity of these neurons.

00:02:25.339 --> 00:02:28.331
But to do that, you have to get
really close to the neurons

00:02:28.355 --> 00:02:31.283
with some kind of electrical
recording device or a microscope.

00:02:31.563 --> 00:02:34.373
And so you can do that in mice
and you can do it in monkeys,

00:02:34.397 --> 00:02:36.945
because you can physically
put things into their brain

00:02:36.969 --> 00:02:40.015
but for some reason we still
can't do that in humans, OK?

00:02:40.533 --> 00:02:43.903
So instead, we've invented
all these proxies.

00:02:43.927 --> 00:02:46.442
So the most popular one is probably this,

00:02:46.466 --> 00:02:48.863
functional MRI, fMRI,

00:02:48.887 --> 00:02:51.579
which allows you to make these
pretty pictures like this,

00:02:51.603 --> 00:02:53.659
that show which parts
of your brain light up

00:02:53.683 --> 00:02:55.809
when you're engaged
in different activities.

00:02:55.833 --> 00:02:57.753
But this is a proxy.

00:02:57.777 --> 00:03:01.069
You're not actually measuring
neural activity here.

00:03:01.093 --> 00:03:03.935
What you're doing
is you're measuring, essentially,

00:03:03.959 --> 00:03:05.791
like, blood flow in the brain.

00:03:05.815 --> 00:03:07.053
Where there's more blood.

00:03:07.077 --> 00:03:10.180
It's actually where there's more oxygen,
but you get the idea, OK?

00:03:10.204 --> 00:03:12.723
The other thing that you can do
is you can do this --

00:03:12.747 --> 00:03:16.338
electroencephalography -- you can put
these electrodes on your head, OK?

00:03:16.362 --> 00:03:18.505
And then you can measure your brain waves.

00:03:19.125 --> 00:03:22.204
And here, you're actually measuring
electrical activity.

00:03:22.228 --> 00:03:24.593
But you're not measuring
the activity of neurons.

00:03:24.911 --> 00:03:27.355
You're measuring
these electrical currents,

00:03:27.379 --> 00:03:29.678
sloshing back and forth in your brain.

00:03:30.157 --> 00:03:32.831
So the point is just
that these technologies that we have

00:03:32.855 --> 00:03:35.291
are really measuring the wrong thing.

00:03:35.315 --> 00:03:38.268
Because, for most of the diseases
that we want to understand --

00:03:38.292 --> 00:03:40.490
like, Parkinson's is the classic example.

00:03:40.514 --> 00:03:44.068
In Parkinson's, there's one particular
kind of neuron deep in your brain

00:03:44.092 --> 00:03:45.823
that is responsible for the disease,

00:03:45.847 --> 00:03:49.029
and these technologies just don't have
the resolution that you need

00:03:49.053 --> 00:03:50.426
to get at that.

00:03:50.450 --> 00:03:54.424
And so that's why
we're still stuck with the animals.

00:03:54.448 --> 00:03:56.981
Not that anyone wants
to be studying depression

00:03:57.005 --> 00:03:59.267
by putting mice into jars, right?

00:03:59.291 --> 00:04:03.044
It's just that there's this pervasive
sense that it's not possible

00:04:03.068 --> 00:04:06.915
to look at the activity of neurons
in healthy humans.

00:04:08.180 --> 00:04:09.672
So here's what I want to do.

00:04:09.974 --> 00:04:12.495
I want to take you into the future.

00:04:12.519 --> 00:04:17.001
To have a look at one way in which
I think it could potentially be possible.

00:04:17.526 --> 00:04:20.824
And I want to preface this by saying,
I don't have all the details.

00:04:21.272 --> 00:04:24.239
So I'm just going to provide you
with a kind of outline.

00:04:24.263 --> 00:04:26.663
But we're going to go the year 2100.

00:04:27.732 --> 00:04:30.031
Now what does the year 2100 look like?

00:04:30.055 --> 00:04:33.573
Well, to start with, the climate
is a bit warmer that what you're used to.

00:04:33.597 --> 00:04:37.180
(Laughter)

00:04:37.204 --> 00:04:42.156
And that robotic vacuum cleaner
that you know and love

00:04:42.180 --> 00:04:43.694
went through a few generations,

00:04:43.718 --> 00:04:46.561
and the improvements
were not always so good.

00:04:46.585 --> 00:04:48.180
(Laughter)

00:04:48.530 --> 00:04:50.840
It was not always for the better.

00:04:52.221 --> 00:04:56.759
But actually, in the year 2100
most things are surprisingly recognizable.

00:04:57.458 --> 00:05:00.192
It's just the brain is totally different.

00:05:00.740 --> 00:05:03.287
For example, in the year 2100,

00:05:03.311 --> 00:05:06.168
we understand the root causes
of Alzheimer's.

00:05:06.192 --> 00:05:09.906
So we can deliver targeted
genetic therapies or drugs

00:05:09.930 --> 00:05:12.806
to stop the degenerative process
before it begins.

00:05:13.629 --> 00:05:14.962
So how did we do it?

00:05:15.898 --> 00:05:18.136
Well, there were essentially three steps.

00:05:18.589 --> 00:05:21.403
The first step was
that we had to figure out

00:05:21.427 --> 00:05:24.720
some way to get electrical
connections through the skull

00:05:24.744 --> 00:05:27.759
so we could measure
the electrical activity of neurons.

00:05:28.339 --> 00:05:32.688
And not only that,
it had to be easy and risk-free.

00:05:32.712 --> 00:05:35.090
Something that basically anyone
would be OK with,

00:05:35.114 --> 00:05:36.714
like getting a piercing.

00:05:37.156 --> 00:05:39.903
Because back in 2017,

00:05:39.927 --> 00:05:42.840
the only way that we knew of
to get through the skull

00:05:42.864 --> 00:05:45.681
was to drill these holes
the size of quarters.

00:05:46.015 --> 00:05:48.054
You would never let
someone do that to you.

00:05:48.967 --> 00:05:51.220
So in the 2020s,

00:05:51.244 --> 00:05:54.625
people began to experiment --
rather than drilling these gigantic holes,

00:05:54.649 --> 00:05:57.764
drilling microscopic holes,
no thicker than a piece of hair.

00:05:58.735 --> 00:06:00.831
And the idea here
was really for diagnosis --

00:06:00.855 --> 00:06:03.641
there are lots of times in the diagnosis
of brain disorders

00:06:03.665 --> 00:06:08.537
when you would like to be able to look
at the neural activity beneath the skull

00:06:08.561 --> 00:06:11.752
and being able to drill
these microscopic holes

00:06:11.776 --> 00:06:13.918
would make that much easier
for the patient.

00:06:13.942 --> 00:06:16.291
In the end, it would be
like getting a shot.

00:06:16.315 --> 00:06:17.895
You just go in and you sit down

00:06:17.919 --> 00:06:20.220
and there's a thing
that comes down on your head,

00:06:20.244 --> 00:06:22.197
and a momentary sting and then it's done,

00:06:22.221 --> 00:06:24.085
and you can go back about your day.

00:06:24.736 --> 00:06:29.529
So we're eventually able to do it

00:06:29.553 --> 00:06:32.220
using lasers to drill the holes.

00:06:32.244 --> 00:06:34.864
And with the lasers,
it was fast and extremely reliable,

00:06:34.888 --> 00:06:37.101
you couldn't even tell
the holes were there,

00:06:37.125 --> 00:06:40.125
any more than you could tell
that one of your hairs was missing.

00:06:40.753 --> 00:06:45.491
And I know it might sound crazy,
using lasers to drill holes in your skull,

00:06:45.515 --> 00:06:46.881
but back in 2017,

00:06:46.905 --> 00:06:51.014
people were OK with surgeons
shooting lasers into their eyes

00:06:51.038 --> 00:06:52.252
for corrective surgery

00:06:52.276 --> 00:06:56.163
So when you're already here,
it's not that big of a step.

00:06:57.561 --> 00:06:58.712
OK?

00:06:58.736 --> 00:07:02.307
So the next step,
that happened in the 2030s,

00:07:02.331 --> 00:07:05.417
was that it's not just about
getting through the skull.

00:07:05.441 --> 00:07:07.141
To measure the activity of neurons,

00:07:07.165 --> 00:07:10.990
you have to actually make it
into the brain tissue itself.

00:07:11.344 --> 00:07:14.312
And the risk, whenever
you put something into the brain tissue,

00:07:14.336 --> 00:07:15.775
is essentially that of stroke.

00:07:15.799 --> 00:07:17.995
That you would hit
a blood vessel and burst it,

00:07:18.019 --> 00:07:19.538
and that causes a stroke.

00:07:19.916 --> 00:07:23.641
So, by the mid 2030s,
we had invented these flexible probes

00:07:23.665 --> 00:07:25.943
that were capable of going
around blood vessels,

00:07:25.967 --> 00:07:27.443
rather than through them.

00:07:27.467 --> 00:07:33.164
And thus, we could put
huge batteries of these probes

00:07:33.188 --> 00:07:34.545
into the brains of patients

00:07:34.569 --> 00:07:37.839
and record from thousands of their neurons
without any risk to them.

00:07:39.458 --> 00:07:43.519
And what we discovered,
sort of to our surprise,

00:07:43.543 --> 00:07:45.733
is that the neurons that we could identify

00:07:45.757 --> 00:07:49.281
were not responding
to things like ideas or emotion,

00:07:49.305 --> 00:07:50.932
which was what we had expected.

00:07:50.956 --> 00:07:54.752
They were mostly responding
to things like Jennifer Aniston

00:07:54.776 --> 00:07:57.180
or Halle Berry

00:07:57.204 --> 00:07:58.514
or Justin Trudeau.

00:07:58.538 --> 00:07:59.791
I mean --

00:07:59.815 --> 00:08:02.141
(Laughter)

00:08:02.165 --> 00:08:04.602
In hindsight, we shouldn't
have been that surprised.

00:08:04.626 --> 00:08:07.888
I mean, what do your neurons
spend most of their time thinking about?

00:08:07.912 --> 00:08:09.062
(Laughter)

00:08:09.380 --> 00:08:11.420
But really, the point is that

00:08:11.444 --> 00:08:15.874
this technology enabled us to begin
studying neuroscience in individuals.

00:08:15.898 --> 00:08:20.128
So much like the transition to genetics,
at the single cell level,

00:08:20.152 --> 00:08:23.358
we started to study neuroscience,
at the single human level.

00:08:23.890 --> 00:08:25.508
But we weren't quite there yet.

00:08:25.895 --> 00:08:27.537
Because these technologies

00:08:27.561 --> 00:08:30.617
were still restricted
to medical applications,

00:08:30.641 --> 00:08:34.032
which meant that we were studying
sick brains, not healthy brains.

00:08:35.235 --> 00:08:38.989
Because no matter how safe
your technology is,

00:08:39.013 --> 00:08:41.743
you can't stick something
into someone's brain

00:08:41.767 --> 00:08:43.187
for research purposes.

00:08:43.211 --> 00:08:44.760
They have to want it.

00:08:44.784 --> 00:08:46.244
And why would they want it?

00:08:46.268 --> 00:08:49.839
Because as soon as you have
an electrical connection to the brain,

00:08:49.863 --> 00:08:52.307
you can use it to hook
the brain up to a computer.

00:08:53.061 --> 00:08:56.490
Oh, well, you know, the general public
was very skeptical at first.

00:08:56.514 --> 00:08:59.383
I mean, who wants to hook
their brain up to their computers?

00:08:59.926 --> 00:09:04.162
Well just imagine being able
to send an email with a thought.

00:09:04.186 --> 00:09:06.439
(Laughter)

00:09:06.463 --> 00:09:10.963
Imagine being able to take
a picture with your eyes, OK?

00:09:10.987 --> 00:09:12.217
(Laughter)

00:09:12.241 --> 00:09:15.204
Imagine never forgetting anything anymore,

00:09:15.228 --> 00:09:17.387
because anything
that you choose to remember

00:09:17.411 --> 00:09:19.888
will be stored permanently
on a hard drive somewhere,

00:09:19.912 --> 00:09:21.941
able to be recalled at will.

00:09:21.965 --> 00:09:25.331
(Laughter)

00:09:25.355 --> 00:09:28.736
The line here
between crazy and visionary

00:09:28.760 --> 00:09:30.227
was never quite clear.

00:09:30.720 --> 00:09:32.577
But the systems were safe.

00:09:32.879 --> 00:09:37.895
So when the FDA decided to deregulate
these laser-drilling systems, in 2043,

00:09:37.919 --> 00:09:40.276
commercial demand just exploded.

00:09:40.300 --> 00:09:42.188
People started signing their emails,

00:09:42.212 --> 00:09:43.553
"Please excuse any typos.

00:09:43.577 --> 00:09:44.910
Sent from my brain."

00:09:44.934 --> 00:09:45.935
(Laughter)

00:09:45.959 --> 00:09:48.031
Commercial systems
popped up left and right,

00:09:48.055 --> 00:09:51.293
offering the latest and greatest
in neural interfacing technology.

00:09:51.792 --> 00:09:53.545
There were 100 electrodes.

00:09:53.569 --> 00:09:55.480
A thousand electrodes.

00:09:55.504 --> 00:09:57.980
High bandwidth for only 99.99 a month.

00:09:58.004 --> 00:09:59.543
(Laughter)

00:09:59.567 --> 00:10:01.101
Soon, everyone had them.

00:10:01.694 --> 00:10:03.265
And that was the key.

00:10:03.289 --> 00:10:06.212
Because, in the 2050s,
if you were a neuroscientist,

00:10:06.236 --> 00:10:10.175
you could have someone come into your lab
essentially from off the street.

00:10:10.792 --> 00:10:13.656
And you could have them engaged
in some emotional task

00:10:13.680 --> 00:10:16.117
or social behavior or abstract reasoning,

00:10:16.141 --> 00:10:18.672
things you could never study in mice.

00:10:18.696 --> 00:10:21.807
And you could record
the activity of their neurons

00:10:21.831 --> 00:10:25.022
using the interfaces
that they already had.

00:10:25.046 --> 00:10:28.235
And then you could also ask them
about what they were experiencing.

00:10:28.259 --> 00:10:31.608
So this link between
psychology and neuroscience

00:10:31.632 --> 00:10:35.013
that you could never make
in the animals, was suddenly there.

00:10:35.695 --> 00:10:37.879
So perhaps the classic example of this

00:10:37.903 --> 00:10:41.426
was the discovery
of the neural basis for insight.

00:10:41.450 --> 00:10:45.050
That "Aha!" moment, the moment
it all comes together, it clicks.

00:10:45.593 --> 00:10:49.649
And this was discovered
by two scientists in 2055,

00:10:49.673 --> 00:10:51.045
Barry and Late,

00:10:51.069 --> 00:10:54.732
who observed, in the dorsal
prefrontal cortex,

00:10:54.756 --> 00:10:59.978
how in the brain of someone
trying to understand an idea,

00:11:00.002 --> 00:11:03.371
how different populations of neurons
would reorganize themselves --

00:11:03.395 --> 00:11:05.831
you're looking at neural
activity here in orange --

00:11:05.855 --> 00:11:09.593
until finally their activity aligns
in a way that leads to positive feedback.

00:11:10.339 --> 00:11:11.489
Right there.

00:11:12.723 --> 00:11:14.190
That is understanding.

00:11:15.413 --> 00:11:19.850
So finally, we were able to get
at the things that make us human.

00:11:21.871 --> 00:11:26.449
And that's what really opened the way
to major insights from medicine.

00:11:27.465 --> 00:11:30.220
Because, starting in the 2060s,

00:11:30.244 --> 00:11:32.728
with the ability to record
the neural activity

00:11:32.752 --> 00:11:36.339
in the brains of patients
with these different mental diseases,

00:11:36.363 --> 00:11:41.053
rather than defining the diseases
on the basis of their symptoms,

00:11:41.077 --> 00:11:43.117
as we had at the beginning of the century,

00:11:43.141 --> 00:11:44.363
we started to define them

00:11:44.387 --> 00:11:47.926
on the basis of the actual pathology
that we observed at the neural level.

00:11:48.768 --> 00:11:52.593
So for example, in the case of ADHD,

00:11:52.617 --> 00:11:55.791
we discovered that there are
dozens of different diseases,

00:11:55.815 --> 00:11:58.824
all of which had been called ADHD
at the start of the century,

00:11:58.848 --> 00:12:01.149
that actually had nothing
to do with each other,

00:12:01.173 --> 00:12:03.291
except that they had similar symptoms.

00:12:03.625 --> 00:12:05.997
And they needed to be treated
in different ways.

00:12:06.307 --> 00:12:08.554
So it was kind of incredible,
in retrospect,

00:12:08.578 --> 00:12:10.355
that at the beginning of the century,

00:12:10.379 --> 00:12:12.696
we had been treating
all those different diseases

00:12:12.720 --> 00:12:13.903
with the same drug,

00:12:13.927 --> 00:12:17.141
just by giving people amphetamine,
basically is what we were doing.

00:12:17.165 --> 00:12:19.653
And schizophrenia and depression
are the same way.

00:12:19.677 --> 00:12:23.709
So rather than prescribing drugs to people
essentially at random,

00:12:23.733 --> 00:12:24.883
as we had,

00:12:24.907 --> 00:12:28.418
we learned how to predict
which drugs would be most effective

00:12:28.442 --> 00:12:29.625
in which patients,

00:12:29.649 --> 00:12:32.405
and that just led to this huge
improvement in outcomes.

00:12:33.498 --> 00:12:36.974
OK, I want to bring you back now
to the year 2017.

00:12:38.117 --> 00:12:41.490
Some of this may sound satirical
or even far fetched.

00:12:41.514 --> 00:12:42.807
And some of it is.

00:12:43.291 --> 00:12:45.942
I mean, I can't actually
see into the future.

00:12:45.966 --> 00:12:47.332
I don't actually know

00:12:47.356 --> 00:12:51.023
if we're going to be drilling hundreds
or thousands of microscopic holes

00:12:51.047 --> 00:12:52.714
in our heads in 30 years.

00:12:53.762 --> 00:12:55.468
But what I can tell you

00:12:55.492 --> 00:12:57.667
is that we're not going
to make any progress

00:12:57.691 --> 00:13:01.418
towards understanding the human brain
or human diseases

00:13:01.442 --> 00:13:05.958
until we figure out how to get
at the electrical activity of neurons

00:13:05.982 --> 00:13:07.182
in healthy humans.

00:13:07.918 --> 00:13:11.157
And almost no one is working
on figuring out how to do that today.

00:13:12.077 --> 00:13:14.411
That is the future of neuroscience.

00:13:14.752 --> 00:13:19.145
And I think it's time for neuroscientists
to put down the mouse brain

00:13:19.169 --> 00:13:21.923
and to dedicate the thought
and investment necessary

00:13:21.947 --> 00:13:25.214
to understand the human brain
and human disease.

00:13:27.629 --> 00:13:28.780
Thank you.

00:13:28.804 --> 00:13:29.976
(Applause)

