WEBVTT
Kind: captions
Language: en

00:00:12.777 --> 00:00:13.960
I'm James.

00:00:13.984 --> 00:00:15.670
I'm a writer and artist,

00:00:15.694 --> 00:00:18.035
and I make work about technology.

00:00:18.454 --> 00:00:22.365
I do things like draw life-size outlines
of military drones

00:00:22.389 --> 00:00:24.217
in city streets around the world,

00:00:24.241 --> 00:00:27.191
so that people can start to think
and get their heads around

00:00:27.215 --> 00:00:30.655
these really quite hard-to-see
and hard-to-think-about technologies.

00:00:31.494 --> 00:00:35.330
I make things like neural networks
that predict the results of elections

00:00:35.354 --> 00:00:37.091
based on weather reports,

00:00:37.115 --> 00:00:38.429
because I'm intrigued about

00:00:38.453 --> 00:00:42.377
what the actual possibilities
of these weird new technologies are.

00:00:43.405 --> 00:00:45.831
Last year, I built
my own self-driving car.

00:00:45.855 --> 00:00:48.381
But because I don't
really trust technology,

00:00:48.405 --> 00:00:50.333
I also designed a trap for it.

00:00:50.777 --> 00:00:51.863
(Laughter)

00:00:51.887 --> 00:00:56.185
And I do these things mostly because
I find them completely fascinating,

00:00:56.209 --> 00:00:58.811
but also because I think
when we talk about technology,

00:00:58.835 --> 00:01:01.454
we're largely talking about ourselves

00:01:01.478 --> 00:01:03.777
and the way that we understand the world.

00:01:03.801 --> 00:01:06.243
So here's a story about technology.

00:01:07.520 --> 00:01:10.350
This is a "surprise egg" video.

00:01:10.374 --> 00:01:13.722
It's basically a video of someone
opening up loads of chocolate eggs

00:01:13.746 --> 00:01:15.872
and showing the toys inside to the viewer.

00:01:16.461 --> 00:01:19.110
That's it. That's all it does
for seven long minutes.

00:01:19.428 --> 00:01:22.479
And I want you to notice
two things about this.

00:01:22.503 --> 00:01:26.577
First of all, this video
has 30 million views.

00:01:26.601 --> 00:01:27.876
(Laughter)

00:01:28.376 --> 00:01:29.542
And the other thing is,

00:01:29.566 --> 00:01:33.435
it comes from a channel
that has 6.3 million subscribers,

00:01:33.459 --> 00:01:36.139
that has a total of eight billion views,

00:01:36.163 --> 00:01:39.269
and it's all just more videos like this --

00:01:40.256 --> 00:01:44.164
30 million people watching a guy
opening up these eggs.

00:01:44.188 --> 00:01:48.669
It sounds pretty weird, but if you search
for "surprise eggs" on YouTube,

00:01:48.693 --> 00:01:52.216
it'll tell you there's
10 million of these videos,

00:01:52.240 --> 00:01:53.897
and I think that's an undercount.

00:01:53.921 --> 00:01:55.818
I think there's way, way more of these.

00:01:55.842 --> 00:01:58.084
If you keep searching, they're endless.

00:01:58.108 --> 00:02:00.267
There's millions and millions
of these videos

00:02:00.291 --> 00:02:03.745
in increasingly baroque combinations
of brands and materials,

00:02:03.769 --> 00:02:07.615
and there's more and more of them
being uploaded every single day.

00:02:07.639 --> 00:02:11.150
Like, this is a strange world. Right?

00:02:11.174 --> 00:02:14.557
But the thing is, it's not adults
who are watching these videos.

00:02:14.581 --> 00:02:17.502
It's kids, small children.

00:02:17.526 --> 00:02:19.680
These videos are
like crack for little kids.

00:02:19.704 --> 00:02:21.779
There's something about the repetition,

00:02:21.803 --> 00:02:24.271
the constant little
dopamine hit of the reveal,

00:02:24.295 --> 00:02:26.161
that completely hooks them in.

00:02:26.185 --> 00:02:30.994
And little kids watch these videos
over and over and over again,

00:02:31.018 --> 00:02:33.345
and they do it for hours
and hours and hours.

00:02:33.369 --> 00:02:35.725
And if you try and take
the screen away from them,

00:02:35.749 --> 00:02:37.531
they'll scream and scream and scream.

00:02:37.555 --> 00:02:38.817
If you don't believe me --

00:02:38.841 --> 00:02:41.448
and I've already seen people
in the audience nodding --

00:02:41.472 --> 00:02:44.863
if you don't believe me, find someone
with small children and ask them,

00:02:44.887 --> 00:02:47.227
and they'll know about
the surprise egg videos.

00:02:47.251 --> 00:02:49.321
So this is where we start.

00:02:49.345 --> 00:02:52.987
It's 2018, and someone, or lots of people,

00:02:53.011 --> 00:02:56.952
are using the same mechanism that, like,
Facebook and Instagram are using

00:02:56.976 --> 00:02:58.965
to get you to keep checking that app,

00:02:58.989 --> 00:03:02.974
and they're using it on YouTube
to hack the brains of very small children

00:03:02.998 --> 00:03:04.956
in return for advertising revenue.

00:03:06.346 --> 00:03:08.347
At least, I hope
that's what they're doing.

00:03:08.371 --> 00:03:10.326
I hope that's what they're doing it for,

00:03:10.350 --> 00:03:15.658
because there's easier ways
of making ad revenue on YouTube.

00:03:15.682 --> 00:03:18.014
You can just make stuff up or steal stuff.

00:03:18.038 --> 00:03:20.673
So if you search for really
popular kids' cartoons

00:03:20.697 --> 00:03:22.351
like "Peppa Pig" or "Paw Patrol,"

00:03:22.375 --> 00:03:25.522
you'll find there's millions and millions
of these online as well.

00:03:25.546 --> 00:03:28.898
Of course, most of them aren't posted
by the original content creators.

00:03:28.922 --> 00:03:31.921
They come from loads and loads
of different random accounts,

00:03:31.945 --> 00:03:34.185
and it's impossible to know
who's posting them

00:03:34.209 --> 00:03:36.031
or what their motives might be.

00:03:36.428 --> 00:03:38.358
Does that sound kind of familiar?

00:03:38.382 --> 00:03:40.362
Because it's exactly the same mechanism

00:03:40.386 --> 00:03:42.986
that's happening across most
of our digital services,

00:03:43.010 --> 00:03:46.217
where it's impossible to know
where this information is coming from.

00:03:46.241 --> 00:03:48.070
It's basically fake news for kids,

00:03:48.094 --> 00:03:50.255
and we're training them from birth

00:03:50.279 --> 00:03:52.785
to click on the very first link
that comes along,

00:03:52.809 --> 00:03:54.762
regardless of what the source is.

00:03:54.786 --> 00:03:57.389
That's doesn't seem like
a terribly good idea.

00:03:58.399 --> 00:04:01.109
Here's another thing
that's really big on kids' YouTube.

00:04:01.133 --> 00:04:03.061
This is called the "Finger Family Song."

00:04:03.085 --> 00:04:05.103
I just heard someone groan
in the audience.

00:04:05.127 --> 00:04:06.751
This is the "Finger Family Song."

00:04:06.775 --> 00:04:08.705
This is the very first one I could find.

00:04:08.729 --> 00:04:11.558
It's from 2007, and it only has
200,000 views,

00:04:11.582 --> 00:04:13.558
which is, like, nothing in this game.

00:04:13.582 --> 00:04:16.434
But it has this insanely earwormy tune,

00:04:16.458 --> 00:04:18.140
which I'm not going to play to you,

00:04:18.164 --> 00:04:20.172
because it will sear itself
into your brain

00:04:20.196 --> 00:04:22.591
in the same way that
it seared itself into mine,

00:04:22.615 --> 00:04:24.385
and I'm not going to do that to you.

00:04:24.409 --> 00:04:25.753
But like the surprise eggs,

00:04:25.777 --> 00:04:27.941
it's got inside kids' heads

00:04:27.965 --> 00:04:29.572
and addicted them to it.

00:04:29.596 --> 00:04:32.127
So within a few years,
these finger family videos

00:04:32.151 --> 00:04:33.454
start appearing everywhere,

00:04:33.478 --> 00:04:35.507
and you get versions
in different languages

00:04:35.531 --> 00:04:37.652
with popular kids' cartoons using food

00:04:37.676 --> 00:04:40.226
or, frankly, using whatever kind
of animation elements

00:04:40.250 --> 00:04:42.502
you seem to have lying around.

00:04:43.002 --> 00:04:48.199
And once again, there are millions
and millions and millions of these videos

00:04:48.223 --> 00:04:51.658
available online in all of these
kind of insane combinations.

00:04:51.682 --> 00:04:53.910
And the more time
you start to spend with them,

00:04:53.934 --> 00:04:57.628
the crazier and crazier
you start to feel that you might be.

00:04:57.652 --> 00:05:00.985
And that's where I
kind of launched into this,

00:05:01.009 --> 00:05:04.675
that feeling of deep strangeness
and deep lack of understanding

00:05:04.699 --> 00:05:08.874
of how this thing was constructed
that seems to be presented around me.

00:05:08.898 --> 00:05:12.065
Because it's impossible to know
where these things are coming from.

00:05:12.089 --> 00:05:13.330
Like, who is making them?

00:05:13.354 --> 00:05:16.497
Some of them appear to be made
of teams of professional animators.

00:05:16.521 --> 00:05:19.403
Some of them are just randomly
assembled by software.

00:05:19.427 --> 00:05:23.680
Some of them are quite wholesome-looking
young kids' entertainers.

00:05:23.704 --> 00:05:25.256
And some of them are from people

00:05:25.280 --> 00:05:28.287
who really clearly
shouldn't be around children at all.

00:05:28.311 --> 00:05:29.926
(Laughter)

00:05:30.987 --> 00:05:35.627
And once again, this impossibility
of figuring out who's making this stuff --

00:05:35.651 --> 00:05:36.807
like, this is a bot?

00:05:36.831 --> 00:05:39.478
Is this a person? Is this a troll?

00:05:39.502 --> 00:05:41.884
What does it mean
that we can't tell the difference

00:05:41.908 --> 00:05:43.491
between these things anymore?

00:05:43.515 --> 00:05:48.363
And again, doesn't that uncertainty
feel kind of familiar right now?

00:05:50.145 --> 00:05:52.725
So the main way people get views
on their videos --

00:05:52.749 --> 00:05:54.456
and remember, views mean money --

00:05:54.480 --> 00:05:59.222
is that they stuff the titles
of these videos with these popular terms.

00:05:59.246 --> 00:06:00.933
So you take, like, "surprise eggs"

00:06:00.957 --> 00:06:03.023
and then you add
"Paw Patrol," "Easter egg,"

00:06:03.047 --> 00:06:04.440
or whatever these things are,

00:06:04.464 --> 00:06:07.357
all of these words from other
popular videos into your title,

00:06:07.381 --> 00:06:10.349
until you end up with this kind of
meaningless mash of language

00:06:10.373 --> 00:06:12.871
that doesn't make sense to humans at all.

00:06:12.895 --> 00:06:16.441
Because of course it's only really
tiny kids who are watching your video,

00:06:16.465 --> 00:06:18.292
and what the hell do they know?

00:06:18.316 --> 00:06:21.323
Your real audience
for this stuff is software.

00:06:21.347 --> 00:06:22.503
It's the algorithms.

00:06:22.527 --> 00:06:24.382
It's the software that YouTube uses

00:06:24.406 --> 00:06:26.889
to select which videos
are like other videos,

00:06:26.913 --> 00:06:29.156
to make them popular,
to make them recommended.

00:06:29.180 --> 00:06:32.641
And that's why you end up with this
kind of completely meaningless mash,

00:06:32.665 --> 00:06:34.736
both of title and of content.

00:06:35.792 --> 00:06:37.686
But the thing is, you have to remember,

00:06:37.710 --> 00:06:42.188
there really are still people within
this algorithmically optimized system,

00:06:42.212 --> 00:06:45.002
people who are kind
of increasingly forced to act out

00:06:45.026 --> 00:06:48.092
these increasingly bizarre
combinations of words,

00:06:48.116 --> 00:06:53.289
like a desperate improvisation artist
responding to the combined screams

00:06:53.313 --> 00:06:55.516
of a million toddlers at once.

00:06:57.168 --> 00:06:59.636
There are real people
trapped within these systems,

00:06:59.660 --> 00:07:03.715
and that's the other deeply strange thing
about this algorithmically driven culture,

00:07:03.739 --> 00:07:05.120
because even if you're human,

00:07:05.144 --> 00:07:07.289
you have to end up behaving like a machine

00:07:07.313 --> 00:07:09.113
just to survive.

00:07:09.137 --> 00:07:11.237
And also, on the other side of the screen,

00:07:11.261 --> 00:07:14.208
there still are these little kids
watching this stuff,

00:07:14.232 --> 00:07:18.438
stuck, their full attention grabbed
by these weird mechanisms.

00:07:18.768 --> 00:07:21.566
And most of these kids are too small
to even use a website.

00:07:21.590 --> 00:07:24.866
They're just kind of hammering
on the screen with their little hands.

00:07:24.890 --> 00:07:26.107
And so there's autoplay,

00:07:26.131 --> 00:07:29.710
where it just keeps playing these videos
over and over and over in a loop,

00:07:29.734 --> 00:07:31.793
endlessly for hours and hours at a time.

00:07:31.817 --> 00:07:34.660
And there's so much weirdness
in the system now

00:07:34.684 --> 00:07:37.693
that autoplay takes you
to some pretty strange places.

00:07:37.717 --> 00:07:40.205
This is how, within a dozen steps,

00:07:40.229 --> 00:07:43.387
you can go from a cute video
of a counting train

00:07:43.411 --> 00:07:45.853
to masturbating Mickey Mouse.

00:07:46.529 --> 00:07:48.817
Yeah. I'm sorry about that.

00:07:48.841 --> 00:07:50.541
This does get worse.

00:07:50.565 --> 00:07:51.847
This is what happens

00:07:51.871 --> 00:07:54.957
when all of these different keywords,

00:07:54.981 --> 00:07:57.442
all these different pieces of attention,

00:07:57.466 --> 00:08:00.273
this desperate generation of content,

00:08:00.297 --> 00:08:02.879
all comes together into a single place.

00:08:03.871 --> 00:08:08.343
This is where all those deeply weird
keywords come home to roost.

00:08:08.367 --> 00:08:10.758
You cross-breed the finger family video

00:08:10.782 --> 00:08:12.870
with some live-action superhero stuff,

00:08:12.894 --> 00:08:16.150
you add in some weird,
trollish in-jokes or something,

00:08:16.174 --> 00:08:19.540
and suddenly, you come
to a very weird place indeed.

00:08:19.564 --> 00:08:21.677
The stuff that tends to upset parents

00:08:21.701 --> 00:08:25.032
is the stuff that has kind of violent
or sexual content, right?

00:08:25.056 --> 00:08:27.878
Children's cartoons getting assaulted,

00:08:27.902 --> 00:08:29.920
getting killed,

00:08:29.944 --> 00:08:33.287
weird pranks that actually
genuinely terrify children.

00:08:33.311 --> 00:08:36.986
What you have is software pulling in
all of these different influences

00:08:37.010 --> 00:08:39.971
to automatically generate
kids' worst nightmares.

00:08:39.995 --> 00:08:42.696
And this stuff really, really
does affect small children.

00:08:42.720 --> 00:08:45.586
Parents report their children
being traumatized,

00:08:45.610 --> 00:08:47.002
becoming afraid of the dark,

00:08:47.026 --> 00:08:50.076
becoming afraid of their favorite
cartoon characters.

00:08:50.524 --> 00:08:54.135
If you take one thing away from this,
it's that if you have small children,

00:08:54.159 --> 00:08:56.155
keep them the hell away from YouTube.

00:08:56.743 --> 00:09:00.692
(Applause)

00:09:02.504 --> 00:09:05.600
But the other thing, the thing
that really gets to me about this,

00:09:05.624 --> 00:09:10.253
is that I'm not sure we even really
understand how we got to this point.

00:09:10.951 --> 00:09:13.882
We've taken all of this influence,
all of these things,

00:09:13.906 --> 00:09:16.859
and munged them together in a way
that no one really intended.

00:09:16.883 --> 00:09:20.039
And yet, this is also the way
that we're building the entire world.

00:09:20.063 --> 00:09:21.836
We're taking all of this data,

00:09:21.860 --> 00:09:23.307
a lot of it bad data,

00:09:23.331 --> 00:09:26.360
a lot of historical data
full of prejudice,

00:09:26.384 --> 00:09:29.221
full of all of our worst
impulses of history,

00:09:29.245 --> 00:09:31.294
and we're building that
into huge data sets

00:09:31.318 --> 00:09:32.741
and then we're automating it.

00:09:32.765 --> 00:09:36.267
And we're munging it together
into things like credit reports,

00:09:36.291 --> 00:09:37.925
into insurance premiums,

00:09:37.949 --> 00:09:40.642
into things like predictive
policing systems,

00:09:40.666 --> 00:09:42.428
into sentencing guidelines.

00:09:42.452 --> 00:09:45.273
This is the way we're actually
constructing the world today

00:09:45.297 --> 00:09:46.448
out of this data.

00:09:46.472 --> 00:09:48.170
And I don't know what's worse,

00:09:48.194 --> 00:09:51.422
that we built a system
that seems to be entirely optimized

00:09:51.446 --> 00:09:54.254
for the absolute worst aspects
of human behavior,

00:09:54.278 --> 00:09:56.703
or that we seem
to have done it by accident,

00:09:56.727 --> 00:09:58.934
without even realizing
that we were doing it,

00:09:58.958 --> 00:10:02.340
because we didn't really understand
the systems that we were building,

00:10:02.364 --> 00:10:06.047
and we didn't really understand
how to do anything differently with it.

00:10:06.769 --> 00:10:10.134
There's a couple of things I think
that really seem to be driving this

00:10:10.158 --> 00:10:11.347
most fully on YouTube,

00:10:11.371 --> 00:10:13.198
and the first of those is advertising,

00:10:13.222 --> 00:10:16.059
which is the monetization of attention

00:10:16.083 --> 00:10:19.219
without any real other variables at work,

00:10:19.243 --> 00:10:23.128
any care for the people who are
actually developing this content,

00:10:23.152 --> 00:10:26.788
the centralization of the power,
the separation of those things.

00:10:26.812 --> 00:10:29.956
And I think however you feel
about the use of advertising

00:10:29.980 --> 00:10:31.218
to kind of support stuff,

00:10:31.242 --> 00:10:34.309
the sight of grown men in diapers
rolling around in the sand

00:10:34.333 --> 00:10:37.316
in the hope that an algorithm
that they don't really understand

00:10:37.340 --> 00:10:38.655
will give them money for it

00:10:38.679 --> 00:10:40.716
suggests that this
probably isn't the thing

00:10:40.740 --> 00:10:43.303
that we should be basing
our society and culture upon,

00:10:43.327 --> 00:10:45.487
and the way in which
we should be funding it.

00:10:45.511 --> 00:10:49.030
And the other thing that's kind of
the major driver of this is automation,

00:10:49.054 --> 00:10:51.383
which is the deployment
of all of this technology

00:10:51.407 --> 00:10:53.928
as soon as it arrives,
without any kind of oversight,

00:10:53.952 --> 00:10:55.364
and then once it's out there,

00:10:55.388 --> 00:10:59.231
kind of throwing up our hands and going,
"Hey, it's not us, it's the technology."

00:10:59.255 --> 00:11:00.897
Like, "We're not involved in it."

00:11:00.921 --> 00:11:02.688
That's not really good enough,

00:11:02.712 --> 00:11:05.422
because this stuff isn't
just algorithmically governed,

00:11:05.446 --> 00:11:07.944
it's also algorithmically policed.

00:11:07.968 --> 00:11:10.816
When YouTube first started
to pay attention to this,

00:11:10.840 --> 00:11:12.927
the first thing they said
they'd do about it

00:11:12.951 --> 00:11:15.646
was that they'd deploy
better machine learning algorithms

00:11:15.670 --> 00:11:16.999
to moderate the content.

00:11:17.023 --> 00:11:20.508
Well, machine learning,
as any expert in it will tell you,

00:11:20.532 --> 00:11:22.428
is basically what we've started to call

00:11:22.452 --> 00:11:25.040
software that we don't really
understand how it works.

00:11:25.064 --> 00:11:29.047
And I think we have
enough of that already.

00:11:29.071 --> 00:11:32.237
We shouldn't be leaving
this stuff up to AI to decide

00:11:32.261 --> 00:11:33.512
what's appropriate or not,

00:11:33.536 --> 00:11:34.972
because we know what happens.

00:11:34.996 --> 00:11:36.684
It'll start censoring other things.

00:11:36.708 --> 00:11:38.491
It'll start censoring queer content.

00:11:38.515 --> 00:11:40.752
It'll start censoring
legitimate public speech.

00:11:40.776 --> 00:11:42.701
What's allowed in these discourses,

00:11:42.725 --> 00:11:45.822
it shouldn't be something
that's left up to unaccountable systems.

00:11:45.846 --> 00:11:48.793
It's part of a discussion
all of us should be having.

00:11:48.817 --> 00:11:50.125
But I'd leave a reminder

00:11:50.149 --> 00:11:52.902
that the alternative isn't
very pleasant, either.

00:11:52.926 --> 00:11:54.461
YouTube also announced recently

00:11:54.485 --> 00:11:57.252
that they're going to release
a version of their kids' app

00:11:57.276 --> 00:11:59.683
that would be entirely
moderated by humans.

00:12:00.134 --> 00:12:03.752
Facebook -- Zuckerberg said
much the same thing at Congress,

00:12:03.776 --> 00:12:06.763
when pressed about how they
were going to moderate their stuff.

00:12:06.787 --> 00:12:08.534
He said they'd have humans doing it.

00:12:08.558 --> 00:12:10.017
And what that really means is,

00:12:10.041 --> 00:12:13.264
instead of having toddlers being
the first person to see this stuff,

00:12:13.288 --> 00:12:16.076
you're going to have underpaid,
precarious contract workers

00:12:16.100 --> 00:12:17.826
without proper mental health support

00:12:17.850 --> 00:12:19.226
being damaged by it as well.

00:12:19.250 --> 00:12:20.346
(Laughter)

00:12:20.370 --> 00:12:22.971
And I think we can all do
quite a lot better than that.

00:12:22.995 --> 00:12:25.494
(Applause)

00:12:26.068 --> 00:12:30.681
The thought, I think, that brings those
two things together, really, for me,

00:12:30.705 --> 00:12:32.125
is agency.

00:12:32.149 --> 00:12:35.306
It's like, how much do we really
understand -- by agency, I mean:

00:12:35.330 --> 00:12:39.720
how we know how to act
in our own best interests.

00:12:39.744 --> 00:12:41.531
Which -- it's almost impossible to do

00:12:41.555 --> 00:12:45.040
in these systems that we don't
really fully understand.

00:12:45.064 --> 00:12:48.135
Inequality of power
always leads to violence.

00:12:48.159 --> 00:12:49.844
And we can see inside these systems

00:12:49.868 --> 00:12:52.479
that inequality of understanding
does the same thing.

00:12:52.503 --> 00:12:56.282
If there's one thing that we can do
to start to improve these systems,

00:12:56.306 --> 00:12:59.024
it's to make them more legible
to the people who use them,

00:12:59.048 --> 00:13:01.244
so that all of us have
a common understanding

00:13:01.268 --> 00:13:03.119
of what's actually going on here.

00:13:03.970 --> 00:13:06.938
The thing, though, I think
most about these systems

00:13:06.962 --> 00:13:10.819
is that this isn't, as I hope
I've explained, really about YouTube.

00:13:10.843 --> 00:13:12.155
It's about everything.

00:13:12.179 --> 00:13:14.623
These issues of accountability and agency,

00:13:14.647 --> 00:13:16.872
of opacity and complexity,

00:13:16.896 --> 00:13:20.073
of the violence and exploitation
that inherently results

00:13:20.097 --> 00:13:22.891
from the concentration
of power in a few hands --

00:13:22.915 --> 00:13:25.494
these are much, much larger issues.

00:13:26.395 --> 00:13:30.082
And they're issues not just of YouTube
and not just of technology in general,

00:13:30.106 --> 00:13:31.371
and they're not even new.

00:13:31.395 --> 00:13:32.856
They've been with us for ages.

00:13:32.880 --> 00:13:37.270
But we finally built this system,
this global system, the internet,

00:13:37.294 --> 00:13:40.313
that's actually showing them to us
in this extraordinary way,

00:13:40.337 --> 00:13:41.884
making them undeniable.

00:13:41.908 --> 00:13:44.728
Technology has this extraordinary capacity

00:13:44.752 --> 00:13:48.725
to both instantiate and continue

00:13:48.749 --> 00:13:52.997
all of our most extraordinary,
often hidden desires and biases

00:13:53.021 --> 00:13:54.887
and encoding them into the world,

00:13:54.911 --> 00:13:58.385
but it also writes them down
so that we can see them,

00:13:58.409 --> 00:14:01.739
so that we can't pretend
they don't exist anymore.

00:14:01.763 --> 00:14:06.082
We need to stop thinking about technology
as a solution to all of our problems,

00:14:06.106 --> 00:14:09.863
but think of it as a guide
to what those problems actually are,

00:14:09.887 --> 00:14:12.031
so we can start thinking
about them properly

00:14:12.055 --> 00:14:13.821
and start to address them.

00:14:13.845 --> 00:14:15.180
Thank you very much.

00:14:15.204 --> 00:14:20.396
(Applause)

00:14:21.733 --> 00:14:22.921
Thank you.

00:14:22.945 --> 00:14:25.814
(Applause)

00:14:28.839 --> 00:14:32.017
Helen Walters: James, thank you
for coming and giving us that talk.

00:14:32.041 --> 00:14:33.230
So it's interesting:

00:14:33.254 --> 00:14:36.749
when you think about the films where
the robotic overlords take over,

00:14:36.773 --> 00:14:40.052
it's all a bit more glamorous
than what you're describing.

00:14:40.076 --> 00:14:43.825
But I wonder -- in those films,
you have the resistance mounting.

00:14:43.849 --> 00:14:47.065
Is there a resistance mounting
towards this stuff?

00:14:47.089 --> 00:14:50.885
Do you see any positive signs,
green shoots of resistance?

00:14:52.507 --> 00:14:54.923
James Bridle: I don't know
about direct resistance,

00:14:54.947 --> 00:14:57.211
because I think this stuff
is super long-term.

00:14:57.235 --> 00:14:59.745
I think it's baked into culture
in really deep ways.

00:14:59.769 --> 00:15:01.901
A friend of mine,
Eleanor Saitta, always says

00:15:01.935 --> 00:15:05.544
that any technological problems
of sufficient scale and scope

00:15:05.568 --> 00:15:07.835
are political problems first of all.

00:15:07.859 --> 00:15:10.644
So all of these things we're working
to address within this

00:15:10.668 --> 00:15:13.942
are not going to be addressed
just by building the technology better,

00:15:13.966 --> 00:15:17.430
but actually by changing the society
that's producing these technologies.

00:15:17.454 --> 00:15:20.481
So no, right now, I think we've got
a hell of a long way to go.

00:15:20.505 --> 00:15:22.491
But as I said, I think by unpacking them,

00:15:22.515 --> 00:15:25.212
by explaining them, by talking
about them super honestly,

00:15:25.236 --> 00:15:27.741
we can actually start
to at least begin that process.

00:15:27.765 --> 00:15:31.327
HW: And so when you talk about
legibility and digital literacy,

00:15:31.351 --> 00:15:32.942
I find it difficult to imagine

00:15:32.966 --> 00:15:36.646
that we need to place the burden
of digital literacy on users themselves.

00:15:36.670 --> 00:15:41.232
But whose responsibility
is education in this new world?

00:15:41.256 --> 00:15:44.868
JB: Again, I think this responsibility
is kind of up to all of us,

00:15:44.892 --> 00:15:47.876
that everything we do,
everything we build, everything we make,

00:15:47.900 --> 00:15:51.592
needs to be made
in a consensual discussion

00:15:51.616 --> 00:15:53.556
with everyone who's avoiding it;

00:15:53.580 --> 00:15:57.921
that we're not building systems
intended to trick and surprise people

00:15:57.945 --> 00:16:00.245
into doing the right thing,

00:16:00.269 --> 00:16:03.505
but that they're actually involved
in every step in educating them,

00:16:03.529 --> 00:16:05.807
because each of these systems
is educational.

00:16:05.831 --> 00:16:08.933
That's what I'm hopeful about,
about even this really grim stuff,

00:16:08.957 --> 00:16:11.219
that if you can take it
and look at it properly,

00:16:11.243 --> 00:16:13.332
it's actually in itself
a piece of education

00:16:13.356 --> 00:16:17.118
that allows you to start seeing
how complex systems come together and work

00:16:17.142 --> 00:16:20.643
and maybe be able to apply
that knowledge elsewhere in the world.

00:16:20.667 --> 00:16:22.782
HW: James, it's such
an important discussion,

00:16:22.806 --> 00:16:26.033
and I know many people here
are really open and prepared to have it,

00:16:26.057 --> 00:16:27.916
so thanks for starting off our morning.

00:16:27.940 --> 00:16:29.340
JB: Thanks very much. Cheers.

00:16:29.364 --> 00:16:31.015
(Applause)

