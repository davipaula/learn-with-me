WEBVTT
Kind: captions
Language: en

00:00:12.944 --> 00:00:16.953
Back in the 1980s, actually,
I gave my first talk at TED,

00:00:16.977 --> 00:00:21.239
and I brought some of the very,
very first public demonstrations

00:00:21.263 --> 00:00:25.497
of virtual reality ever to the TED stage.

00:00:26.375 --> 00:00:33.242
And at that time, we knew
that we were facing a knife-edge future

00:00:33.266 --> 00:00:38.467
where the technology we needed,

00:00:38.491 --> 00:00:40.342
the technology we loved,

00:00:40.366 --> 00:00:42.413
could also be our undoing.

00:00:43.266 --> 00:00:47.357
We knew that if we thought
of our technology

00:00:47.381 --> 00:00:50.635
as a means to ever more power,

00:00:50.659 --> 00:00:54.366
if it was just a power trip,
we'd eventually destroy ourselves.

00:00:54.390 --> 00:00:55.571
That's what happens

00:00:55.595 --> 00:00:58.382
when you're on a power trip
and nothing else.

00:00:59.509 --> 00:01:02.898
So the idealism

00:01:02.922 --> 00:01:07.731
of digital culture back then

00:01:07.755 --> 00:01:12.494
was all about starting with
that recognition of the possible darkness

00:01:12.518 --> 00:01:15.868
and trying to imagine
a way to transcend it

00:01:15.892 --> 00:01:18.470
with beauty and creativity.

00:01:19.033 --> 00:01:25.540
I always used to end my early TED Talks
with a rather horrifying line, which is,

00:01:26.478 --> 00:01:30.344
"We have a challenge.

00:01:30.368 --> 00:01:34.392
We have to create
a culture around technology

00:01:34.416 --> 00:01:38.384
that is so beautiful, so meaningful,

00:01:38.408 --> 00:01:40.949
so deep, so endlessly creative,

00:01:40.973 --> 00:01:43.989
so filled with infinite potential

00:01:44.013 --> 00:01:47.266
that it draws us away
from committing mass suicide."

00:01:48.519 --> 00:01:54.107
So we talked about extinction
as being one and the same

00:01:54.131 --> 00:01:58.961
as the need to create an alluring,
infinitely creative future.

00:01:59.639 --> 00:02:05.021
And I still believe
that that alternative of creativity

00:02:05.045 --> 00:02:07.019
as an alternative to death

00:02:07.043 --> 00:02:09.012
is very real and true,

00:02:09.036 --> 00:02:11.019
maybe the most true thing there is.

00:02:11.870 --> 00:02:13.965
In the case of virtual reality --

00:02:13.989 --> 00:02:16.271
well, the way I used to talk about it

00:02:16.295 --> 00:02:18.930
is that it would be something like

00:02:18.954 --> 00:02:21.804
what happened when people
discovered language.

00:02:21.828 --> 00:02:26.503
With language came new adventures,
new depth, new meaning,

00:02:26.527 --> 00:02:28.607
new ways to connect,
new ways to coordinate,

00:02:28.631 --> 00:02:32.665
new ways to imagine,
new ways to raise children,

00:02:32.689 --> 00:02:36.951
and I imagined, with virtual reality,
we'd have this new thing

00:02:36.975 --> 00:02:38.568
that would be like a conversation

00:02:38.592 --> 00:02:41.936
but also like waking-state
intentional dreaming.

00:02:41.960 --> 00:02:44.613
We called it post-symbolic communication,

00:02:44.637 --> 00:02:48.995
because it would be like just directly
making the thing you experienced

00:02:49.019 --> 00:02:52.638
instead of indirectly
making symbols to refer to things.

00:02:53.466 --> 00:02:57.804
It was a beautiful vision,
and it's one I still believe in,

00:02:57.828 --> 00:03:01.043
and yet, haunting that beautiful vision

00:03:01.067 --> 00:03:04.217
was the dark side
of how it could also turn out.

00:03:04.241 --> 00:03:09.289
And I suppose I could mention

00:03:09.313 --> 00:03:12.377
from one of the very earliest
computer scientists,

00:03:12.401 --> 00:03:14.536
whose name was Norbert Wiener,

00:03:14.560 --> 00:03:18.314
and he wrote a book back in the '50s,
from before I was even born,

00:03:18.338 --> 00:03:20.996
called "The Human Use of Human Beings."

00:03:21.779 --> 00:03:25.951
And in the book,
he described the potential

00:03:25.975 --> 00:03:32.156
to create a computer system
that would be gathering data from people

00:03:32.180 --> 00:03:35.752
and providing feedback
to those people in real time

00:03:35.776 --> 00:03:40.911
in order to put them kind of partially,
statistically, in a Skinner box,

00:03:40.935 --> 00:03:43.379
in a behaviorist system,

00:03:43.403 --> 00:03:45.904
and he has this amazing line
where he says,

00:03:45.928 --> 00:03:48.666
one could imagine,
as a thought experiment --

00:03:48.690 --> 00:03:51.151
and I'm paraphrasing,
this isn't a quote --

00:03:51.175 --> 00:03:54.255
one could imagine a global computer system

00:03:54.279 --> 00:03:57.121
where everybody has devices
on them all the time,

00:03:57.145 --> 00:04:00.417
and the devices are giving them
feedback based on what they did,

00:04:00.441 --> 00:04:02.316
and the whole population

00:04:02.340 --> 00:04:05.916
is subject to a degree
of behavior modification.

00:04:05.940 --> 00:04:09.486
And such a society would be insane,

00:04:09.510 --> 00:04:12.607
could not survive,
could not face its problems.

00:04:12.631 --> 00:04:15.252
And then he says, but this is
only a thought experiment,

00:04:15.276 --> 00:04:18.696
and such a future
is technologically infeasible.

00:04:18.720 --> 00:04:19.812
(Laughter)

00:04:19.836 --> 00:04:22.838
And yet, of course,
it's what we have created,

00:04:22.862 --> 00:04:26.139
and it's what we must undo
if we are to survive.

00:04:27.457 --> 00:04:28.608
So --

00:04:28.632 --> 00:04:32.172
(Applause)

00:04:32.631 --> 00:04:38.608
I believe that we made
a very particular mistake,

00:04:38.632 --> 00:04:40.866
and it happened early on,

00:04:40.890 --> 00:04:42.964
and by understanding the mistake we made,

00:04:42.988 --> 00:04:44.847
we can undo it.

00:04:44.871 --> 00:04:47.430
It happened in the '90s,

00:04:47.454 --> 00:04:50.196
and going into the turn of the century,

00:04:50.220 --> 00:04:51.608
and here's what happened.

00:04:53.200 --> 00:04:54.574
Early digital culture,

00:04:54.598 --> 00:04:59.570
and indeed, digital culture to this day,

00:04:59.594 --> 00:05:05.903
had a sense of, I would say,
lefty, socialist mission about it,

00:05:05.927 --> 00:05:08.087
that unlike other things
that have been done,

00:05:08.111 --> 00:05:09.545
like the invention of books,

00:05:09.569 --> 00:05:12.982
everything on the internet
must be purely public,

00:05:13.006 --> 00:05:15.331
must be available for free,

00:05:15.355 --> 00:05:18.743
because if even one person
cannot afford it,

00:05:18.767 --> 00:05:21.339
then that would create
this terrible inequity.

00:05:21.912 --> 00:05:24.436
Now of course, there's other ways
to deal with that.

00:05:24.460 --> 00:05:27.476
If books cost money,
you can have public libraries.

00:05:27.500 --> 00:05:28.674
And so forth.

00:05:28.698 --> 00:05:31.316
But we were thinking, no, no, no,
this is an exception.

00:05:31.340 --> 00:05:35.945
This must be pure public commons,
that's what we want.

00:05:35.969 --> 00:05:38.603
And so that spirit lives on.

00:05:38.627 --> 00:05:42.342
You can experience it in designs
like the Wikipedia, for instance,

00:05:42.366 --> 00:05:43.707
many others.

00:05:43.731 --> 00:05:45.605
But at the same time,

00:05:45.629 --> 00:05:48.217
we also believed, with equal fervor,

00:05:48.241 --> 00:05:52.178
in this other thing
that was completely incompatible,

00:05:52.202 --> 00:05:55.829
which is we loved our tech entrepreneurs.

00:05:55.853 --> 00:05:59.592
We loved Steve Jobs;
we loved this Nietzschean myth

00:05:59.616 --> 00:06:03.084
of the techie who could dent the universe.

00:06:03.108 --> 00:06:04.426
Right?

00:06:04.450 --> 00:06:10.298
And that mythical power
still has a hold on us, as well.

00:06:10.322 --> 00:06:14.781
So you have these two different passions,

00:06:14.805 --> 00:06:16.742
for making everything free

00:06:16.766 --> 00:06:21.932
and for the almost supernatural
power of the tech entrepreneur.

00:06:21.956 --> 00:06:26.308
How do you celebrate entrepreneurship
when everything's free?

00:06:26.332 --> 00:06:29.457
Well, there was only
one solution back then,

00:06:29.481 --> 00:06:31.568
which was the advertising model.

00:06:31.592 --> 00:06:35.595
And so therefore, Google
was born free, with ads,

00:06:35.619 --> 00:06:39.301
Facebook was born free, with ads.

00:06:39.325 --> 00:06:43.190
Now in the beginning, it was cute,

00:06:43.214 --> 00:06:45.174
like with the very earliest Google.

00:06:45.198 --> 00:06:46.484
(Laughter)

00:06:46.508 --> 00:06:49.405
The ads really were kind of ads.

00:06:49.429 --> 00:06:51.914
They would be, like,
your local dentist or something.

00:06:51.938 --> 00:06:53.858
But there's thing called Moore's law

00:06:53.882 --> 00:06:57.024
that makes the computers
more and more efficient and cheaper.

00:06:57.048 --> 00:06:58.906
Their algorithms get better.

00:06:58.930 --> 00:07:01.526
We actually have universities
where people study them,

00:07:01.550 --> 00:07:03.178
and they get better and better.

00:07:03.202 --> 00:07:07.654
And the customers and other entities
who use these systems

00:07:07.678 --> 00:07:11.805
just got more and more experienced
and got cleverer and cleverer.

00:07:11.829 --> 00:07:14.226
And what started out as advertising

00:07:14.250 --> 00:07:16.727
really can't be called
advertising anymore.

00:07:16.751 --> 00:07:19.663
It turned into behavior modification,

00:07:19.687 --> 00:07:24.180
just as Norbert Wiener
had worried it might.

00:07:24.204 --> 00:07:28.824
And so I can't call these things
social networks anymore.

00:07:28.848 --> 00:07:32.662
I call them behavior modification empires.

00:07:32.686 --> 00:07:34.921
(Applause)

00:07:34.945 --> 00:07:39.159
And I refuse to vilify the individuals.

00:07:39.183 --> 00:07:41.454
I have dear friends at these companies,

00:07:41.478 --> 00:07:46.238
sold a company to Google, even though
I think it's one of these empires.

00:07:46.262 --> 00:07:51.322
I don't think this is a matter
of bad people who've done a bad thing.

00:07:51.346 --> 00:07:55.922
I think this is a matter
of a globally tragic,

00:07:55.946 --> 00:08:00.518
astoundingly ridiculous mistake,

00:08:00.542 --> 00:08:04.671
rather than a wave of evil.

00:08:04.695 --> 00:08:07.377
Let me give you
just another layer of detail

00:08:07.401 --> 00:08:10.504
into how this particular
mistake functions.

00:08:11.337 --> 00:08:14.044
So with behaviorism,

00:08:14.068 --> 00:08:19.132
you give the creature,
whether it's a rat or a dog or a person,

00:08:19.156 --> 00:08:21.996
little treats and sometimes
little punishments

00:08:22.020 --> 00:08:23.837
as feedback to what they do.

00:08:24.710 --> 00:08:30.622
So if you have an animal in a cage,
it might be candy and electric shocks.

00:08:30.646 --> 00:08:33.170
But if you have a smartphone,

00:08:33.194 --> 00:08:40.120
it's not those things,
it's symbolic punishment and reward.

00:08:40.144 --> 00:08:42.587
Pavlov, one of the early behaviorists,

00:08:42.611 --> 00:08:45.563
demonstrated the famous principle.

00:08:45.587 --> 00:08:49.548
You could train a dog to salivate
just with the bell, just with the symbol.

00:08:49.572 --> 00:08:51.158
So on social networks,

00:08:51.182 --> 00:08:56.262
social punishment and social reward
function as the punishment and reward.

00:08:56.286 --> 00:08:58.363
And we all know
the feeling of these things.

00:08:58.387 --> 00:08:59.838
You get this little thrill --

00:08:59.862 --> 00:09:02.212
"Somebody liked my stuff
and it's being repeated."

00:09:02.236 --> 00:09:04.570
Or the punishment:
"Oh my God, they don't like me,

00:09:04.594 --> 00:09:06.833
maybe somebody else
is more popular, oh my God."

00:09:06.857 --> 00:09:09.083
So you have those two
very common feelings,

00:09:09.107 --> 00:09:12.671
and they're doled out in such a way
that you get caught in this loop.

00:09:12.695 --> 00:09:16.790
As has been publicly acknowledged
by many of the founders of the system,

00:09:16.814 --> 00:09:19.155
everybody knew this is what was going on.

00:09:19.871 --> 00:09:21.490
But here's the thing:

00:09:21.514 --> 00:09:26.808
traditionally, in the academic study
of the methods of behaviorism,

00:09:26.832 --> 00:09:32.268
there have been comparisons
of positive and negative stimuli.

00:09:32.292 --> 00:09:34.656
In this setting, a commercial setting,

00:09:34.680 --> 00:09:36.276
there's a new kind of difference

00:09:36.300 --> 00:09:39.069
that has kind of evaded
the academic world for a while,

00:09:39.093 --> 00:09:43.141
and that difference
is that whether positive stimuli

00:09:43.165 --> 00:09:46.474
are more effective than negative ones
in different circumstances,

00:09:46.498 --> 00:09:48.602
the negative ones are cheaper.

00:09:48.626 --> 00:09:50.682
They're the bargain stimuli.

00:09:50.706 --> 00:09:56.409
So what I mean by that is it's much easier

00:09:56.433 --> 00:09:59.549
to lose trust than to build trust.

00:09:59.573 --> 00:10:02.745
It takes a long time to build love.

00:10:02.769 --> 00:10:05.375
It takes a short time to ruin love.

00:10:05.399 --> 00:10:09.987
Now the customers of these
behavior modification empires

00:10:10.011 --> 00:10:11.434
are on a very fast loop.

00:10:11.458 --> 00:10:13.503
They're almost like
high-frequency traders.

00:10:13.527 --> 00:10:15.551
They're getting feedbacks
from their spends

00:10:15.575 --> 00:10:18.377
or whatever their activities are
if they're not spending,

00:10:18.401 --> 00:10:21.671
and they see what's working,
and then they do more of that.

00:10:21.695 --> 00:10:23.735
And so they're getting the quick feedback,

00:10:23.759 --> 00:10:26.799
which means they're responding
more to the negative emotions,

00:10:26.823 --> 00:10:30.760
because those are the ones
that rise faster, right?

00:10:30.784 --> 00:10:34.332
And so therefore,
even well-intentioned players

00:10:34.356 --> 00:10:37.221
who think all they're doing
is advertising toothpaste

00:10:37.245 --> 00:10:40.276
end up advancing the cause
of the negative people,

00:10:40.300 --> 00:10:42.634
the negative emotions, the cranks,

00:10:42.658 --> 00:10:44.102
the paranoids,

00:10:44.126 --> 00:10:47.206
the cynics, the nihilists.

00:10:47.230 --> 00:10:50.723
Those are the ones who get
amplified by the system.

00:10:50.747 --> 00:10:56.398
And you can't pay one of these companies
to make the world suddenly nice

00:10:56.422 --> 00:10:57.573
and improve democracy

00:10:57.597 --> 00:11:01.438
nearly as easily as you can pay
to ruin those things.

00:11:01.462 --> 00:11:05.181
And so this is the dilemma
we've gotten ourselves into.

00:11:05.856 --> 00:11:11.088
The alternative is to turn back the clock,
with great difficulty,

00:11:11.112 --> 00:11:13.953
and remake that decision.

00:11:13.977 --> 00:11:18.015
Remaking it would mean two things.

00:11:18.039 --> 00:11:21.967
It would mean first that many people,
those who could afford to,

00:11:21.991 --> 00:11:24.198
would actually pay for these things.

00:11:24.222 --> 00:11:28.629
You'd pay for search,
you'd pay for social networking.

00:11:28.653 --> 00:11:32.114
How would you pay?
Maybe with a subscription fee,

00:11:32.138 --> 00:11:34.876
maybe with micro-payments as you use them.

00:11:34.900 --> 00:11:36.702
There's a lot of options.

00:11:36.726 --> 00:11:39.123
If some of you are recoiling,
and you're thinking,

00:11:39.147 --> 00:11:41.513
"Oh my God, I would never pay
for these things.

00:11:41.537 --> 00:11:43.632
How could you ever get anyone to pay?"

00:11:43.656 --> 00:11:46.895
I want to remind you
of something that just happened.

00:11:46.919 --> 00:11:48.973
Around this same time

00:11:48.997 --> 00:11:54.704
that companies like Google and Facebook
were formulating their free idea,

00:11:54.728 --> 00:11:59.232
a lot of cyber culture
also believed that in the future,

00:11:59.256 --> 00:12:02.278
televisions and movies
would be created in the same way,

00:12:02.302 --> 00:12:04.057
kind of like the Wikipedia.

00:12:04.456 --> 00:12:09.520
But then, companies
like Netflix, Amazon, HBO,

00:12:09.544 --> 00:12:13.283
said, "Actually, you know, subscribe.
We'll give you give you great TV."

00:12:13.307 --> 00:12:14.680
And it worked!

00:12:14.704 --> 00:12:18.578
We now are in this period
called "peak TV," right?

00:12:18.602 --> 00:12:22.800
So sometimes when you pay for stuff,
things get better.

00:12:22.824 --> 00:12:25.110
We can imagine a hypothetical --

00:12:25.134 --> 00:12:29.805
(Applause)

00:12:29.829 --> 00:12:33.488
We can imagine a hypothetical world
of "peak social media."

00:12:33.512 --> 00:12:34.861
What would that be like?

00:12:34.885 --> 00:12:37.655
It would mean when you get on,
you can get really useful,

00:12:37.679 --> 00:12:40.774
authoritative medical advice
instead of cranks.

00:12:41.143 --> 00:12:44.453
It could mean when you want
to get factual information,

00:12:44.477 --> 00:12:47.731
there's not a bunch of weird,
paranoid conspiracy theories.

00:12:47.755 --> 00:12:51.990
We can imagine this wonderful
other possibility.

00:12:52.014 --> 00:12:53.275
Ah.

00:12:53.299 --> 00:12:55.429
I dream of it. I believe it's possible.

00:12:55.453 --> 00:12:58.755
I'm certain it's possible.

00:12:58.779 --> 00:13:03.526
And I'm certain that the companies,
the Googles and the Facebooks,

00:13:03.550 --> 00:13:05.862
would actually do better in this world.

00:13:05.886 --> 00:13:09.052
I don't believe we need
to punish Silicon Valley.

00:13:09.076 --> 00:13:11.329
We just need to remake the decision.

00:13:12.702 --> 00:13:14.584
Of the big tech companies,

00:13:14.608 --> 00:13:20.171
it's really only two that depend
on behavior modification and spying

00:13:20.195 --> 00:13:21.452
as their business plan.

00:13:21.476 --> 00:13:23.235
It's Google and Facebook.

00:13:23.259 --> 00:13:24.569
(Laughter)

00:13:24.593 --> 00:13:26.284
And I love you guys.

00:13:26.308 --> 00:13:29.029
Really, I do. Like, the people
are fantastic.

00:13:30.371 --> 00:13:33.553
I want to point out, if I may,

00:13:33.577 --> 00:13:34.728
if you look at Google,

00:13:34.752 --> 00:13:39.839
they can propagate cost centers
endlessly with all of these companies,

00:13:39.863 --> 00:13:41.911
but they cannot propagate profit centers.

00:13:41.935 --> 00:13:45.116
They cannot diversify,
because they're hooked.

00:13:45.140 --> 00:13:47.767
They're hooked on this model,
just like their own users.

00:13:47.791 --> 00:13:50.089
They're in the same trap as their users,

00:13:50.113 --> 00:13:52.617
and you can't run
a big corporation that way.

00:13:52.641 --> 00:13:56.244
So this is ultimately totally
in the benefit of the shareholders

00:13:56.268 --> 00:13:58.713
and other stakeholders of these companies.

00:13:58.737 --> 00:14:01.087
It's a win-win solution.

00:14:01.111 --> 00:14:03.626
It'll just take some time
to figure it out.

00:14:03.650 --> 00:14:05.912
A lot of details to work out,

00:14:05.936 --> 00:14:07.766
totally doable.

00:14:07.790 --> 00:14:10.205
(Laughter)

00:14:10.229 --> 00:14:14.063
I don't believe our species
can survive unless we fix this.

00:14:14.087 --> 00:14:16.377
We cannot have a society

00:14:16.401 --> 00:14:19.362
in which, if two people
wish to communicate,

00:14:19.386 --> 00:14:22.826
the only way that can happen
is if it's financed by a third person

00:14:22.850 --> 00:14:25.196
who wishes to manipulate them.

00:14:25.220 --> 00:14:31.458
(Applause)

00:14:35.077 --> 00:14:36.228
(Applause ends)

00:14:36.942 --> 00:14:39.887
In the meantime,
if the companies won't change,

00:14:39.911 --> 00:14:41.577
delete your accounts, OK?

00:14:41.601 --> 00:14:42.870
(Laughter)

00:14:42.894 --> 00:14:43.940
(Applause)

00:14:43.964 --> 00:14:45.473
That's enough for now.

00:14:45.497 --> 00:14:46.648
Thank you so much.

00:14:46.672 --> 00:14:53.476
(Applause)

