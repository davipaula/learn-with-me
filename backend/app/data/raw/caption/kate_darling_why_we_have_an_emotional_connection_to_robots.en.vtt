WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:07.000
Translator: Joseph Geni
Reviewer: Krystian Aparta

00:00:13.047 --> 00:00:15.555
There was a day, about 10 years ago,

00:00:15.579 --> 00:00:19.523
when I asked a friend to hold
a baby dinosaur robot upside down.

00:00:21.889 --> 00:00:25.335
It was this toy called a Pleo
that I had ordered,

00:00:25.359 --> 00:00:29.760
and I was really excited about it
because I've always loved robots.

00:00:29.784 --> 00:00:32.063
And this one has really cool
technical features.

00:00:32.087 --> 00:00:34.206
It had motors and touch sensors

00:00:34.230 --> 00:00:36.474
and it had an infrared camera.

00:00:36.498 --> 00:00:39.261
And one of the things it had
was a tilt sensor,

00:00:39.285 --> 00:00:41.603
so it knew what direction it was facing.

00:00:42.095 --> 00:00:44.229
And when you held it upside down,

00:00:44.253 --> 00:00:45.825
it would start to cry.

00:00:46.527 --> 00:00:50.023
And I thought this was super cool,
so I was showing it off to my friend,

00:00:50.047 --> 00:00:52.852
and I said, "Oh, hold it up by the tail.
See what it does."

00:00:55.268 --> 00:00:58.893
So we're watching
the theatrics of this robot

00:00:58.917 --> 00:01:01.116
struggle and cry out.

00:01:02.767 --> 00:01:04.814
And after a few seconds,

00:01:04.838 --> 00:01:06.810
it starts to bother me a little,

00:01:07.744 --> 00:01:11.168
and I said, "OK, that's enough now.

00:01:11.930 --> 00:01:14.235
Let's put him back down."

00:01:14.259 --> 00:01:16.814
And then I pet the robot
to make it stop crying.

00:01:18.973 --> 00:01:21.425
And that was kind of
a weird experience for me.

00:01:22.084 --> 00:01:26.653
For one thing, I wasn't the most
maternal person at the time.

00:01:26.677 --> 00:01:29.408
Although since then I've become
a mother, nine months ago,

00:01:29.432 --> 00:01:32.865
and I've learned that babies also squirm
when you hold them upside down.

00:01:32.889 --> 00:01:34.452
(Laughter)

00:01:35.023 --> 00:01:37.381
But my response to this robot
was also interesting

00:01:37.405 --> 00:01:41.506
because I knew exactly
how this machine worked,

00:01:41.530 --> 00:01:44.792
and yet I still felt
compelled to be kind to it.

00:01:46.450 --> 00:01:49.157
And that observation sparked a curiosity

00:01:49.181 --> 00:01:52.013
that I've spent the past decade pursuing.

00:01:52.911 --> 00:01:54.704
Why did I comfort this robot?

00:01:56.228 --> 00:01:59.807
And one of the things I discovered
was that my treatment of this machine

00:01:59.831 --> 00:02:03.532
was more than just an awkward moment
in my living room,

00:02:03.556 --> 00:02:08.976
that in a world where we're increasingly
integrating robots into our lives,

00:02:09.000 --> 00:02:12.126
an instinct like that
might actually have consequences,

00:02:13.452 --> 00:02:17.201
because the first thing that I discovered
is that it's not just me.

00:02:19.249 --> 00:02:24.051
In 2007, the Washington Post
reported that the United States military

00:02:24.075 --> 00:02:27.305
was testing this robot
that defused land mines.

00:02:27.329 --> 00:02:30.241
And the way it worked
was it was shaped like a stick insect

00:02:30.265 --> 00:02:32.916
and it would walk
around a minefield on its legs,

00:02:32.940 --> 00:02:36.146
and every time it stepped on a mine,
one of the legs would blow up,

00:02:36.170 --> 00:02:39.227
and it would continue on the other legs
to blow up more mines.

00:02:39.251 --> 00:02:43.037
And the colonel who was in charge
of this testing exercise

00:02:43.061 --> 00:02:45.179
ends up calling it off,

00:02:45.203 --> 00:02:47.638
because, he says, it's too inhumane

00:02:47.662 --> 00:02:52.178
to watch this damaged robot
drag itself along the minefield.

00:02:54.978 --> 00:02:58.875
Now, what would cause
a hardened military officer

00:02:58.899 --> 00:03:00.942
and someone like myself

00:03:00.966 --> 00:03:02.823
to have this response to robots?

00:03:03.537 --> 00:03:06.847
Well, of course, we're primed
by science fiction and pop culture

00:03:06.871 --> 00:03:09.450
to really want to personify these things,

00:03:09.474 --> 00:03:12.263
but it goes a little bit deeper than that.

00:03:12.287 --> 00:03:17.596
It turns out that we're biologically
hardwired to project intent and life

00:03:17.620 --> 00:03:22.386
onto any movement in our physical space
that seems autonomous to us.

00:03:23.214 --> 00:03:26.679
So people will treat all sorts
of robots like they're alive.

00:03:26.703 --> 00:03:29.386
These bomb-disposal units get names.

00:03:29.410 --> 00:03:31.092
They get medals of honor.

00:03:31.116 --> 00:03:33.441
They've had funerals for them
with gun salutes.

00:03:34.380 --> 00:03:38.213
And research shows that we do this
even with very simple household robots,

00:03:38.237 --> 00:03:40.372
like the Roomba vacuum cleaner.

00:03:40.396 --> 00:03:41.687
(Laughter)

00:03:41.711 --> 00:03:44.800
It's just a disc that roams
around your floor to clean it,

00:03:44.824 --> 00:03:47.130
but just the fact it's moving
around on its own

00:03:47.154 --> 00:03:49.321
will cause people to name the Roomba

00:03:49.345 --> 00:03:52.527
and feel bad for the Roomba
when it gets stuck under the couch.

00:03:52.551 --> 00:03:54.416
(Laughter)

00:03:54.440 --> 00:03:57.780
And we can design robots
specifically to evoke this response,

00:03:57.804 --> 00:04:01.265
using eyes and faces or movements

00:04:01.289 --> 00:04:04.548
that people automatically,
subconsciously associate

00:04:04.572 --> 00:04:06.592
with states of mind.

00:04:06.616 --> 00:04:09.909
And there's an entire body of research
called human-robot interaction

00:04:09.933 --> 00:04:11.759
that really shows how well this works.

00:04:11.783 --> 00:04:14.909
So for example, researchers
at Stanford University found out

00:04:14.933 --> 00:04:16.934
that it makes people really uncomfortable

00:04:16.958 --> 00:04:19.430
when you ask them to touch
a robot's private parts.

00:04:19.454 --> 00:04:21.574
(Laughter)

00:04:21.598 --> 00:04:23.621
So from this, but from many other studies,

00:04:23.645 --> 00:04:27.868
we know, we know that people
respond to the cues given to them

00:04:27.892 --> 00:04:29.914
by these lifelike machines,

00:04:29.938 --> 00:04:31.955
even if they know that they're not real.

00:04:33.654 --> 00:04:37.710
Now, we're headed towards a world
where robots are everywhere.

00:04:37.734 --> 00:04:40.799
Robotic technology is moving out
from behind factory walls.

00:04:40.823 --> 00:04:43.836
It's entering workplaces, households.

00:04:43.860 --> 00:04:50.069
And as these machines that can sense
and make autonomous decisions and learn

00:04:50.093 --> 00:04:52.645
enter into these shared spaces,

00:04:52.669 --> 00:04:55.165
I think that maybe the best
analogy we have for this

00:04:55.189 --> 00:04:57.124
is our relationship with animals.

00:04:57.523 --> 00:05:01.411
Thousands of years ago,
we started to domesticate animals,

00:05:01.435 --> 00:05:05.480
and we trained them for work
and weaponry and companionship.

00:05:05.504 --> 00:05:10.489
And throughout history, we've treated
some animals like tools or like products,

00:05:10.513 --> 00:05:12.687
and other animals,
we've treated with kindness

00:05:12.711 --> 00:05:15.789
and we've given a place in society
as our companions.

00:05:15.813 --> 00:05:19.662
I think it's plausible we might start
to integrate robots in similar ways.

00:05:21.484 --> 00:05:24.580
And sure, animals are alive.

00:05:24.604 --> 00:05:25.754
Robots are not.

00:05:27.626 --> 00:05:30.206
And I can tell you,
from working with roboticists,

00:05:30.230 --> 00:05:33.752
that we're pretty far away from developing
robots that can feel anything.

00:05:35.072 --> 00:05:36.532
But we feel for them,

00:05:37.835 --> 00:05:39.042
and that matters,

00:05:39.066 --> 00:05:42.693
because if we're trying to integrate
robots into these shared spaces,

00:05:42.717 --> 00:05:47.345
we need to understand that people will
treat them differently than other devices,

00:05:47.369 --> 00:05:49.213
and that in some cases,

00:05:49.237 --> 00:05:52.409
for example, the case of a soldier
who becomes emotionally attached

00:05:52.433 --> 00:05:54.480
to the robot that they work with,

00:05:54.504 --> 00:05:57.008
that can be anything
from inefficient to dangerous.

00:05:58.551 --> 00:06:00.689
But in other cases,
it can actually be useful

00:06:00.713 --> 00:06:03.336
to foster this emotional
connection to robots.

00:06:04.184 --> 00:06:06.318
We're already seeing some great use cases,

00:06:06.342 --> 00:06:08.946
for example, robots working
with autistic children

00:06:08.970 --> 00:06:12.604
to engage them in ways
that we haven't seen previously,

00:06:12.628 --> 00:06:16.628
or robots working with teachers to engage
kids in learning with new results.

00:06:17.433 --> 00:06:18.814
And it's not just for kids.

00:06:19.750 --> 00:06:22.973
Early studies show that robots
can help doctors and patients

00:06:22.997 --> 00:06:24.424
in health care settings.

00:06:25.535 --> 00:06:27.345
This is the PARO baby seal robot.

00:06:27.369 --> 00:06:30.654
It's used in nursing homes
and with dementia patients.

00:06:30.678 --> 00:06:32.248
It's been around for a while.

00:06:32.272 --> 00:06:35.597
And I remember, years ago,
being at a party

00:06:35.621 --> 00:06:38.192
and telling someone about this robot,

00:06:38.216 --> 00:06:40.342
and her response was,

00:06:40.366 --> 00:06:41.628
"Oh my gosh.

00:06:42.508 --> 00:06:43.696
That's horrible.

00:06:45.056 --> 00:06:48.453
I can't believe we're giving people
robots instead of human care."

00:06:50.540 --> 00:06:52.415
And this is a really common response,

00:06:52.439 --> 00:06:54.938
and I think it's absolutely correct,

00:06:54.962 --> 00:06:57.002
because that would be terrible.

00:06:57.795 --> 00:07:00.279
But in this case,
it's not what this robot replaces.

00:07:00.858 --> 00:07:03.978
What this robot replaces is animal therapy

00:07:04.002 --> 00:07:07.200
in contexts where
we can't use real animals

00:07:07.224 --> 00:07:08.392
but we can use robots,

00:07:08.416 --> 00:07:13.646
because people will consistently treat
them more like an animal than a device.

00:07:15.502 --> 00:07:17.882
Acknowledging this emotional
connection to robots

00:07:17.906 --> 00:07:19.875
can also help us anticipate challenges

00:07:19.899 --> 00:07:23.350
as these devices move into more intimate
areas of people's lives.

00:07:24.111 --> 00:07:27.515
For example, is it OK
if your child's teddy bear robot

00:07:27.539 --> 00:07:29.776
records private conversations?

00:07:29.800 --> 00:07:33.863
Is it OK if your sex robot
has compelling in-app purchases?

00:07:33.887 --> 00:07:35.283
(Laughter)

00:07:35.307 --> 00:07:37.808
Because robots plus capitalism

00:07:37.832 --> 00:07:41.537
equals questions around
consumer protection and privacy.

00:07:42.549 --> 00:07:44.161
And those aren't the only reasons

00:07:44.185 --> 00:07:46.755
that our behavior around
these machines could matter.

00:07:48.747 --> 00:07:52.017
A few years after that first
initial experience I had

00:07:52.041 --> 00:07:54.352
with this baby dinosaur robot,

00:07:54.376 --> 00:07:56.877
I did a workshop
with my friend Hannes Gassert.

00:07:56.901 --> 00:07:59.798
And we took five
of these baby dinosaur robots

00:07:59.822 --> 00:08:02.275
and we gave them to five teams of people.

00:08:02.299 --> 00:08:03.996
And we had them name them

00:08:04.020 --> 00:08:07.829
and play with them and interact with them
for about an hour.

00:08:08.707 --> 00:08:10.913
And then we unveiled
a hammer and a hatchet

00:08:10.937 --> 00:08:13.215
and we told them to torture
and kill the robots.

00:08:13.239 --> 00:08:16.246
(Laughter)

00:08:16.857 --> 00:08:19.151
And this turned out to be
a little more dramatic

00:08:19.175 --> 00:08:20.453
than we expected it to be,

00:08:20.477 --> 00:08:23.549
because none of the participants
would even so much as strike

00:08:23.573 --> 00:08:24.880
these baby dinosaur robots,

00:08:24.904 --> 00:08:30.054
so we had to improvise a little,
and at some point, we said,

00:08:30.078 --> 00:08:34.515
"OK, you can save your team's robot
if you destroy another team's robot."

00:08:34.539 --> 00:08:36.394
(Laughter)

00:08:36.839 --> 00:08:39.034
And even that didn't work.
They couldn't do it.

00:08:39.058 --> 00:08:40.209
So finally, we said,

00:08:40.233 --> 00:08:42.265
"We're going to destroy all of the robots

00:08:42.289 --> 00:08:44.574
unless someone takes
a hatchet to one of them."

00:08:45.586 --> 00:08:49.165
And this guy stood up,
and he took the hatchet,

00:08:49.189 --> 00:08:51.895
and the whole room winced
as he brought the hatchet down

00:08:51.919 --> 00:08:53.699
on the robot's neck,

00:08:53.723 --> 00:09:00.061
and there was this half-joking,
half-serious moment of silence in the room

00:09:00.085 --> 00:09:01.783
for this fallen robot.

00:09:01.807 --> 00:09:03.213
(Laughter)

00:09:03.237 --> 00:09:06.931
So that was a really
interesting experience.

00:09:06.955 --> 00:09:09.414
Now, it wasn't a controlled
study, obviously,

00:09:09.438 --> 00:09:12.288
but it did lead to some
later research that I did at MIT

00:09:12.312 --> 00:09:14.540
with Palash Nandy and Cynthia Breazeal,

00:09:14.564 --> 00:09:18.191
where we had people come into the lab
and smash these HEXBUGs

00:09:18.215 --> 00:09:21.302
that move around in a really
lifelike way, like insects.

00:09:21.326 --> 00:09:24.460
So instead of choosing something cute
that people are drawn to,

00:09:24.484 --> 00:09:26.577
we chose something more basic,

00:09:26.601 --> 00:09:30.081
and what we found
was that high-empathy people

00:09:30.105 --> 00:09:32.248
would hesitate more to hit the HEXBUGS.

00:09:33.575 --> 00:09:35.139
Now this is just a little study,

00:09:35.163 --> 00:09:37.552
but it's part of a larger body of research

00:09:37.576 --> 00:09:40.520
that is starting to indicate
that there may be a connection

00:09:40.544 --> 00:09:42.917
between people's tendencies for empathy

00:09:42.941 --> 00:09:44.917
and their behavior around robots.

00:09:45.721 --> 00:09:49.348
But my question for the coming era
of human-robot interaction

00:09:49.372 --> 00:09:52.427
is not: "Do we empathize with robots?"

00:09:53.211 --> 00:09:56.131
It's: "Can robots change
people's empathy?"

00:09:57.489 --> 00:09:59.776
Is there reason to, for example,

00:09:59.800 --> 00:10:02.133
prevent your child
from kicking a robotic dog,

00:10:03.228 --> 00:10:06.142
not just out of respect for property,

00:10:06.166 --> 00:10:09.119
but because the child might be
more likely to kick a real dog?

00:10:10.507 --> 00:10:12.390
And again, it's not just kids.

00:10:13.564 --> 00:10:17.620
This is the violent video games question,
but it's on a completely new level

00:10:17.644 --> 00:10:22.404
because of this visceral physicality
that we respond more intensely to

00:10:22.428 --> 00:10:23.975
than to images on a screen.

00:10:25.674 --> 00:10:28.252
When we behave violently towards robots,

00:10:28.276 --> 00:10:31.396
specifically robots
that are designed to mimic life,

00:10:31.420 --> 00:10:35.312
is that a healthy outlet
for violent behavior

00:10:35.336 --> 00:10:37.880
or is that training our cruelty muscles?

00:10:39.511 --> 00:10:40.661
We don't know ...

00:10:42.622 --> 00:10:46.567
But the answer to this question has
the potential to impact human behavior,

00:10:46.591 --> 00:10:49.359
it has the potential
to impact social norms,

00:10:49.383 --> 00:10:53.232
it has the potential to inspire rules
around what we can and can't do

00:10:53.256 --> 00:10:54.407
with certain robots,

00:10:54.431 --> 00:10:56.279
similar to our animal cruelty laws.

00:10:57.228 --> 00:11:00.092
Because even if robots can't feel,

00:11:00.116 --> 00:11:03.196
our behavior towards them
might matter for us.

00:11:04.889 --> 00:11:07.744
And regardless of whether
we end up changing our rules,

00:11:08.926 --> 00:11:12.482
robots might be able to help us
come to a new understanding of ourselves.

00:11:14.276 --> 00:11:16.592
Most of what I've learned
over the past 10 years

00:11:16.616 --> 00:11:18.854
has not been about technology at all.

00:11:18.878 --> 00:11:21.381
It's been about human psychology

00:11:21.405 --> 00:11:24.008
and empathy and how we relate to others.

00:11:25.524 --> 00:11:27.889
Because when a child is kind to a Roomba,

00:11:29.262 --> 00:11:33.277
when a soldier tries to save
a robot on the battlefield,

00:11:33.301 --> 00:11:36.939
or when a group of people refuses
to harm a robotic baby dinosaur,

00:11:38.248 --> 00:11:41.439
those robots aren't just motors
and gears and algorithms.

00:11:42.501 --> 00:11:44.406
They're reflections of our own humanity.

00:11:45.523 --> 00:11:46.674
Thank you.

00:11:46.698 --> 00:11:50.095
(Applause)

