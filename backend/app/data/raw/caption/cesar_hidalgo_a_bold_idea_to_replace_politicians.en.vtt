WEBVTT
Kind: captions
Language: en

00:00:13.373 --> 00:00:15.117
Is it just me,

00:00:15.141 --> 00:00:17.473
or are there other people here

00:00:17.497 --> 00:00:19.841
that are a little bit
disappointed with democracy?

00:00:20.986 --> 00:00:23.322
(Applause)

00:00:24.141 --> 00:00:26.212
So let's look at a few numbers.

00:00:26.934 --> 00:00:29.107
If we look across the world,

00:00:29.131 --> 00:00:33.023
the median turnout
in presidential elections

00:00:33.047 --> 00:00:34.698
over the last 30 years

00:00:34.722 --> 00:00:37.344
has been just 67 percent.

00:00:38.329 --> 00:00:40.302
Now, if we go to Europe

00:00:40.326 --> 00:00:44.754
and we look at people that participated
in EU parliamentary elections,

00:00:44.778 --> 00:00:46.849
the median turnout in those elections

00:00:46.873 --> 00:00:48.977
is just 42 percent.

00:00:50.125 --> 00:00:51.794
Now let's go to New York,

00:00:51.818 --> 00:00:56.499
and let's see how many people voted
in the last election for mayor.

00:00:56.523 --> 00:01:00.340
We will find that only
24 percent of people showed up to vote.

00:01:01.063 --> 00:01:04.158
What that means is that,
if "Friends" was still running,

00:01:04.182 --> 00:01:07.530
Joey and maybe Phoebe
would have shown up to vote.

00:01:07.554 --> 00:01:08.844
(Laughter)

00:01:09.434 --> 00:01:13.860
And you cannot blame them
because people are tired of politicians.

00:01:13.884 --> 00:01:17.771
And people are tired of other people
using the data that they have generated

00:01:17.795 --> 00:01:19.993
to communicate with
their friends and family,

00:01:20.017 --> 00:01:22.111
to target political propaganda at them.

00:01:22.519 --> 00:01:25.247
But the thing about this
is that this is not new.

00:01:25.271 --> 00:01:28.496
Nowadays, people use likes
to target propaganda at you

00:01:28.520 --> 00:01:31.893
before they use your zip code
or your gender or your age,

00:01:31.917 --> 00:01:35.487
because the idea of targeting people
with propaganda for political purposes

00:01:35.511 --> 00:01:37.103
is as old as politics.

00:01:37.530 --> 00:01:39.808
And the reason why that idea is there

00:01:39.832 --> 00:01:43.304
is because democracy
has a basic vulnerability.

00:01:43.710 --> 00:01:45.642
This is the idea of a representative.

00:01:46.049 --> 00:01:49.993
In principle, democracy is the ability
of people to exert power.

00:01:50.017 --> 00:01:53.835
But in practice, we have to delegate
that power to a representative

00:01:53.859 --> 00:01:56.114
that can exert that power for us.

00:01:56.561 --> 00:01:58.386
That representative is a bottleneck,

00:01:58.410 --> 00:01:59.707
or a weak spot.

00:01:59.731 --> 00:02:03.680
It is the place that you want to target
if you want to attack democracy

00:02:03.704 --> 00:02:07.193
because you can capture democracy
by either capturing that representative

00:02:07.217 --> 00:02:09.365
or capturing the way
that people choose it.

00:02:10.065 --> 00:02:11.481
So the big question is:

00:02:11.505 --> 00:02:13.209
Is this the end of history?

00:02:13.989 --> 00:02:17.092
Is this the best that we can do

00:02:17.878 --> 00:02:20.945
or, actually, are there alternatives?

00:02:22.130 --> 00:02:24.484
Some people have been thinking
about alternatives,

00:02:24.508 --> 00:02:28.187
and one of the ideas that is out there
is the idea of direct democracy.

00:02:28.790 --> 00:02:31.269
This is the idea of bypassing
politicians completely

00:02:31.293 --> 00:02:33.696
and having people vote directly on issues,

00:02:33.720 --> 00:02:35.985
having people vote directly on bills.

00:02:36.415 --> 00:02:37.751
But this idea is naive

00:02:37.775 --> 00:02:40.946
because there's too many things
that we would need to choose.

00:02:40.970 --> 00:02:43.752
If you look at the 114th US Congress,

00:02:43.776 --> 00:02:46.263
you will have seen that
the House of Representatives

00:02:46.287 --> 00:02:49.176
considered more than 6,000 bills,

00:02:49.200 --> 00:02:51.856
the Senate considered
more than 3,000 bills

00:02:51.880 --> 00:02:54.688
and they approved more than 300 laws.

00:02:54.712 --> 00:02:56.291
Those would be many decisions

00:02:56.315 --> 00:02:58.502
that each person would have to make a week

00:02:58.526 --> 00:03:00.668
on topics that they know little about.

00:03:01.229 --> 00:03:03.510
So there's a big cognitive
bandwidth problem

00:03:03.534 --> 00:03:07.526
if we're going to try to think about
direct democracy as a viable alternative.

00:03:08.205 --> 00:03:12.640
So some people think about the idea
of liquid democracy, or fluid democracy,

00:03:12.664 --> 00:03:16.440
which is the idea that you endorse
your political power to someone,

00:03:16.464 --> 00:03:18.164
who can endorse it to someone else,

00:03:18.188 --> 00:03:20.729
and, eventually, you create
a large follower network

00:03:20.753 --> 00:03:24.047
in which, at the end, there's a few people
that are making decisions

00:03:24.071 --> 00:03:27.214
on behalf of all of their followers
and their followers.

00:03:28.326 --> 00:03:32.455
But this idea also doesn't solve
the problem of the cognitive bandwidth

00:03:32.479 --> 00:03:36.356
and, to be honest, it's also quite similar
to the idea of having a representative.

00:03:36.795 --> 00:03:40.263
So what I'm going to do today is
I'm going to be a little bit provocative,

00:03:40.277 --> 00:03:42.577
and I'm going to ask you, well:

00:03:42.601 --> 00:03:49.163
What if, instead of trying
to bypass politicians,

00:03:49.187 --> 00:03:51.387
we tried to automate them?

00:03:57.871 --> 00:04:00.797
The idea of automation is not new.

00:04:00.821 --> 00:04:02.901
It was started more than 300 years ago,

00:04:02.925 --> 00:04:05.993
when French weavers decided
to automate the loom.

00:04:06.820 --> 00:04:11.180
The winner of that industrial war
was Joseph-Marie Jacquard.

00:04:11.204 --> 00:04:12.955
He was a French weaver and merchant

00:04:12.979 --> 00:04:15.419
that married the loom
with the steam engine

00:04:15.443 --> 00:04:17.633
to create autonomous looms.

00:04:17.657 --> 00:04:20.410
And in those autonomous looms,
he gained control.

00:04:20.434 --> 00:04:24.319
He could now make fabrics that were
more complex and more sophisticated

00:04:24.343 --> 00:04:26.471
than the ones they
were able to do by hand.

00:04:27.193 --> 00:04:29.825
But also, by winning that industrial war,

00:04:29.849 --> 00:04:33.373
he laid out what has become
the blueprint of automation.

00:04:34.135 --> 00:04:37.005
The way that we automate things
for the last 300 years

00:04:37.029 --> 00:04:38.411
has always been the same:

00:04:39.006 --> 00:04:41.515
we first identify a need,

00:04:41.539 --> 00:04:44.723
then we create a tool
to satisfy that need,

00:04:44.747 --> 00:04:46.787
like the loom, in this case,

00:04:46.811 --> 00:04:49.202
and then we study how people use that tool

00:04:49.226 --> 00:04:50.711
to automate that user.

00:04:51.242 --> 00:04:54.303
That's how we came
from the mechanical loom

00:04:54.327 --> 00:04:56.223
to the autonomous loom,

00:04:56.247 --> 00:04:58.367
and that took us a thousand years.

00:04:58.391 --> 00:05:00.462
Now, it's taken us only a hundred years

00:05:00.486 --> 00:05:03.697
to use the same script
to automate the car.

00:05:05.286 --> 00:05:07.738
But the thing is that, this time around,

00:05:07.762 --> 00:05:09.891
automation is kind of for real.

00:05:09.915 --> 00:05:13.236
This is a video that a colleague of mine
from Toshiba shared with me

00:05:13.260 --> 00:05:16.519
that shows the factory
that manufactures solid state drives.

00:05:16.543 --> 00:05:18.561
The entire factory is a robot.

00:05:18.585 --> 00:05:20.510
There are no humans in that factory.

00:05:21.033 --> 00:05:23.254
And the robots are soon
to leave the factories

00:05:23.278 --> 00:05:25.300
and become part of our world,

00:05:25.324 --> 00:05:27.159
become part of our workforce.

00:05:27.183 --> 00:05:28.956
So what I do in my day job

00:05:28.980 --> 00:05:32.972
is actually create tools that integrate
data for entire countries

00:05:32.996 --> 00:05:36.462
so that we can ultimately have
the foundations that we need

00:05:36.486 --> 00:05:40.173
for a future in which we need
to also manage those machines.

00:05:41.195 --> 00:05:44.101
But today, I'm not here
to talk to you about these tools

00:05:44.125 --> 00:05:45.949
that integrate data for countries.

00:05:46.463 --> 00:05:49.085
But I'm here to talk to you
about another idea

00:05:49.109 --> 00:05:53.974
that might help us think about how to use
artificial intelligence in democracy.

00:05:53.998 --> 00:05:58.731
Because the tools that I build
are designed for executive decisions.

00:05:58.755 --> 00:06:02.597
These are decisions that can be cast
in some sort of term of objectivity --

00:06:02.621 --> 00:06:04.366
public investment decisions.

00:06:04.885 --> 00:06:07.516
But there are decisions
that are legislative,

00:06:07.540 --> 00:06:11.327
and these decisions that are legislative
require communication among people

00:06:11.351 --> 00:06:13.051
that have different points of view,

00:06:13.075 --> 00:06:15.688
require participation, require debate,

00:06:15.712 --> 00:06:17.190
require deliberation.

00:06:18.241 --> 00:06:21.045
And for a long time,
we have thought that, well,

00:06:21.069 --> 00:06:24.529
what we need to improve democracy
is actually more communication.

00:06:24.553 --> 00:06:28.262
So all of the technologies that we have
advanced in the context of democracy,

00:06:28.286 --> 00:06:31.064
whether they are newspapers
or whether it is social media,

00:06:31.088 --> 00:06:33.470
have tried to provide us
with more communication.

00:06:34.103 --> 00:06:35.925
But we've been down that rabbit hole,

00:06:35.949 --> 00:06:38.697
and we know that's not
what's going to solve the problem.

00:06:38.721 --> 00:06:40.717
Because it's not a communication problem,

00:06:40.741 --> 00:06:42.489
it's a cognitive bandwidth problem.

00:06:42.513 --> 00:06:44.879
So if the problem is one
of cognitive bandwidth,

00:06:44.903 --> 00:06:47.490
well, adding more communication to people

00:06:47.514 --> 00:06:50.258
is not going to be
what's going to solve it.

00:06:50.282 --> 00:06:53.395
What we are going to need instead
is to have other technologies

00:06:53.419 --> 00:06:56.465
that help us deal with
some of the communication

00:06:56.489 --> 00:06:58.731
that we are overloaded with.

00:06:58.755 --> 00:07:00.454
Think of, like, a little avatar,

00:07:00.478 --> 00:07:01.817
a software agent,

00:07:01.841 --> 00:07:03.719
a digital Jiminy Cricket --

00:07:03.743 --> 00:07:04.981
(Laughter)

00:07:05.005 --> 00:07:09.017
that basically is able
to answer things on your behalf.

00:07:09.759 --> 00:07:11.546
And if we had that technology,

00:07:11.570 --> 00:07:14.048
we would be able to offload
some of the communication

00:07:14.072 --> 00:07:18.219
and help, maybe, make better decisions
or decisions at a larger scale.

00:07:18.860 --> 00:07:22.579
And the thing is that the idea
of software agents is also not new.

00:07:22.603 --> 00:07:24.712
We already use them all the time.

00:07:25.216 --> 00:07:26.737
We use software agents

00:07:26.761 --> 00:07:30.436
to choose the way that we're going
to drive to a certain location,

00:07:31.070 --> 00:07:33.171
the music that we're going to listen to

00:07:33.758 --> 00:07:36.779
or to get suggestions
for the next books that we should read.

00:07:37.994 --> 00:07:40.568
So there is an obvious idea
in the 21st century

00:07:40.592 --> 00:07:43.235
that was as obvious as the idea

00:07:43.259 --> 00:07:48.840
of putting together a steam engine
with a loom at the time of Jacquard.

00:07:49.538 --> 00:07:53.994
And that idea is combining
direct democracy with software agents.

00:07:54.849 --> 00:07:56.970
Imagine, for a second, a world

00:07:56.994 --> 00:08:00.160
in which, instead of having
a representative that represents you

00:08:00.184 --> 00:08:01.758
and millions of other people,

00:08:01.782 --> 00:08:04.818
you can have a representative
that represents only you,

00:08:05.504 --> 00:08:07.758
with your nuanced political views --

00:08:07.782 --> 00:08:11.126
that weird combination
of libertarian and liberal

00:08:11.150 --> 00:08:13.542
and maybe a little bit
conservative on some issues

00:08:13.566 --> 00:08:15.674
and maybe very progressive on others.

00:08:15.698 --> 00:08:18.965
Politicians nowadays are packages,
and they're full of compromises.

00:08:18.989 --> 00:08:22.623
But you might have someone
that can represent only you,

00:08:22.647 --> 00:08:24.499
if you are willing to give up the idea

00:08:24.523 --> 00:08:26.772
that that representative is a human.

00:08:27.229 --> 00:08:29.311
If that representative
is a software agent,

00:08:29.335 --> 00:08:33.505
we could have a senate that has
as many senators as we have citizens.

00:08:33.529 --> 00:08:36.387
And those senators are going to be able
to read every bill

00:08:36.411 --> 00:08:39.158
and they're going to be able
to vote on each one of them.

00:08:39.822 --> 00:08:42.778
So there's an obvious idea
that maybe we want to consider.

00:08:42.802 --> 00:08:45.224
But I understand that in this day and age,

00:08:45.248 --> 00:08:47.137
this idea might be quite scary.

00:08:48.391 --> 00:08:51.831
In fact, thinking of a robot
coming from the future

00:08:51.855 --> 00:08:53.528
to help us run our governments

00:08:53.552 --> 00:08:55.183
sounds terrifying.

00:08:56.223 --> 00:08:57.874
But we've been there before.

00:08:57.898 --> 00:08:59.171
(Laughter)

00:08:59.195 --> 00:09:01.690
And actually he was quite a nice guy.

00:09:03.677 --> 00:09:10.111
So what would the Jacquard loom
version of this idea look like?

00:09:10.135 --> 00:09:12.036
It would be a very simple system.

00:09:12.060 --> 00:09:15.518
Imagine a system that you log in
and you create your avatar,

00:09:15.542 --> 00:09:17.998
and then you're going
to start training your avatar.

00:09:18.022 --> 00:09:20.704
So you can provide your avatar
with your reading habits,

00:09:20.728 --> 00:09:22.589
or connect it to your social media,

00:09:22.613 --> 00:09:25.021
or you can connect it to other data,

00:09:25.045 --> 00:09:27.317
for example by taking
psychological tests.

00:09:27.341 --> 00:09:30.309
And the nice thing about this
is that there's no deception.

00:09:30.333 --> 00:09:33.672
You are not providing data to communicate
with your friends and family

00:09:33.696 --> 00:09:36.847
that then gets used in a political system.

00:09:36.871 --> 00:09:40.575
You are providing data to a system
that is designed to be used

00:09:40.599 --> 00:09:42.715
to make political decisions
on your behalf.

00:09:43.264 --> 00:09:47.244
Then you take that data and you choose
a training algorithm,

00:09:47.268 --> 00:09:48.831
because it's an open marketplace

00:09:48.855 --> 00:09:51.641
in which different people
can submit different algorithms

00:09:51.665 --> 00:09:56.059
to predict how you're going to vote,
based on the data you have provided.

00:09:56.083 --> 00:09:59.538
And the system is open,
so nobody controls the algorithms;

00:09:59.562 --> 00:10:01.674
there are algorithms
that become more popular

00:10:01.698 --> 00:10:03.421
and others that become less popular.

00:10:03.445 --> 00:10:05.252
Eventually, you can audit the system.

00:10:05.276 --> 00:10:07.157
You can see how your avatar is working.

00:10:07.181 --> 00:10:09.333
If you like it,
you can leave it on autopilot.

00:10:09.357 --> 00:10:11.419
If you want to be
a little more controlling,

00:10:11.443 --> 00:10:13.411
you can actually choose that they ask you

00:10:13.435 --> 00:10:15.503
every time they're going
to make a decision,

00:10:15.527 --> 00:10:17.162
or you can be anywhere in between.

00:10:17.186 --> 00:10:19.591
One of the reasons
why we use democracy so little

00:10:19.615 --> 00:10:23.183
may be because democracy
has a very bad user interface.

00:10:23.207 --> 00:10:25.690
And if we improve the user
interface of democracy,

00:10:25.714 --> 00:10:27.841
we might be able to use it more.

00:10:28.452 --> 00:10:31.659
Of course, there's a lot of questions
that you might have.

00:10:32.473 --> 00:10:34.634
Well, how do you train these avatars?

00:10:34.658 --> 00:10:36.552
How do you keep the data secure?

00:10:36.576 --> 00:10:39.824
How do you keep the systems
distributed and auditable?

00:10:39.848 --> 00:10:41.910
How about my grandmother,
who's 80 years old

00:10:41.946 --> 00:10:43.906
and doesn't know how to use the internet?

00:10:44.262 --> 00:10:46.483
Trust me, I've heard them all.

00:10:46.507 --> 00:10:51.067
So when you think about an idea like this,
you have to beware of pessimists

00:10:51.091 --> 00:10:55.410
because they are known to have
a problem for every solution.

00:10:55.434 --> 00:10:57.259
(Laughter)

00:10:57.283 --> 00:11:00.323
So I want to invite you to think
about the bigger ideas.

00:11:00.347 --> 00:11:03.973
The questions I just showed you
are little ideas

00:11:03.997 --> 00:11:06.899
because they are questions
about how this would not work.

00:11:07.502 --> 00:11:09.483
The big ideas are ideas of:

00:11:09.507 --> 00:11:11.314
What else can you do with this

00:11:11.338 --> 00:11:13.227
if this would happen to work?

00:11:13.774 --> 00:11:17.219
And one of those ideas is,
well, who writes the laws?

00:11:17.854 --> 00:11:22.077
In the beginning, we could have
the avatars that we already have,

00:11:22.101 --> 00:11:25.598
voting on laws that are written
by the senators or politicians

00:11:25.622 --> 00:11:26.973
that we already have.

00:11:27.491 --> 00:11:29.205
But if this were to work,

00:11:29.902 --> 00:11:32.252
you could write an algorithm

00:11:32.276 --> 00:11:34.426
that could try to write a law

00:11:34.450 --> 00:11:36.871
that would get a certain
percentage of approval,

00:11:36.895 --> 00:11:38.677
and you could reverse the process.

00:11:38.701 --> 00:11:42.213
Now, you might think that this idea
is ludicrous and we should not do it,

00:11:42.237 --> 00:11:45.023
but you cannot deny that it's an idea
that is only possible

00:11:45.047 --> 00:11:48.067
in a world in which direct democracy
and software agents

00:11:48.091 --> 00:11:50.747
are a viable form of participation.

00:11:52.596 --> 00:11:55.349
So how do we start the revolution?

00:11:56.238 --> 00:11:59.548
We don't start this revolution
with picket fences or protests

00:11:59.572 --> 00:12:03.762
or by demanding our current politicians
to be changed into robots.

00:12:03.786 --> 00:12:05.335
That's not going to work.

00:12:05.359 --> 00:12:06.971
This is much more simple,

00:12:06.995 --> 00:12:08.154
much slower

00:12:08.178 --> 00:12:09.592
and much more humble.

00:12:09.616 --> 00:12:13.965
We start this revolution by creating
simple systems like this in grad schools,

00:12:13.989 --> 00:12:16.083
in libraries, in nonprofits.

00:12:16.107 --> 00:12:18.761
And we try to figure out
all of those little questions

00:12:18.785 --> 00:12:20.006
and those little problems

00:12:20.030 --> 00:12:23.931
that we're going to have to figure out
to make this idea something viable,

00:12:23.955 --> 00:12:26.306
to make this idea something
that we can trust.

00:12:26.330 --> 00:12:29.965
And as we create those systems that have
a hundred people, a thousand people,

00:12:29.989 --> 00:12:33.759
a hundred thousand people voting
in ways that are not politically binding,

00:12:33.783 --> 00:12:35.801
we're going to develop trust in this idea,

00:12:35.825 --> 00:12:37.344
the world is going to change,

00:12:37.368 --> 00:12:40.243
and those that are as little
as my daughter is right now

00:12:40.267 --> 00:12:41.604
are going to grow up.

00:12:42.580 --> 00:12:44.949
And by the time my daughter is my age,

00:12:44.973 --> 00:12:49.409
maybe this idea, that I know
today is very crazy,

00:12:49.433 --> 00:12:53.567
might not be crazy to her
and to her friends.

00:12:53.956 --> 00:12:55.793
And at that point,

00:12:55.817 --> 00:12:58.420
we will be at the end of our history,

00:12:58.444 --> 00:13:01.265
but they will be
at the beginning of theirs.

00:13:01.646 --> 00:13:02.829
Thank you.

00:13:02.853 --> 00:13:05.895
(Applause)

