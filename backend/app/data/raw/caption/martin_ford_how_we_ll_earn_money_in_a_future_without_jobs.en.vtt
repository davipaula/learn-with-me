WEBVTT
Kind: captions
Language: en

00:00:12.787 --> 00:00:15.635
I'm going to begin with a scary question:

00:00:15.659 --> 00:00:18.409
Are we headed toward
a future without jobs?

00:00:18.987 --> 00:00:21.056
The remarkable progress that we're seeing

00:00:21.080 --> 00:00:22.941
in technologies like self-driving cars

00:00:22.965 --> 00:00:26.030
has led to an explosion
of interest in this question,

00:00:26.054 --> 00:00:28.204
but because it's something
that's been asked

00:00:28.228 --> 00:00:29.484
so many times in the past,

00:00:29.508 --> 00:00:31.348
maybe what we should really be asking

00:00:31.372 --> 00:00:34.272
is whether this time is really different.

00:00:35.252 --> 00:00:38.213
The fear that automation
might displace workers

00:00:38.237 --> 00:00:40.354
and potentially lead
to lots of unemployment

00:00:40.378 --> 00:00:44.266
goes back at a minimum 200 years
to the Luddite revolts in England.

00:00:44.290 --> 00:00:47.486
And since then, this concern
has come up again and again.

00:00:47.510 --> 00:00:48.671
I'm going to guess

00:00:48.695 --> 00:00:53.161
that most of you have probably never
heard of the Triple Revolution report,

00:00:53.185 --> 00:00:55.478
but this was a very prominent report.

00:00:55.502 --> 00:00:58.033
It was put together
by a brilliant group of people --

00:00:58.057 --> 00:01:01.114
it actually included
two Nobel laureates --

00:01:01.138 --> 00:01:04.361
and this report was presented
to the President of the United States,

00:01:04.385 --> 00:01:09.879
and it argued that the US was on the brink
of economic and social upheaval

00:01:09.903 --> 00:01:13.005
because industrial automation
was going to put millions of people

00:01:13.029 --> 00:01:14.181
out of work.

00:01:14.205 --> 00:01:17.862
Now, that report was delivered
to President Lyndon Johnson

00:01:17.886 --> 00:01:19.687
in March of 1964.

00:01:19.711 --> 00:01:21.927
So that's now over 50 years,

00:01:21.951 --> 00:01:24.009
and, of course, that
hasn't really happened.

00:01:24.033 --> 00:01:26.177
And that's been the story again and again.

00:01:26.201 --> 00:01:28.310
This alarm has been raised repeatedly,

00:01:28.334 --> 00:01:30.347
but it's always been a false alarm.

00:01:30.371 --> 00:01:32.180
And because it's been a false alarm,

00:01:32.204 --> 00:01:35.011
it's led to a very conventional way
of thinking about this.

00:01:35.035 --> 00:01:37.567
And that says essentially that yes,

00:01:37.591 --> 00:01:40.139
technology may devastate
entire industries.

00:01:40.163 --> 00:01:43.895
It may wipe out whole occupations
and types of work.

00:01:43.919 --> 00:01:45.527
But at the same time, of course,

00:01:45.551 --> 00:01:47.902
progress is going to lead
to entirely new things.

00:01:47.926 --> 00:01:50.888
So there will be new industries
that will arise in the future,

00:01:50.912 --> 00:01:53.770
and those industries, of course,
will have to hire people.

00:01:53.794 --> 00:01:56.032
There'll be new kinds of work
that will appear,

00:01:56.056 --> 00:01:59.266
and those might be things that today
we can't really even imagine.

00:01:59.290 --> 00:02:01.037
And that has been the story so far,

00:02:01.061 --> 00:02:02.555
and it's been a positive story.

00:02:03.095 --> 00:02:06.420
It turns out that the new jobs
that have been created

00:02:06.444 --> 00:02:08.914
have generally been
a lot better than the old ones.

00:02:08.938 --> 00:02:11.594
They have, for example,
been more engaging.

00:02:11.618 --> 00:02:15.047
They've been in safer,
more comfortable work environments,

00:02:15.071 --> 00:02:16.751
and, of course, they've paid more.

00:02:16.775 --> 00:02:18.640
So it has been a positive story.

00:02:18.664 --> 00:02:20.872
That's the way things
have played out so far.

00:02:21.292 --> 00:02:24.240
But there is one particular
class of worker

00:02:24.264 --> 00:02:26.516
for whom the story
has been quite different.

00:02:27.938 --> 00:02:29.088
For these workers,

00:02:29.112 --> 00:02:32.133
technology has completely
decimated their work,

00:02:32.157 --> 00:02:35.371
and it really hasn't created
any new opportunities at all.

00:02:35.395 --> 00:02:37.590
And these workers, of course,

00:02:37.614 --> 00:02:38.902
are horses.

00:02:38.926 --> 00:02:40.369
(Laughter)

00:02:40.393 --> 00:02:43.143
So I can ask a very provocative question:

00:02:43.167 --> 00:02:46.602
Is it possible that at some
point in the future,

00:02:46.626 --> 00:02:51.254
a significant fraction of the human
workforce is going to be made redundant

00:02:51.278 --> 00:02:52.980
in the way that horses were?

00:02:53.485 --> 00:02:56.485
Now, you might have a very visceral,
reflexive reaction to that.

00:02:56.509 --> 00:02:58.156
You might say, "That's absurd.

00:02:58.180 --> 00:03:01.849
How can you possibly compare
human beings to horses?"

00:03:02.437 --> 00:03:04.206
Horses, of course, are very limited,

00:03:04.230 --> 00:03:07.123
and when cars and trucks
and tractors came along,

00:03:07.147 --> 00:03:09.192
horses really had nowhere else to turn.

00:03:09.844 --> 00:03:12.204
People, on the other hand,
are intelligent;

00:03:12.228 --> 00:03:14.013
we can learn, we can adapt.

00:03:14.037 --> 00:03:15.201
And in theory,

00:03:15.225 --> 00:03:18.352
that ought to mean that we can
always find something new to do,

00:03:18.376 --> 00:03:21.682
and that we can always remain
relevant to the future economy.

00:03:21.706 --> 00:03:24.143
But here's the really
critical thing to understand.

00:03:24.790 --> 00:03:27.655
The machines that will threaten
workers in the future

00:03:27.679 --> 00:03:30.913
are really nothing like those cars
and trucks and tractors

00:03:30.937 --> 00:03:32.553
that displaced horses.

00:03:32.577 --> 00:03:37.416
The future is going to be full
of thinking, learning, adapting machines.

00:03:37.440 --> 00:03:38.848
And what that really means

00:03:38.872 --> 00:03:41.706
is that technology is finally
beginning to encroach

00:03:41.730 --> 00:03:44.579
on that fundamental human capability --

00:03:44.603 --> 00:03:47.406
the thing that makes us
so different from horses,

00:03:47.430 --> 00:03:49.664
and the very thing that, so far,

00:03:49.688 --> 00:03:52.335
has allowed us to stay ahead
of the march of progress

00:03:52.359 --> 00:03:53.548
and remain relevant,

00:03:53.572 --> 00:03:56.639
and, in fact, indispensable
to the economy.

00:03:58.407 --> 00:04:00.902
So what is it that is really so different

00:04:00.926 --> 00:04:02.969
about today's information technology

00:04:02.993 --> 00:04:04.940
relative to what we've seen in the past?

00:04:04.964 --> 00:04:07.617
I would point to three fundamental things.

00:04:07.641 --> 00:04:12.050
The first thing is that we have seen
this ongoing process

00:04:12.074 --> 00:04:13.962
of exponential acceleration.

00:04:14.420 --> 00:04:16.515
I know you all know about Moore's law,

00:04:16.539 --> 00:04:18.835
but in fact, it's more
broad-based than that;

00:04:18.859 --> 00:04:22.009
it extends in many cases,
for example, to software,

00:04:22.033 --> 00:04:25.033
it extends to communications,
bandwidth and so forth.

00:04:25.057 --> 00:04:27.041
But the really key thing to understand

00:04:27.065 --> 00:04:30.936
is that this acceleration has now
been going on for a really long time.

00:04:30.960 --> 00:04:32.885
In fact, it's been going on for decades.

00:04:32.909 --> 00:04:35.665
If you measure from the late 1950s,

00:04:35.689 --> 00:04:38.114
when the first integrated
circuits were fabricated,

00:04:38.138 --> 00:04:42.923
we've seen something on the order
of 30 doublings in computational power

00:04:42.947 --> 00:04:44.103
since then.

00:04:44.127 --> 00:04:47.815
That's just an extraordinary number
of times to double any quantity,

00:04:47.839 --> 00:04:49.079
and what it really means

00:04:49.103 --> 00:04:51.627
is that we're now at a point
where we're going to see

00:04:51.651 --> 00:04:54.062
just an extraordinary amount
of absolute progress,

00:04:54.086 --> 00:04:57.061
and, of course, things are going
to continue to also accelerate

00:04:57.085 --> 00:04:58.244
from this point.

00:04:58.268 --> 00:05:00.808
So as we look forward
to the coming years and decades,

00:05:00.832 --> 00:05:03.170
I think that means
that we're going to see things

00:05:03.194 --> 00:05:04.867
that we're really not prepared for.

00:05:04.891 --> 00:05:06.968
We're going to see things
that astonish us.

00:05:06.992 --> 00:05:08.258
The second key thing

00:05:08.282 --> 00:05:12.188
is that the machines are,
in a limited sense, beginning to think.

00:05:12.212 --> 00:05:14.669
And by this, I don't mean human-level AI,

00:05:14.693 --> 00:05:17.629
or science fiction
artificial intelligence;

00:05:17.653 --> 00:05:22.115
I simply mean that machines and algorithms
are making decisions.

00:05:22.139 --> 00:05:25.999
They're solving problems,
and most importantly, they're learning.

00:05:26.023 --> 00:05:29.326
In fact, if there's one technology
that is truly central to this

00:05:29.350 --> 00:05:32.427
and has really become
the driving force behind this,

00:05:32.451 --> 00:05:33.623
it's machine learning,

00:05:33.647 --> 00:05:36.367
which is just becoming
this incredibly powerful,

00:05:36.391 --> 00:05:39.029
disruptive, scalable technology.

00:05:39.561 --> 00:05:42.030
One of the best examples
I've seen of that recently

00:05:42.054 --> 00:05:44.805
was what Google's DeepMind
division was able to do

00:05:44.829 --> 00:05:46.382
with its AlphaGo system.

00:05:46.406 --> 00:05:50.706
Now, this is the system that was able
to beat the best player in the world

00:05:50.730 --> 00:05:52.709
at the ancient game of Go.

00:05:52.733 --> 00:05:53.883
Now, at least to me,

00:05:53.907 --> 00:05:57.024
there are two things that really
stand out about the game of Go.

00:05:57.048 --> 00:05:59.344
One is that as you're playing the game,

00:05:59.368 --> 00:06:02.234
the number of configurations
that the board can be in

00:06:02.258 --> 00:06:03.669
is essentially infinite.

00:06:03.693 --> 00:06:07.526
There are actually more possibilities
than there are atoms in the universe.

00:06:07.980 --> 00:06:09.164
So what that means is,

00:06:09.188 --> 00:06:12.785
you're never going to be able to build
a computer to win at the game of Go

00:06:12.809 --> 00:06:14.989
the way chess was approached, for example,

00:06:15.013 --> 00:06:19.539
which is basically to throw
brute-force computational power at it.

00:06:19.563 --> 00:06:23.740
So clearly, a much more sophisticated,
thinking-like approach is needed.

00:06:24.368 --> 00:06:27.639
The second thing
that really stands out is that,

00:06:27.663 --> 00:06:30.310
if you talk to one
of the championship Go players,

00:06:30.334 --> 00:06:34.819
this person cannot necessarily
even really articulate what exactly it is

00:06:34.843 --> 00:06:37.058
they're thinking about
as they play the game.

00:06:37.082 --> 00:06:39.275
It's often something
that's very intuitive,

00:06:39.299 --> 00:06:42.621
it's almost just like a feeling
about which move they should make.

00:06:42.645 --> 00:06:44.052
So given those two qualities,

00:06:44.076 --> 00:06:48.013
I would say that playing Go
at a world champion level

00:06:48.037 --> 00:06:51.275
really ought to be something
that's safe from automation,

00:06:51.299 --> 00:06:55.745
and the fact that it isn't should really
raise a cautionary flag for us.

00:06:55.769 --> 00:06:59.686
And the reason is that we tend
to draw a very distinct line,

00:06:59.710 --> 00:07:03.219
and on one side of that line
are all the jobs and tasks

00:07:03.243 --> 00:07:07.991
that we perceive as being on some level
fundamentally routine and repetitive

00:07:08.015 --> 00:07:09.365
and predictable.

00:07:09.389 --> 00:07:12.247
And we know that these jobs
might be in different industries,

00:07:12.271 --> 00:07:15.644
they might be in different occupations
and at different skill levels,

00:07:15.668 --> 00:07:17.878
but because they are innately predictable,

00:07:17.902 --> 00:07:21.029
we know they're probably at some point
going to be susceptible

00:07:21.053 --> 00:07:22.230
to machine learning,

00:07:22.254 --> 00:07:23.673
and therefore, to automation.

00:07:23.697 --> 00:07:25.794
And make no mistake --
that's a lot of jobs.

00:07:25.818 --> 00:07:28.497
That's probably something
on the order of roughly half

00:07:28.521 --> 00:07:30.088
the jobs in the economy.

00:07:30.112 --> 00:07:32.271
But then on the other side of that line,

00:07:32.295 --> 00:07:36.366
we have all the jobs
that require some capability

00:07:36.390 --> 00:07:38.762
that we perceive as being uniquely human,

00:07:38.786 --> 00:07:41.009
and these are the jobs
that we think are safe.

00:07:41.033 --> 00:07:43.298
Now, based on what I know
about the game of Go,

00:07:43.322 --> 00:07:47.025
I would've guessed that it really ought
to be on the safe side of that line.

00:07:47.049 --> 00:07:50.227
But the fact that it isn't,
and that Google solved this problem,

00:07:50.251 --> 00:07:52.683
suggests that that line is going
to be very dynamic.

00:07:52.707 --> 00:07:53.886
It's going to shift,

00:07:53.910 --> 00:07:58.045
and it's going to shift in a way
that consumes more and more jobs and tasks

00:07:58.069 --> 00:08:01.086
that we currently perceive
as being safe from automation.

00:08:01.921 --> 00:08:03.578
The other key thing to understand

00:08:03.602 --> 00:08:08.740
is that this is by no means just about
low-wage jobs or blue-collar jobs,

00:08:08.764 --> 00:08:10.639
or jobs and tasks done by people

00:08:10.663 --> 00:08:12.767
that have relatively
low levels of education.

00:08:12.791 --> 00:08:14.315
There's lots of evidence to show

00:08:14.339 --> 00:08:17.499
that these technologies are rapidly
climbing the skills ladder.

00:08:17.523 --> 00:08:21.139
So we already see an impact
on professional jobs --

00:08:21.163 --> 00:08:25.598
tasks done by people like accountants,

00:08:25.622 --> 00:08:26.939
financial analysts,

00:08:26.963 --> 00:08:28.259
journalists,

00:08:28.283 --> 00:08:30.660
lawyers, radiologists and so forth.

00:08:30.684 --> 00:08:32.622
So a lot of the assumptions that we make

00:08:32.646 --> 00:08:35.866
about the kind of occupations
and tasks and jobs

00:08:35.890 --> 00:08:38.709
that are going to be threatened
by automation in the future

00:08:38.733 --> 00:08:40.931
are very likely to be
challenged going forward.

00:08:40.955 --> 00:08:42.655
So as we put these trends together,

00:08:42.679 --> 00:08:45.971
I think what it shows is that we could
very well end up in a future

00:08:45.995 --> 00:08:47.502
with significant unemployment.

00:08:48.254 --> 00:08:49.406
Or at a minimum,

00:08:49.430 --> 00:08:53.211
we could face lots of underemployment
or stagnant wages,

00:08:53.235 --> 00:08:55.332
maybe even declining wages.

00:08:56.142 --> 00:08:58.952
And, of course, soaring levels
of inequality.

00:08:58.976 --> 00:09:03.009
All of that, of course, is going to put
a terrific amount of stress

00:09:03.033 --> 00:09:04.950
on the fabric of society.

00:09:04.974 --> 00:09:08.033
But beyond that, there's also
a fundamental economic problem,

00:09:08.057 --> 00:09:13.252
and that arises because jobs
are currently the primary mechanism

00:09:13.276 --> 00:09:16.821
that distributes income,
and therefore purchasing power,

00:09:16.845 --> 00:09:21.977
to all the consumers that buy the products
and services we're producing.

00:09:22.831 --> 00:09:25.346
In order to have a vibrant market economy,

00:09:25.370 --> 00:09:27.490
you've got to have
lots and lots of consumers

00:09:27.514 --> 00:09:30.543
that are really capable of buying
the products and services

00:09:30.567 --> 00:09:31.718
that are being produced.

00:09:31.742 --> 00:09:34.128
If you don't have that,
then you run the risk

00:09:34.152 --> 00:09:35.567
of economic stagnation,

00:09:35.591 --> 00:09:39.260
or maybe even a declining economic spiral,

00:09:39.284 --> 00:09:41.598
as there simply aren't enough
customers out there

00:09:41.622 --> 00:09:44.081
to buy the products
and services being produced.

00:09:44.105 --> 00:09:46.033
It's really important to realize

00:09:46.057 --> 00:09:52.071
that all of us as individuals rely
on access to that market economy

00:09:52.095 --> 00:09:53.824
in order to be successful.

00:09:53.848 --> 00:09:58.284
You can visualize that by thinking
in terms of one really exceptional person.

00:09:58.308 --> 00:10:01.296
Imagine for a moment you take,
say, Steve Jobs,

00:10:01.320 --> 00:10:03.901
and you drop him
on an island all by himself.

00:10:03.925 --> 00:10:06.219
On that island, he's going
to be running around,

00:10:06.243 --> 00:10:08.781
gathering coconuts just like anyone else.

00:10:08.805 --> 00:10:10.993
He's really not going to be
anything special,

00:10:11.017 --> 00:10:14.189
and the reason, of course,
is that there is no market

00:10:14.213 --> 00:10:16.999
for him to scale
his incredible talents across.

00:10:17.023 --> 00:10:20.493
So access to this market
is really critical to us as individuals,

00:10:20.517 --> 00:10:24.539
and also to the entire system
in terms of it being sustainable.

00:10:25.063 --> 00:10:28.907
So the question then becomes:
What exactly could we do about this?

00:10:29.285 --> 00:10:32.517
And I think you can view this
through a very utopian framework.

00:10:32.541 --> 00:10:35.184
You can imagine a future
where we all have to work less,

00:10:35.208 --> 00:10:38.209
we have more time for leisure,

00:10:38.233 --> 00:10:40.161
more time to spend with our families,

00:10:40.185 --> 00:10:43.440
more time to do things that we find
genuinely rewarding

00:10:43.464 --> 00:10:44.621
and so forth.

00:10:44.645 --> 00:10:46.500
And I think that's a terrific vision.

00:10:46.524 --> 00:10:50.153
That's something that we should
absolutely strive to move toward.

00:10:50.177 --> 00:10:52.853
But at the same time, I think
we have to be realistic,

00:10:52.877 --> 00:10:54.270
and we have to realize

00:10:54.294 --> 00:10:59.154
that we're very likely to face
a significant income distribution problem.

00:10:59.178 --> 00:11:02.145
A lot of people are likely
to be left behind.

00:11:03.186 --> 00:11:05.590
And I think that in order
to solve that problem,

00:11:05.614 --> 00:11:07.712
we're ultimately going
to have to find a way

00:11:07.736 --> 00:11:10.342
to decouple incomes from traditional work.

00:11:10.366 --> 00:11:13.232
And the best, more straightforward
way I know to do that

00:11:13.256 --> 00:11:16.824
is some kind of a guaranteed income
or universal basic income.

00:11:16.848 --> 00:11:19.336
Now, basic income is becoming
a very important idea.

00:11:19.360 --> 00:11:21.499
It's getting a lot
of traction and attention,

00:11:21.523 --> 00:11:23.796
there are a lot of important
pilot projects

00:11:23.820 --> 00:11:25.995
and experiments going on
throughout the world.

00:11:26.628 --> 00:11:29.828
My own view is that a basic income
is not a panacea;

00:11:29.852 --> 00:11:32.384
it's not necessarily
a plug-and-play solution,

00:11:32.408 --> 00:11:34.043
but rather, it's a place to start.

00:11:34.067 --> 00:11:36.849
It's an idea that we can
build on and refine.

00:11:36.873 --> 00:11:39.690
For example, one thing that I have
written quite a lot about

00:11:39.714 --> 00:11:44.306
is the possibility of incorporating
explicit incentives into a basic income.

00:11:44.930 --> 00:11:46.099
To illustrate that,

00:11:46.123 --> 00:11:48.891
imagine that you are a struggling
high school student.

00:11:48.915 --> 00:11:51.749
Imagine that you are at risk
of dropping out of school.

00:11:52.289 --> 00:11:55.667
And yet, suppose you know
that at some point in the future,

00:11:55.691 --> 00:11:56.915
no matter what,

00:11:56.939 --> 00:12:00.636
you're going to get the same
basic income as everyone else.

00:12:00.660 --> 00:12:03.702
Now, to my mind, that creates
a very perverse incentive

00:12:03.726 --> 00:12:06.223
for you to simply give up
and drop out of school.

00:12:06.247 --> 00:12:08.752
So I would say, let's not
structure things that way.

00:12:08.776 --> 00:12:14.092
Instead, let's pay people who graduate
from high school somewhat more

00:12:14.116 --> 00:12:15.812
than those who simply drop out.

00:12:16.329 --> 00:12:19.807
And we can take that idea of building
incentives into a basic income,

00:12:19.831 --> 00:12:21.498
and maybe extend it to other areas.

00:12:21.522 --> 00:12:25.099
For example, we might create
an incentive to work in the community

00:12:25.123 --> 00:12:26.281
to help others,

00:12:26.305 --> 00:12:29.369
or perhaps to do positive
things for the environment,

00:12:29.393 --> 00:12:30.563
and so forth.

00:12:30.587 --> 00:12:33.598
So by incorporating incentives
into a basic income,

00:12:33.622 --> 00:12:35.251
we might actually improve it,

00:12:35.275 --> 00:12:37.901
and also, perhaps, take at least
a couple of steps

00:12:37.925 --> 00:12:40.350
towards solving another problem

00:12:40.374 --> 00:12:43.318
that I think we're quite possibly
going to face in the future,

00:12:43.342 --> 00:12:47.094
and that is, how do we all find
meaning and fulfillment,

00:12:47.118 --> 00:12:49.436
and how do we occupy our time

00:12:49.460 --> 00:12:53.809
in a world where perhaps
there's less demand for traditional work?

00:12:54.201 --> 00:12:57.006
So by extending and refining
a basic income,

00:12:57.030 --> 00:12:59.366
I think we can make it look better,

00:12:59.390 --> 00:13:04.688
and we can also, perhaps, make it
more politically and socially acceptable

00:13:04.712 --> 00:13:05.876
and feasible --

00:13:05.900 --> 00:13:07.374
and, of course, by doing that,

00:13:07.398 --> 00:13:10.848
we increase the odds
that it will actually come to be.

00:13:11.731 --> 00:13:14.001
I think one of the most fundamental,

00:13:14.025 --> 00:13:16.193
almost instinctive objections

00:13:16.217 --> 00:13:19.670
that many of us have
to the idea of a basic income,

00:13:19.694 --> 00:13:23.426
or really to any significant
expansion of the safety net,

00:13:23.450 --> 00:13:27.210
is this fear that we're going to end up
with too many people

00:13:27.234 --> 00:13:28.972
riding in the economic cart,

00:13:28.996 --> 00:13:31.043
and not enough people pulling that cart.

00:13:31.067 --> 00:13:33.901
And yet, really, the whole point
I'm making here, of course,

00:13:33.925 --> 00:13:35.286
is that in the future,

00:13:35.310 --> 00:13:39.136
machines are increasingly going
to be capable of pulling that cart for us.

00:13:39.160 --> 00:13:41.150
That should give us more options

00:13:41.174 --> 00:13:44.985
for the way we structure
our society and our economy,

00:13:45.009 --> 00:13:48.451
And I think eventually, it's going to go
beyond simply being an option,

00:13:48.475 --> 00:13:50.376
and it's going to become an imperative.

00:13:50.400 --> 00:13:53.222
The reason, of course,
is that all of this is going to put

00:13:53.246 --> 00:13:55.260
such a degree of stress on our society,

00:13:55.284 --> 00:13:57.798
and also because jobs are that mechanism

00:13:57.822 --> 00:13:59.787
that gets purchasing power to consumers

00:13:59.811 --> 00:14:02.327
so they can then drive the economy.

00:14:02.351 --> 00:14:05.898
If, in fact, that mechanism
begins to erode in the future,

00:14:05.922 --> 00:14:08.737
then we're going to need to replace
it with something else

00:14:08.761 --> 00:14:10.324
or we're going to face the risk

00:14:10.348 --> 00:14:12.915
that our whole system simply
may not be sustainable.

00:14:12.939 --> 00:14:15.321
But the bottom line here
is that I really think

00:14:15.345 --> 00:14:17.781
that solving these problems,

00:14:17.805 --> 00:14:21.205
and especially finding a way
to build a future economy

00:14:21.229 --> 00:14:23.242
that works for everyone,

00:14:23.266 --> 00:14:25.127
at every level of our society,

00:14:25.151 --> 00:14:28.691
is going to be one of the most important
challenges that we all face

00:14:28.715 --> 00:14:30.758
in the coming years and decades.

00:14:30.782 --> 00:14:32.030
Thank you very much.

00:14:32.054 --> 00:14:33.914
(Applause)

