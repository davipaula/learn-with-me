WEBVTT
Kind: captions
Language: en

00:00:13.000 --> 00:00:14.809
Chris Anderson: Nick Bostrom.

00:00:14.833 --> 00:00:18.809
So, you have already given us
so many crazy ideas out there.

00:00:18.833 --> 00:00:20.559
I think a couple of decades ago,

00:00:20.583 --> 00:00:23.518
you made the case that we might
all be living in a simulation,

00:00:23.542 --> 00:00:25.351
or perhaps probably were.

00:00:25.375 --> 00:00:26.726
More recently,

00:00:26.750 --> 00:00:31.351
you've painted the most vivid examples
of how artificial general intelligence

00:00:31.375 --> 00:00:33.208
could go horribly wrong.

00:00:33.750 --> 00:00:35.143
And now this year,

00:00:35.167 --> 00:00:37.393
you're about to publish

00:00:37.417 --> 00:00:41.351
a paper that presents something called
the vulnerable world hypothesis.

00:00:41.375 --> 00:00:45.958
And our job this evening is to
give the illustrated guide to that.

00:00:46.417 --> 00:00:48.250
So let's do that.

00:00:48.833 --> 00:00:50.625
What is that hypothesis?

00:00:52.000 --> 00:00:54.434
Nick Bostrom: It's trying to think about

00:00:54.458 --> 00:00:57.542
a sort of structural feature
of the current human condition.

00:00:59.125 --> 00:01:01.476
You like the urn metaphor,

00:01:01.500 --> 00:01:03.393
so I'm going to use that to explain it.

00:01:03.417 --> 00:01:07.768
So picture a big urn filled with balls

00:01:07.792 --> 00:01:11.750
representing ideas, methods,
possible technologies.

00:01:12.833 --> 00:01:16.559
You can think of the history
of human creativity

00:01:16.583 --> 00:01:20.393
as the process of reaching into this urn
and pulling out one ball after another,

00:01:20.417 --> 00:01:23.643
and the net effect so far
has been hugely beneficial, right?

00:01:23.667 --> 00:01:26.393
We've extracted a great many white balls,

00:01:26.417 --> 00:01:29.292
some various shades of gray,
mixed blessings.

00:01:30.042 --> 00:01:33.000
We haven't so far
pulled out the black ball --

00:01:34.292 --> 00:01:39.768
a technology that invariably destroys
the civilization that discovers it.

00:01:39.792 --> 00:01:43.059
So the paper tries to think
about what could such a black ball be.

00:01:43.083 --> 00:01:44.893
CA: So you define that ball

00:01:44.917 --> 00:01:48.601
as one that would inevitably
bring about civilizational destruction.

00:01:48.625 --> 00:01:53.934
NB: Unless we exit what I call
the semi-anarchic default condition.

00:01:53.958 --> 00:01:55.458
But sort of, by default.

00:01:56.333 --> 00:01:59.851
CA: So, you make the case compelling

00:01:59.875 --> 00:02:01.893
by showing some sort of counterexamples

00:02:01.917 --> 00:02:04.851
where you believe that so far
we've actually got lucky,

00:02:04.875 --> 00:02:07.726
that we might have pulled out
that death ball

00:02:07.750 --> 00:02:09.309
without even knowing it.

00:02:09.333 --> 00:02:11.625
So there's this quote, what's this quote?

00:02:12.625 --> 00:02:15.309
NB: Well, I guess
it's just meant to illustrate

00:02:15.333 --> 00:02:17.434
the difficulty of foreseeing

00:02:17.458 --> 00:02:20.143
what basic discoveries will lead to.

00:02:20.167 --> 00:02:23.226
We just don't have that capability.

00:02:23.250 --> 00:02:26.601
Because we have become quite good
at pulling out balls,

00:02:26.625 --> 00:02:30.351
but we don't really have the ability
to put the ball back into the urn, right.

00:02:30.375 --> 00:02:32.542
We can invent, but we can't un-invent.

00:02:33.583 --> 00:02:36.351
So our strategy, such as it is,

00:02:36.375 --> 00:02:38.809
is to hope that there is
no black ball in the urn.

00:02:38.833 --> 00:02:42.893
CA: So once it's out, it's out,
and you can't put it back in,

00:02:42.917 --> 00:02:44.434
and you think we've been lucky.

00:02:44.458 --> 00:02:46.684
So talk through a couple
of these examples.

00:02:46.708 --> 00:02:49.809
You talk about different
types of vulnerability.

00:02:49.833 --> 00:02:52.268
NB: So the easiest type to understand

00:02:52.292 --> 00:02:55.434
is a technology
that just makes it very easy

00:02:55.458 --> 00:02:57.583
to cause massive amounts of destruction.

00:02:59.375 --> 00:03:02.893
Synthetic biology might be a fecund
source of that kind of black ball,

00:03:02.917 --> 00:03:05.601
but many other possible things we could --

00:03:05.625 --> 00:03:08.143
think of geoengineering,
really great, right?

00:03:08.167 --> 00:03:10.393
We could combat global warming,

00:03:10.417 --> 00:03:12.559
but you don't want it
to get too easy either,

00:03:12.583 --> 00:03:15.059
you don't want any random person
and his grandmother

00:03:15.083 --> 00:03:18.143
to have the ability to radically
alter the earth's climate.

00:03:18.167 --> 00:03:21.726
Or maybe lethal autonomous drones,

00:03:21.750 --> 00:03:25.083
massed-produced, mosquito-sized
killer bot swarms.

00:03:26.500 --> 00:03:29.226
Nanotechnology,
artificial general intelligence.

00:03:29.250 --> 00:03:30.559
CA: You argue in the paper

00:03:30.583 --> 00:03:33.476
that it's a matter of luck
that when we discovered

00:03:33.500 --> 00:03:36.934
that nuclear power could create a bomb,

00:03:36.958 --> 00:03:38.351
it might have been the case

00:03:38.375 --> 00:03:40.226
that you could have created a bomb

00:03:40.250 --> 00:03:43.809
with much easier resources,
accessible to anyone.

00:03:43.833 --> 00:03:47.393
NB: Yeah, so think back to the 1930s

00:03:47.417 --> 00:03:52.018
where for the first time we make
some breakthroughs in nuclear physics,

00:03:52.042 --> 00:03:55.726
some genius figures out that it's possible
to create a nuclear chain reaction

00:03:55.750 --> 00:03:58.934
and then realizes
that this could lead to the bomb.

00:03:58.958 --> 00:04:00.851
And we do some more work,

00:04:00.875 --> 00:04:03.601
it turns out that what you require
to make a nuclear bomb

00:04:03.625 --> 00:04:06.018
is highly enriched uranium or plutonium,

00:04:06.042 --> 00:04:08.059
which are very difficult materials to get.

00:04:08.083 --> 00:04:10.351
You need ultracentrifuges,

00:04:10.375 --> 00:04:14.143
you need reactors, like,
massive amounts of energy.

00:04:14.167 --> 00:04:15.976
But suppose it had turned out instead

00:04:16.000 --> 00:04:19.976
there had been an easy way
to unlock the energy of the atom.

00:04:20.000 --> 00:04:22.768
That maybe by baking sand
in the microwave oven

00:04:22.792 --> 00:04:24.059
or something like that

00:04:24.083 --> 00:04:26.184
you could have created
a nuclear detonation.

00:04:26.208 --> 00:04:28.351
So we know that that's
physically impossible.

00:04:28.375 --> 00:04:30.268
But before you did the relevant physics

00:04:30.292 --> 00:04:32.483
how could you have known
how it would turn out?

00:04:32.507 --> 00:04:34.059
CA: Although, couldn't you argue

00:04:34.083 --> 00:04:36.018
that for life to evolve on Earth

00:04:36.042 --> 00:04:39.309
that implied sort of stable environment,

00:04:39.333 --> 00:04:43.518
that if it was possible to create
massive nuclear reactions relatively easy,

00:04:43.542 --> 00:04:45.400
the Earth would never have been stable,

00:04:45.424 --> 00:04:46.976
that we wouldn't be here at all.

00:04:47.000 --> 00:04:50.393
NB: Yeah, unless there were something
that is easy to do on purpose

00:04:50.417 --> 00:04:53.268
but that wouldn't happen by random chance.

00:04:53.292 --> 00:04:54.871
So, like things we can easily do,

00:04:54.896 --> 00:04:57.006
we can stack 10 blocks
on top of one another,

00:04:57.031 --> 00:05:00.228
but in nature, you're not going to find,
like, a stack of 10 blocks.

00:05:00.253 --> 00:05:01.926
CA: OK, so this is probably the one

00:05:01.950 --> 00:05:03.893
that many of us worry about most,

00:05:03.917 --> 00:05:07.434
and yes, synthetic biology
is perhaps the quickest route

00:05:07.458 --> 00:05:10.476
that we can foresee
in our near future to get us here.

00:05:10.500 --> 00:05:13.434
NB: Yeah, and so think
about what that would have meant

00:05:13.458 --> 00:05:17.101
if, say, anybody by working
in their kitchen for an afternoon

00:05:17.125 --> 00:05:18.518
could destroy a city.

00:05:18.542 --> 00:05:22.101
It's hard to see how
modern civilization as we know it

00:05:22.125 --> 00:05:23.559
could have survived that.

00:05:23.583 --> 00:05:26.101
Because in any population
of a million people,

00:05:26.125 --> 00:05:28.809
there will always be some
who would, for whatever reason,

00:05:28.833 --> 00:05:30.917
choose to use that destructive power.

00:05:31.750 --> 00:05:34.893
So if that apocalyptic residual

00:05:34.917 --> 00:05:36.893
would choose to destroy a city, or worse,

00:05:36.917 --> 00:05:38.476
then cities would get destroyed.

00:05:38.500 --> 00:05:40.851
CA: So here's another type
of vulnerability.

00:05:40.875 --> 00:05:42.518
Talk about this.

00:05:42.542 --> 00:05:46.518
NB: Yeah, so in addition to these
kind of obvious types of black balls

00:05:46.542 --> 00:05:49.352
that would just make it possible
to blow up a lot of things,

00:05:49.376 --> 00:05:53.809
other types would act
by creating bad incentives

00:05:53.833 --> 00:05:56.059
for humans to do things that are harmful.

00:05:56.083 --> 00:06:00.184
So, the Type-2a, we might call it that,

00:06:00.208 --> 00:06:04.726
is to think about some technology
that incentivizes great powers

00:06:04.750 --> 00:06:09.226
to use their massive amounts of force
to create destruction.

00:06:09.250 --> 00:06:12.083
So, nuclear weapons were actually
very close to this, right?

00:06:14.083 --> 00:06:17.143
What we did, we spent
over 10 trillion dollars

00:06:17.167 --> 00:06:19.684
to build 70,000 nuclear warheads

00:06:19.708 --> 00:06:22.143
and put them on hair-trigger alert.

00:06:22.167 --> 00:06:24.434
And there were several times
during the Cold War

00:06:24.458 --> 00:06:25.893
we almost blew each other up.

00:06:25.917 --> 00:06:29.018
It's not because a lot of people felt
this would be a great idea,

00:06:29.042 --> 00:06:31.726
let's all spend 10 trillion dollars
to blow ourselves up,

00:06:31.750 --> 00:06:34.684
but the incentives were such
that we were finding ourselves --

00:06:34.708 --> 00:06:36.018
this could have been worse.

00:06:36.042 --> 00:06:38.476
Imagine if there had been
a safe first strike.

00:06:38.500 --> 00:06:40.809
Then it might have been very tricky,

00:06:40.833 --> 00:06:42.101
in a crisis situation,

00:06:42.125 --> 00:06:44.602
to refrain from launching
all their nuclear missiles.

00:06:44.626 --> 00:06:48.018
If nothing else, because you would fear
that the other side might do it.

00:06:48.042 --> 00:06:49.851
CA: Right, mutual assured destruction

00:06:49.875 --> 00:06:52.601
kept the Cold War relatively stable,

00:06:52.625 --> 00:06:54.559
without that, we might not be here now.

00:06:54.583 --> 00:06:56.893
NB: It could have been
more unstable than it was.

00:06:56.917 --> 00:06:59.268
And there could be
other properties of technology.

00:06:59.292 --> 00:07:01.601
It could have been harder
to have arms treaties,

00:07:01.625 --> 00:07:03.226
if instead of nuclear weapons

00:07:03.250 --> 00:07:06.268
there had been some smaller thing
or something less distinctive.

00:07:06.292 --> 00:07:08.851
CA: And as well as bad incentives
for powerful actors,

00:07:08.875 --> 00:07:12.393
you also worry about bad incentives
for all of us, in Type-2b here.

00:07:12.417 --> 00:07:16.667
NB: Yeah, so, here we might
take the case of global warming.

00:07:18.958 --> 00:07:20.851
There are a lot of little conveniences

00:07:20.875 --> 00:07:23.059
that cause each one of us to do things

00:07:23.083 --> 00:07:25.934
that individually 
have no significant effect, right?

00:07:25.958 --> 00:07:27.934
But if billions of people do it,

00:07:27.958 --> 00:07:30.018
cumulatively, it has a damaging effect.

00:07:30.042 --> 00:07:32.851
Now, global warming
could have been a lot worse than it is.

00:07:32.875 --> 00:07:35.851
So we have the climate
sensitivity parameter, right.

00:07:35.875 --> 00:07:39.518
It's a parameter that says
how much warmer does it get

00:07:39.542 --> 00:07:42.226
if you emit a certain amount
of greenhouse gases.

00:07:42.250 --> 00:07:44.643
But, suppose that it had been the case

00:07:44.667 --> 00:07:47.184
that with the amount
of greenhouse gases we emitted,

00:07:47.208 --> 00:07:49.268
instead of the temperature rising by, say,

00:07:49.292 --> 00:07:53.018
between three and 4.5 degrees by 2100,

00:07:53.042 --> 00:07:55.542
suppose it had been
15 degrees or 20 degrees.

00:07:56.375 --> 00:07:58.934
Like, then we might have been
in a very bad situation.

00:07:58.958 --> 00:08:02.101
Or suppose that renewable energy
had just been a lot harder to do.

00:08:02.125 --> 00:08:04.768
Or that there had been
more fossil fuels in the ground.

00:08:04.792 --> 00:08:07.434
CA: Couldn't you argue
that if in that case of --

00:08:07.458 --> 00:08:09.184
if what we are doing today

00:08:09.208 --> 00:08:13.768
had resulted in 10 degrees difference
in the time period that we could see,

00:08:13.792 --> 00:08:17.476
actually humanity would have got
off its ass and done something about it.

00:08:17.500 --> 00:08:20.309
We're stupid, but we're not
maybe that stupid.

00:08:20.333 --> 00:08:21.601
Or maybe we are.

00:08:21.625 --> 00:08:22.893
NB: I wouldn't bet on it.

00:08:22.917 --> 00:08:25.101
(Laughter)

00:08:25.125 --> 00:08:26.809
You could imagine other features.

00:08:26.833 --> 00:08:32.351
So, right now, it's a little bit difficult
to switch to renewables and stuff, right,

00:08:32.375 --> 00:08:33.643
but it can be done.

00:08:33.667 --> 00:08:36.643
But it might just have been,
with slightly different physics,

00:08:36.667 --> 00:08:39.458
it could have been much more expensive
to do these things.

00:08:40.375 --> 00:08:41.893
CA: And what's your view, Nick?

00:08:41.917 --> 00:08:44.351
Do you think, putting
these possibilities together,

00:08:44.375 --> 00:08:48.643
that this earth, humanity that we are,

00:08:48.667 --> 00:08:50.226
we count as a vulnerable world?

00:08:50.250 --> 00:08:52.667
That there is a death ball in our future?

00:08:55.958 --> 00:08:57.226
NB: It's hard to say.

00:08:57.250 --> 00:09:02.309
I mean, I think there might
well be various black balls in the urn,

00:09:02.333 --> 00:09:03.643
that's what it looks like.

00:09:03.667 --> 00:09:06.059
There might also be some golden balls

00:09:06.083 --> 00:09:09.559
that would help us
protect against black balls.

00:09:09.583 --> 00:09:12.559
And I don't know which order
they will come out.

00:09:12.583 --> 00:09:16.434
CA: I mean, one possible
philosophical critique of this idea

00:09:16.458 --> 00:09:22.101
is that it implies a view
that the future is essentially settled.

00:09:22.125 --> 00:09:24.601
That there either
is that ball there or it's not.

00:09:24.625 --> 00:09:27.643
And in a way,

00:09:27.667 --> 00:09:30.268
that's not a view of the future
that I want to believe.

00:09:30.292 --> 00:09:32.643
I want to believe
that the future is undetermined,

00:09:32.667 --> 00:09:34.601
that our decisions today will determine

00:09:34.625 --> 00:09:36.833
what kind of balls
we pull out of that urn.

00:09:37.917 --> 00:09:41.684
NB: I mean, if we just keep inventing,

00:09:41.708 --> 00:09:44.042
like, eventually we will
pull out all the balls.

00:09:44.875 --> 00:09:48.268
I mean, I think there's a kind
of weak form of technological determinism

00:09:48.292 --> 00:09:49.559
that is quite plausible,

00:09:49.583 --> 00:09:52.226
like, you're unlikely
to encounter a society

00:09:52.250 --> 00:09:55.083
that uses flint axes and jet planes.

00:09:56.208 --> 00:10:00.268
But you can almost think
of a technology as a set of affordances.

00:10:00.292 --> 00:10:03.309
So technology is the thing
that enables us to do various things

00:10:03.333 --> 00:10:05.309
and achieve various effects in the world.

00:10:05.333 --> 00:10:08.143
How we'd then use that,
of course depends on human choice.

00:10:08.167 --> 00:10:10.851
But if we think about these
three types of vulnerability,

00:10:10.875 --> 00:10:14.268
they make quite weak assumptions
about how we would choose to use them.

00:10:14.292 --> 00:10:17.684
So a Type-1 vulnerability, again,
this massive, destructive power,

00:10:17.708 --> 00:10:19.143
it's a fairly weak assumption

00:10:19.167 --> 00:10:21.559
to think that in a population
of millions of people

00:10:21.583 --> 00:10:24.518
there would be some that would choose
to use it destructively.

00:10:24.542 --> 00:10:26.976
CA: For me, the most single
disturbing argument

00:10:27.000 --> 00:10:31.559
is that we actually might have
some kind of view into the urn

00:10:31.583 --> 00:10:35.101
that makes it actually
very likely that we're doomed.

00:10:35.125 --> 00:10:39.768
Namely, if you believe
in accelerating power,

00:10:39.792 --> 00:10:42.059
that technology inherently accelerates,

00:10:42.083 --> 00:10:44.518
that we build the tools
that make us more powerful,

00:10:44.542 --> 00:10:47.184
then at some point you get to a stage

00:10:47.208 --> 00:10:50.268
where a single individual
can take us all down,

00:10:50.292 --> 00:10:53.143
and then it looks like we're screwed.

00:10:53.167 --> 00:10:56.101
Isn't that argument quite alarming?

00:10:56.125 --> 00:10:57.875
NB: Ah, yeah.

00:10:58.708 --> 00:10:59.976
(Laughter)

00:11:00.000 --> 00:11:01.333
I think --

00:11:02.875 --> 00:11:04.476
Yeah, we get more and more power,

00:11:04.500 --> 00:11:08.434
and [it's] easier and easier 
to use those powers,

00:11:08.458 --> 00:11:12.018
but we can also invent technologies
that kind of help us control

00:11:12.042 --> 00:11:14.059
how people use those powers.

00:11:14.083 --> 00:11:16.934
CA: So let's talk about that,
let's talk about the response.

00:11:16.958 --> 00:11:19.268
Suppose that thinking
about all the possibilities

00:11:19.292 --> 00:11:21.393
that are out there now --

00:11:21.417 --> 00:11:25.143
it's not just synbio,
it's things like cyberwarfare,

00:11:25.167 --> 00:11:28.518
artificial intelligence, etc., etc. --

00:11:28.542 --> 00:11:33.059
that there might be
serious doom in our future.

00:11:33.083 --> 00:11:34.684
What are the possible responses?

00:11:34.708 --> 00:11:39.601
And you've talked about
four possible responses as well.

00:11:39.625 --> 00:11:43.268
NB: Restricting technological development
doesn't seem promising,

00:11:43.292 --> 00:11:46.518
if we are talking about a general halt
to technological progress.

00:11:46.542 --> 00:11:47.809
I think neither feasible,

00:11:47.833 --> 00:11:50.143
nor would it be desirable
even if we could do it.

00:11:50.167 --> 00:11:53.184
I think there might be very limited areas

00:11:53.208 --> 00:11:55.934
where maybe you would want
slower technological progress.

00:11:55.958 --> 00:11:59.351
You don't, I think, want
faster progress in bioweapons,

00:11:59.375 --> 00:12:01.434
or in, say, isotope separation,

00:12:01.458 --> 00:12:03.708
that would make it easier to create nukes.

00:12:04.583 --> 00:12:07.893
CA: I mean, I used to be
fully on board with that.

00:12:07.917 --> 00:12:11.184
But I would like to actually
push back on that for a minute.

00:12:11.208 --> 00:12:12.518
Just because, first of all,

00:12:12.542 --> 00:12:15.226
if you look at the history
of the last couple of decades,

00:12:15.250 --> 00:12:18.809
you know, it's always been
push forward at full speed,

00:12:18.833 --> 00:12:20.684
it's OK, that's our only choice.

00:12:20.708 --> 00:12:24.976
But if you look at globalization
and the rapid acceleration of that,

00:12:25.000 --> 00:12:28.434
if you look at the strategy of
"move fast and break things"

00:12:28.458 --> 00:12:30.518
and what happened with that,

00:12:30.542 --> 00:12:33.309
and then you look at the potential
for synthetic biology,

00:12:33.333 --> 00:12:37.768
I don't know that we should
move forward rapidly

00:12:37.792 --> 00:12:39.434
or without any kind of restriction

00:12:39.458 --> 00:12:42.768
to a world where you could have
a DNA printer in every home

00:12:42.792 --> 00:12:44.125
and high school lab.

00:12:45.167 --> 00:12:46.851
There are some restrictions, right?

00:12:46.875 --> 00:12:49.518
NB: Possibly, there is
the first part, the not feasible.

00:12:49.542 --> 00:12:51.726
If you think it would be
desirable to stop it,

00:12:51.750 --> 00:12:53.476
there's the problem of feasibility.

00:12:53.500 --> 00:12:56.309
So it doesn't really help
if one nation kind of --

00:12:56.333 --> 00:12:58.351
CA: No, it doesn't help
if one nation does,

00:12:58.375 --> 00:13:01.309
but we've had treaties before.

00:13:01.333 --> 00:13:04.684
That's really how we survived
the nuclear threat,

00:13:04.708 --> 00:13:05.976
was by going out there

00:13:06.000 --> 00:13:08.518
and going through
the painful process of negotiating.

00:13:08.542 --> 00:13:13.976
I just wonder whether the logic isn't
that we, as a matter of global priority,

00:13:14.000 --> 00:13:15.684
we shouldn't go out there and try,

00:13:15.708 --> 00:13:18.393
like, now start negotiating
really strict rules

00:13:18.417 --> 00:13:21.101
on where synthetic bioresearch is done,

00:13:21.125 --> 00:13:23.976
that it's not something
that you want to democratize, no?

00:13:24.000 --> 00:13:25.809
NB: I totally agree with that --

00:13:25.833 --> 00:13:30.059
that it would be desirable, for example,

00:13:30.083 --> 00:13:33.684
maybe to have DNA synthesis machines,

00:13:33.708 --> 00:13:37.268
not as a product where each lab
has their own device,

00:13:37.292 --> 00:13:38.768
but maybe as a service.

00:13:38.792 --> 00:13:41.309
Maybe there could be
four or five places in the world

00:13:41.333 --> 00:13:44.851
where you send in your digital blueprint
and the DNA comes back, right?

00:13:44.875 --> 00:13:46.643
And then, you would have the ability,

00:13:46.667 --> 00:13:49.059
if one day it really looked
like it was necessary,

00:13:49.083 --> 00:13:51.434
we would have like,
a finite set of choke points.

00:13:51.458 --> 00:13:54.976
So I think you want to look
for kind of special opportunities,

00:13:55.000 --> 00:13:57.059
where you could have tighter control.

00:13:57.083 --> 00:13:58.726
CA: Your belief is, fundamentally,

00:13:58.750 --> 00:14:01.643
we are not going to be successful
in just holding back.

00:14:01.667 --> 00:14:04.393
Someone, somewhere --
North Korea, you know --

00:14:04.417 --> 00:14:07.934
someone is going to go there
and discover this knowledge,

00:14:07.958 --> 00:14:09.226
if it's there to be found.

00:14:09.250 --> 00:14:11.601
NB: That looks plausible
under current conditions.

00:14:11.625 --> 00:14:13.559
It's not just synthetic biology, either.

00:14:13.583 --> 00:14:16.101
I mean, any kind of profound,
new change in the world

00:14:16.101 --> 00:14:17.727
could turn out to be a black ball.

00:14:17.727 --> 00:14:19.823
CA: Let's look at 
another possible response.

00:14:19.823 --> 00:14:22.226
NB: This also, I think,
has only limited potential.

00:14:22.250 --> 00:14:25.809
So, with the Type-1 vulnerability again,

00:14:25.833 --> 00:14:30.184
I mean, if you could reduce the number
of people who are incentivized

00:14:30.208 --> 00:14:31.476
to destroy the world,

00:14:31.500 --> 00:14:33.559
if only they could get
access and the means,

00:14:33.583 --> 00:14:34.851
that would be good.

00:14:34.875 --> 00:14:36.851
CA: In this image that you asked us to do

00:14:36.875 --> 00:14:39.434
you're imagining these drones
flying around the world

00:14:39.458 --> 00:14:40.726
with facial recognition.

00:14:40.750 --> 00:14:43.643
When they spot someone
showing signs of sociopathic behavior,

00:14:43.667 --> 00:14:45.851
they shower them with love, they fix them.

00:14:45.875 --> 00:14:47.768
NB: I think it's like a hybrid picture.

00:14:47.792 --> 00:14:51.809
Eliminate can either mean,
like, incarcerate or kill,

00:14:51.833 --> 00:14:54.851
or it can mean persuade them
to a better view of the world.

00:14:54.875 --> 00:14:56.601
But the point is that,

00:14:56.625 --> 00:14:58.768
suppose you were
extremely successful in this,

00:14:58.792 --> 00:15:02.101
and you reduced the number
of such individuals by half.

00:15:02.125 --> 00:15:04.018
And if you want to do it by persuasion,

00:15:04.042 --> 00:15:06.434
you are competing against
all other powerful forces

00:15:06.458 --> 00:15:08.143
that are trying to persuade people,

00:15:08.167 --> 00:15:09.934
parties, religion, education system.

00:15:09.958 --> 00:15:11.863
But suppose you could reduce it by half,

00:15:11.887 --> 00:15:14.143
I don't think the risk
would be reduced by half.

00:15:14.167 --> 00:15:15.726
Maybe by five or 10 percent.

00:15:15.750 --> 00:15:20.101
CA: You're not recommending that we gamble
humanity's future on response two.

00:15:20.125 --> 00:15:23.143
NB: I think it's all good
to try to deter and persuade people,

00:15:23.167 --> 00:15:26.143
but we shouldn't rely on that
as our only safeguard.

00:15:26.167 --> 00:15:27.434
CA: How about three?

00:15:27.458 --> 00:15:30.351
NB: I think there are two general methods

00:15:30.375 --> 00:15:34.351
that we could use to achieve
the ability to stabilize the world

00:15:34.375 --> 00:15:37.351
against the whole spectrum
of possible vulnerabilities.

00:15:37.375 --> 00:15:38.934
And we probably would need both.

00:15:38.958 --> 00:15:43.476
So, one is an extremely effective ability

00:15:43.500 --> 00:15:45.268
to do preventive policing.

00:15:45.292 --> 00:15:46.816
Such that you could intercept.

00:15:46.840 --> 00:15:49.601
If anybody started to do
this dangerous thing,

00:15:49.625 --> 00:15:52.309
you could intercept them
in real time, and stop them.

00:15:52.333 --> 00:15:54.809
So this would require
ubiquitous surveillance,

00:15:54.833 --> 00:15:57.208
everybody would be monitored all the time.

00:15:58.333 --> 00:16:00.893
CA: This is "Minority Report,"
essentially, a form of.

00:16:00.917 --> 00:16:02.851
NB: You would have maybe AI algorithms,

00:16:02.875 --> 00:16:07.292
big freedom centers
that were reviewing this, etc., etc.

00:16:08.583 --> 00:16:12.976
CA: You know that mass surveillance
is not a very popular term right now?

00:16:13.000 --> 00:16:14.250
(Laughter)

00:16:15.458 --> 00:16:17.268
NB: Yeah, so this little device there,

00:16:17.292 --> 00:16:20.893
imagine that kind of necklace
that you would have to wear at all times

00:16:20.917 --> 00:16:22.917
with multidirectional cameras.

00:16:23.792 --> 00:16:25.601
But, to make it go down better,

00:16:25.625 --> 00:16:28.149
just call it the "freedom tag"
or something like that.

00:16:28.173 --> 00:16:30.184
(Laughter)

00:16:30.208 --> 00:16:31.476
CA: OK.

00:16:31.500 --> 00:16:33.601
I mean, this is the conversation, friends,

00:16:33.625 --> 00:16:37.184
this is why this is
such a mind-blowing conversation.

00:16:37.208 --> 00:16:39.809
NB: Actually, there's
a whole big conversation on this

00:16:39.833 --> 00:16:41.143
on its own, obviously.

00:16:41.167 --> 00:16:43.643
There are huge problems and risks
with that, right?

00:16:43.667 --> 00:16:44.934
We may come back to that.

00:16:44.958 --> 00:16:46.226
So the other, the final,

00:16:46.250 --> 00:16:48.809
the other general stabilization capability

00:16:48.833 --> 00:16:50.893
is kind of plugging
another governance gap.

00:16:50.917 --> 00:16:55.101
So the surveillance would be kind of
governance gap at the microlevel,

00:16:55.125 --> 00:16:58.226
like, preventing anybody
from ever doing something highly illegal.

00:16:58.250 --> 00:17:00.559
Then, there's a corresponding
governance gap

00:17:00.583 --> 00:17:02.518
at the macro level, at the global level.

00:17:02.542 --> 00:17:06.476
You would need the ability, reliably,

00:17:06.500 --> 00:17:09.309
to prevent the worst kinds
of global coordination failures,

00:17:09.333 --> 00:17:13.101
to avoid wars between great powers,

00:17:13.125 --> 00:17:14.458
arms races,

00:17:15.500 --> 00:17:17.708
cataclysmic commons problems,

00:17:19.667 --> 00:17:23.851
in order to deal with
the Type-2a vulnerabilities.

00:17:23.875 --> 00:17:25.809
CA: Global governance is a term

00:17:25.833 --> 00:17:28.059
that's definitely way out
of fashion right now,

00:17:28.083 --> 00:17:30.601
but could you make the case
that throughout history,

00:17:30.625 --> 00:17:31.893
the history of humanity

00:17:31.917 --> 00:17:37.351
is that at every stage
of technological power increase,

00:17:37.375 --> 00:17:40.601
people have reorganized
and sort of centralized the power.

00:17:40.625 --> 00:17:44.059
So, for example,
when a roving band of criminals

00:17:44.083 --> 00:17:45.768
could take over a society,

00:17:45.792 --> 00:17:48.031
the response was,
well, you have a nation-state

00:17:48.055 --> 00:17:50.489
and you centralize force,
a police force or an army,

00:17:50.513 --> 00:17:52.143
so, "No, you can't do that."

00:17:52.167 --> 00:17:56.726
The logic, perhaps, of having
a single person or a single group

00:17:56.750 --> 00:17:58.393
able to take out humanity

00:17:58.417 --> 00:18:01.143
means at some point
we're going to have to go this route,

00:18:01.167 --> 00:18:02.601
at least in some form, no?

00:18:02.625 --> 00:18:06.309
NB: It's certainly true that the scale
of political organization has increased

00:18:06.333 --> 00:18:08.476
over the course of human history.

00:18:08.500 --> 00:18:10.518
It used to be hunter-gatherer band, right,

00:18:10.542 --> 00:18:13.476
and then chiefdom, city-states, nations,

00:18:13.500 --> 00:18:17.476
now there are international organizations
and so on and so forth.

00:18:17.500 --> 00:18:19.018
Again, I just want to make sure

00:18:19.042 --> 00:18:20.684
I get the chance to stress

00:18:20.708 --> 00:18:22.684
that obviously there are huge downsides

00:18:22.708 --> 00:18:24.226
and indeed, massive risks,

00:18:24.250 --> 00:18:27.601
both to mass surveillance
and to global governance.

00:18:27.625 --> 00:18:30.184
I'm just pointing out
that if we are lucky,

00:18:30.208 --> 00:18:32.893
the world could be such
that these would be the only ways

00:18:32.917 --> 00:18:34.434
you could survive a black ball.

00:18:34.458 --> 00:18:36.976
CA: The logic of this theory,

00:18:37.000 --> 00:18:38.268
it seems to me,

00:18:38.292 --> 00:18:41.893
is that we've got to recognize
we can't have it all.

00:18:41.917 --> 00:18:43.750
That the sort of,

00:18:45.500 --> 00:18:48.476
I would say, naive dream
that many of us had

00:18:48.500 --> 00:18:51.851
that technology is always
going to be a force for good,

00:18:51.875 --> 00:18:54.851
keep going, don't stop,
go as fast as you can

00:18:54.875 --> 00:18:57.226
and not pay attention
to some of the consequences,

00:18:57.250 --> 00:18:58.934
that's actually just not an option.

00:18:58.958 --> 00:19:00.893
We can have that.

00:19:00.917 --> 00:19:02.184
If we have that,

00:19:02.208 --> 00:19:03.643
we're going to have to accept

00:19:03.667 --> 00:19:06.226
some of these other
very uncomfortable things with it,

00:19:06.250 --> 00:19:08.476
and kind of be in this
arms race with ourselves

00:19:08.500 --> 00:19:10.768
of, you want the power,
you better limit it,

00:19:10.792 --> 00:19:12.934
you better figure out how to limit it.

00:19:12.958 --> 00:19:16.434
NB: I think it is an option,

00:19:16.458 --> 00:19:19.226
a very tempting option,
it's in a sense the easiest option

00:19:19.250 --> 00:19:20.518
and it might work,

00:19:20.542 --> 00:19:25.351
but it means we are fundamentally
vulnerable to extracting a black ball.

00:19:25.375 --> 00:19:27.518
Now, I think with a bit of coordination,

00:19:27.542 --> 00:19:30.268
like, if you did solve this
macrogovernance problem,

00:19:30.292 --> 00:19:31.893
and the microgovernance problem,

00:19:31.917 --> 00:19:34.226
then we could extract
all the balls from the urn

00:19:34.250 --> 00:19:36.518
and we'd benefit greatly.

00:19:36.542 --> 00:19:39.976
CA: I mean, if we're living
in a simulation, does it matter?

00:19:40.000 --> 00:19:41.309
We just reboot.

00:19:41.333 --> 00:19:42.601
(Laughter)

00:19:42.625 --> 00:19:44.268
NB: Then ... I ...

00:19:44.292 --> 00:19:46.768
(Laughter)

00:19:46.792 --> 00:19:48.208
I didn't see that one coming.

00:19:50.125 --> 00:19:51.393
CA: So what's your view?

00:19:51.417 --> 00:19:56.226
Putting all the pieces together,
how likely is it that we're doomed?

00:19:56.250 --> 00:19:58.208
(Laughter)

00:19:59.042 --> 00:20:01.434
I love how people laugh
when you ask that question.

00:20:01.458 --> 00:20:02.809
NB: On an individual level,

00:20:02.833 --> 00:20:06.684
we seem to kind of be doomed anyway,
just with the time line,

00:20:06.708 --> 00:20:09.309
we're rotting and aging
and all kinds of things, right?

00:20:09.333 --> 00:20:10.934
(Laughter)

00:20:10.958 --> 00:20:12.643
It's actually a little bit tricky.

00:20:12.667 --> 00:20:15.434
If you want to set up
so that you can attach a probability,

00:20:15.458 --> 00:20:16.726
first, who are we?

00:20:16.750 --> 00:20:19.476
If you're very old,
probably you'll die of natural causes,

00:20:19.500 --> 00:20:21.851
if you're very young,
you might have a 100-year --

00:20:21.875 --> 00:20:24.018
the probability might depend
on who you ask.

00:20:24.042 --> 00:20:28.268
Then the threshold, like, what counts
as civilizational devastation?

00:20:28.292 --> 00:20:33.934
In the paper I don't require
an existential catastrophe

00:20:33.958 --> 00:20:35.393
in order for it to count.

00:20:35.417 --> 00:20:37.101
This is just a definitional matter,

00:20:37.125 --> 00:20:38.434
I say a billion dead,

00:20:38.458 --> 00:20:40.518
or a reduction of world GDP by 50 percent,

00:20:40.542 --> 00:20:42.768
but depending on what
you say the threshold is,

00:20:42.792 --> 00:20:44.768
you get a different probability estimate.

00:20:44.792 --> 00:20:49.309
But I guess you could
put me down as a frightened optimist.

00:20:49.333 --> 00:20:50.434
(Laughter)

00:20:50.458 --> 00:20:52.101
CA: You're a frightened optimist,

00:20:52.125 --> 00:20:56.393
and I think you've just created
a large number of other frightened ...

00:20:56.417 --> 00:20:57.684
people.

00:20:57.708 --> 00:20:58.768
(Laughter)

00:20:58.792 --> 00:21:00.059
NB: In the simulation.

00:21:00.083 --> 00:21:01.351
CA: In a simulation.

00:21:01.375 --> 00:21:03.059
Nick Bostrom, your mind amazes me,

00:21:03.083 --> 00:21:05.976
thank you so much for scaring
the living daylights out of us.

00:21:06.000 --> 00:21:08.375
(Applause)

