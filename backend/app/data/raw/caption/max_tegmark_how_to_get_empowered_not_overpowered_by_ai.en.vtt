WEBVTT
Kind: captions
Language: en

00:00:12.760 --> 00:00:17.176
After 13.8 billion years
of cosmic history,

00:00:17.200 --> 00:00:19.296
our universe has woken up

00:00:19.320 --> 00:00:20.840
and become aware of itself.

00:00:21.480 --> 00:00:23.416
From a small blue planet,

00:00:23.440 --> 00:00:27.576
tiny, conscious parts of our universe
have begun gazing out into the cosmos

00:00:27.600 --> 00:00:28.976
with telescopes,

00:00:29.000 --> 00:00:30.480
discovering something humbling.

00:00:31.320 --> 00:00:34.216
We've discovered that our universe
is vastly grander

00:00:34.240 --> 00:00:35.576
than our ancestors imagined

00:00:35.600 --> 00:00:39.856
and that life seems to be an almost
imperceptibly small perturbation

00:00:39.880 --> 00:00:41.600
on an otherwise dead universe.

00:00:42.320 --> 00:00:45.336
But we've also discovered
something inspiring,

00:00:45.360 --> 00:00:48.336
which is that the technology
we're developing has the potential

00:00:48.360 --> 00:00:51.216
to help life flourish like never before,

00:00:51.240 --> 00:00:54.336
not just for centuries
but for billions of years,

00:00:54.360 --> 00:00:58.480
and not just on earth but throughout
much of this amazing cosmos.

00:00:59.680 --> 00:01:03.016
I think of the earliest life as "Life 1.0"

00:01:03.040 --> 00:01:04.416
because it was really dumb,

00:01:04.440 --> 00:01:08.736
like bacteria, unable to learn
anything during its lifetime.

00:01:08.760 --> 00:01:12.136
I think of us humans as "Life 2.0"
because we can learn,

00:01:12.160 --> 00:01:13.656
which we in nerdy, geek speak,

00:01:13.680 --> 00:01:16.896
might think of as installing
new software into our brains,

00:01:16.920 --> 00:01:19.040
like languages and job skills.

00:01:19.680 --> 00:01:23.976
"Life 3.0," which can design not only
its software but also its hardware

00:01:24.000 --> 00:01:25.656
of course doesn't exist yet.

00:01:25.680 --> 00:01:29.456
But perhaps our technology
has already made us "Life 2.1,"

00:01:29.480 --> 00:01:33.816
with our artificial knees,
pacemakers and cochlear implants.

00:01:33.840 --> 00:01:37.720
So let's take a closer look
at our relationship with technology, OK?

00:01:38.800 --> 00:01:40.016
As an example,

00:01:40.040 --> 00:01:45.336
the Apollo 11 moon mission
was both successful and inspiring,

00:01:45.360 --> 00:01:48.376
showing that when we humans
use technology wisely,

00:01:48.400 --> 00:01:52.336
we can accomplish things
that our ancestors could only dream of.

00:01:52.360 --> 00:01:55.336
But there's an even more inspiring journey

00:01:55.360 --> 00:01:58.040
propelled by something
more powerful than rocket engines,

00:01:59.200 --> 00:02:01.536
where the passengers
aren't just three astronauts

00:02:01.560 --> 00:02:03.336
but all of humanity.

00:02:03.360 --> 00:02:06.296
Let's talk about our collective
journey into the future

00:02:06.320 --> 00:02:08.320
with artificial intelligence.

00:02:08.960 --> 00:02:13.496
My friend Jaan Tallinn likes to point out
that just as with rocketry,

00:02:13.520 --> 00:02:16.680
it's not enough to make
our technology powerful.

00:02:17.560 --> 00:02:20.735
We also have to figure out,
if we're going to be really ambitious,

00:02:20.759 --> 00:02:22.175
how to steer it

00:02:22.199 --> 00:02:23.880
and where we want to go with it.

00:02:24.880 --> 00:02:27.720
So let's talk about all three
for artificial intelligence:

00:02:28.440 --> 00:02:31.496
the power, the steering
and the destination.

00:02:31.520 --> 00:02:32.806
Let's start with the power.

00:02:33.600 --> 00:02:36.696
I define intelligence very inclusively --

00:02:36.720 --> 00:02:41.056
simply as our ability
to accomplish complex goals,

00:02:41.080 --> 00:02:44.896
because I want to include both
biological and artificial intelligence.

00:02:44.920 --> 00:02:48.936
And I want to avoid
the silly carbon-chauvinism idea

00:02:48.960 --> 00:02:51.320
that you can only be smart
if you're made of meat.

00:02:52.880 --> 00:02:57.056
It's really amazing how the power
of AI has grown recently.

00:02:57.080 --> 00:02:58.336
Just think about it.

00:02:58.360 --> 00:03:01.560
Not long ago, robots couldn't walk.

00:03:03.040 --> 00:03:04.760
Now, they can do backflips.

00:03:06.080 --> 00:03:07.896
Not long ago,

00:03:07.920 --> 00:03:09.680
we didn't have self-driving cars.

00:03:10.920 --> 00:03:13.400
Now, we have self-flying rockets.

00:03:15.960 --> 00:03:17.376
Not long ago,

00:03:17.400 --> 00:03:20.016
AI couldn't do face recognition.

00:03:20.040 --> 00:03:23.016
Now, AI can generate fake faces

00:03:23.040 --> 00:03:27.200
and simulate your face
saying stuff that you never said.

00:03:28.400 --> 00:03:29.976
Not long ago,

00:03:30.000 --> 00:03:31.880
AI couldn't beat us at the game of Go.

00:03:32.400 --> 00:03:37.496
Then, Google DeepMind's AlphaZero AI
took 3,000 years of human Go games

00:03:37.520 --> 00:03:38.776
and Go wisdom,

00:03:38.800 --> 00:03:43.776
ignored it all and became the world's best
player by just playing against itself.

00:03:43.800 --> 00:03:47.496
And the most impressive feat here
wasn't that it crushed human gamers,

00:03:47.520 --> 00:03:50.096
but that it crushed human AI researchers

00:03:50.120 --> 00:03:53.800
who had spent decades
handcrafting game-playing software.

00:03:54.200 --> 00:03:58.856
And AlphaZero crushed human AI researchers
not just in Go but even at chess,

00:03:58.880 --> 00:04:01.360
which we have been working on since 1950.

00:04:02.000 --> 00:04:06.240
So all this amazing recent progress in AI
really begs the question:

00:04:07.280 --> 00:04:08.840
How far will it go?

00:04:09.800 --> 00:04:11.496
I like to think about this question

00:04:11.520 --> 00:04:14.496
in terms of this abstract
landscape of tasks,

00:04:14.520 --> 00:04:17.976
where the elevation represents
how hard it is for AI to do each task

00:04:18.000 --> 00:04:19.216
at human level,

00:04:19.240 --> 00:04:22.000
and the sea level represents
what AI can do today.

00:04:23.120 --> 00:04:25.176
The sea level is rising
as AI improves,

00:04:25.200 --> 00:04:28.640
so there's a kind of global warming
going on here in the task landscape.

00:04:30.040 --> 00:04:33.375
And the obvious takeaway
is to avoid careers at the waterfront --

00:04:33.399 --> 00:04:34.656
(Laughter)

00:04:34.680 --> 00:04:37.536
which will soon be
automated and disrupted.

00:04:37.560 --> 00:04:40.536
But there's a much
bigger question as well.

00:04:40.560 --> 00:04:42.370
How high will the water end up rising?

00:04:43.440 --> 00:04:46.640
Will it eventually rise
to flood everything,

00:04:47.840 --> 00:04:50.336
matching human intelligence at all tasks.

00:04:50.360 --> 00:04:54.096
This is the definition
of artificial general intelligence --

00:04:54.120 --> 00:04:55.416
AGI,

00:04:55.440 --> 00:04:58.520
which has been the holy grail
of AI research since its inception.

00:04:59.000 --> 00:05:00.776
By this definition, people who say,

00:05:00.800 --> 00:05:04.216
"Ah, there will always be jobs
that humans can do better than machines,"

00:05:04.240 --> 00:05:07.160
are simply saying
that we'll never get AGI.

00:05:07.680 --> 00:05:11.256
Sure, we might still choose
to have some human jobs

00:05:11.280 --> 00:05:14.376
or to give humans income
and purpose with our jobs,

00:05:14.400 --> 00:05:18.136
but AGI will in any case
transform life as we know it

00:05:18.160 --> 00:05:20.896
with humans no longer being
the most intelligent.

00:05:20.920 --> 00:05:24.616
Now, if the water level does reach AGI,

00:05:24.640 --> 00:05:29.936
then further AI progress will be driven
mainly not by humans but by AI,

00:05:29.960 --> 00:05:31.816
which means that there's a possibility

00:05:31.840 --> 00:05:34.176
that further AI progress
could be way faster

00:05:34.200 --> 00:05:37.576
than the typical human research
and development timescale of years,

00:05:37.600 --> 00:05:41.616
raising the controversial possibility
of an intelligence explosion

00:05:41.640 --> 00:05:43.936
where recursively self-improving AI

00:05:43.960 --> 00:05:47.376
rapidly leaves human
intelligence far behind,

00:05:47.400 --> 00:05:49.840
creating what's known
as superintelligence.

00:05:51.800 --> 00:05:54.080
Alright, reality check:

00:05:55.120 --> 00:05:57.560
Are we going to get AGI any time soon?

00:05:58.360 --> 00:06:01.056
Some famous AI researchers,
like Rodney Brooks,

00:06:01.080 --> 00:06:03.576
think it won't happen
for hundreds of years.

00:06:03.600 --> 00:06:07.496
But others, like Google DeepMind
founder Demis Hassabis,

00:06:07.520 --> 00:06:08.776
are more optimistic

00:06:08.800 --> 00:06:11.376
and are working to try to make
it happen much sooner.

00:06:11.400 --> 00:06:14.696
And recent surveys have shown
that most AI researchers

00:06:14.720 --> 00:06:17.576
actually share Demis's optimism,

00:06:17.600 --> 00:06:20.680
expecting that we will
get AGI within decades,

00:06:21.640 --> 00:06:23.896
so within the lifetime of many of us,

00:06:23.920 --> 00:06:25.880
which begs the question -- and then what?

00:06:27.040 --> 00:06:29.256
What do we want the role of humans to be

00:06:29.280 --> 00:06:31.960
if machines can do everything better
and cheaper than us?

00:06:35.000 --> 00:06:37.000
The way I see it, we face a choice.

00:06:38.000 --> 00:06:39.576
One option is to be complacent.

00:06:39.600 --> 00:06:43.376
We can say, "Oh, let's just build machines
that can do everything we can do

00:06:43.400 --> 00:06:45.216
and not worry about the consequences.

00:06:45.240 --> 00:06:48.496
Come on, if we build technology
that makes all humans obsolete,

00:06:48.520 --> 00:06:50.616
what could possibly go wrong?"

00:06:50.640 --> 00:06:52.296
(Laughter)

00:06:52.320 --> 00:06:55.080
But I think that would be
embarrassingly lame.

00:06:56.080 --> 00:06:59.576
I think we should be more ambitious --
in the spirit of TED.

00:06:59.600 --> 00:07:03.096
Let's envision a truly inspiring
high-tech future

00:07:03.120 --> 00:07:04.520
and try to steer towards it.

00:07:05.720 --> 00:07:09.256
This brings us to the second part
of our rocket metaphor: the steering.

00:07:09.280 --> 00:07:11.176
We're making AI more powerful,

00:07:11.200 --> 00:07:15.016
but how can we steer towards a future

00:07:15.040 --> 00:07:18.120
where AI helps humanity flourish
rather than flounder?

00:07:18.760 --> 00:07:20.016
To help with this,

00:07:20.040 --> 00:07:22.016
I cofounded the Future of Life Institute.

00:07:22.040 --> 00:07:24.816
It's a small nonprofit promoting
beneficial technology use,

00:07:24.840 --> 00:07:27.576
and our goal is simply
for the future of life to exist

00:07:27.600 --> 00:07:29.656
and to be as inspiring as possible.

00:07:29.680 --> 00:07:32.856
You know, I love technology.

00:07:32.880 --> 00:07:35.800
Technology is why today
is better than the Stone Age.

00:07:36.600 --> 00:07:40.680
And I'm optimistic that we can create
a really inspiring high-tech future ...

00:07:41.680 --> 00:07:43.136
if -- and this is a big if --

00:07:43.160 --> 00:07:45.616
if we win the wisdom race --

00:07:45.640 --> 00:07:48.496
the race between the growing
power of our technology

00:07:48.520 --> 00:07:50.720
and the growing wisdom
with which we manage it.

00:07:51.240 --> 00:07:53.536
But this is going to require
a change of strategy

00:07:53.560 --> 00:07:56.600
because our old strategy
has been learning from mistakes.

00:07:57.280 --> 00:07:58.816
We invented fire,

00:07:58.840 --> 00:08:00.376
screwed up a bunch of times --

00:08:00.400 --> 00:08:02.216
invented the fire extinguisher.

00:08:02.240 --> 00:08:03.576
(Laughter)

00:08:03.600 --> 00:08:06.016
We invented the car,
screwed up a bunch of times --

00:08:06.040 --> 00:08:08.707
invented the traffic light,
the seat belt and the airbag,

00:08:08.731 --> 00:08:12.576
but with more powerful technology
like nuclear weapons and AGI,

00:08:12.600 --> 00:08:15.976
learning from mistakes
is a lousy strategy,

00:08:16.000 --> 00:08:17.216
don't you think?

00:08:17.240 --> 00:08:18.256
(Laughter)

00:08:18.280 --> 00:08:20.856
It's much better to be proactive
rather than reactive;

00:08:20.880 --> 00:08:23.176
plan ahead and get things
right the first time

00:08:23.200 --> 00:08:25.696
because that might be
the only time we'll get.

00:08:25.720 --> 00:08:28.056
But it is funny because
sometimes people tell me,

00:08:28.080 --> 00:08:30.816
"Max, shhh, don't talk like that.

00:08:30.840 --> 00:08:32.560
That's Luddite scaremongering."

00:08:34.040 --> 00:08:35.576
But it's not scaremongering.

00:08:35.600 --> 00:08:38.480
It's what we at MIT
call safety engineering.

00:08:39.200 --> 00:08:40.416
Think about it:

00:08:40.440 --> 00:08:42.656
before NASA launched
the Apollo 11 mission,

00:08:42.680 --> 00:08:45.816
they systematically thought through
everything that could go wrong

00:08:45.840 --> 00:08:48.216
when you put people
on top of explosive fuel tanks

00:08:48.240 --> 00:08:50.856
and launch them somewhere
where no one could help them.

00:08:50.880 --> 00:08:52.816
And there was a lot that could go wrong.

00:08:52.840 --> 00:08:54.320
Was that scaremongering?

00:08:55.159 --> 00:08:56.376
No.

00:08:56.400 --> 00:08:58.416
That's was precisely
the safety engineering

00:08:58.440 --> 00:09:00.376
that ensured the success of the mission,

00:09:00.400 --> 00:09:04.576
and that is precisely the strategy
I think we should take with AGI.

00:09:04.600 --> 00:09:08.656
Think through what can go wrong
to make sure it goes right.

00:09:08.680 --> 00:09:11.216
So in this spirit,
we've organized conferences,

00:09:11.240 --> 00:09:14.056
bringing together leading
AI researchers and other thinkers

00:09:14.080 --> 00:09:17.816
to discuss how to grow this wisdom
we need to keep AI beneficial.

00:09:17.840 --> 00:09:21.136
Our last conference
was in Asilomar, California last year

00:09:21.160 --> 00:09:24.216
and produced this list of 23 principles

00:09:24.240 --> 00:09:27.136
which have since been signed
by over 1,000 AI researchers

00:09:27.160 --> 00:09:28.456
and key industry leaders,

00:09:28.480 --> 00:09:31.656
and I want to tell you
about three of these principles.

00:09:31.680 --> 00:09:36.640
One is that we should avoid an arms race
and lethal autonomous weapons.

00:09:37.480 --> 00:09:41.096
The idea here is that any science
can be used for new ways of helping people

00:09:41.120 --> 00:09:42.656
or new ways of harming people.

00:09:42.680 --> 00:09:46.616
For example, biology and chemistry
are much more likely to be used

00:09:46.640 --> 00:09:51.496
for new medicines or new cures
than for new ways of killing people,

00:09:51.520 --> 00:09:53.696
because biologists
and chemists pushed hard --

00:09:53.720 --> 00:09:54.976
and successfully --

00:09:55.000 --> 00:09:57.176
for bans on biological
and chemical weapons.

00:09:57.200 --> 00:09:58.456
And in the same spirit,

00:09:58.480 --> 00:10:02.920
most AI researchers want to stigmatize
and ban lethal autonomous weapons.

00:10:03.600 --> 00:10:05.416
Another Asilomar AI principle

00:10:05.440 --> 00:10:09.136
is that we should mitigate
AI-fueled income inequality.

00:10:09.160 --> 00:10:13.616
I think that if we can grow
the economic pie dramatically with AI

00:10:13.640 --> 00:10:16.096
and we still can't figure out
how to divide this pie

00:10:16.120 --> 00:10:17.696
so that everyone is better off,

00:10:17.720 --> 00:10:18.976
then shame on us.

00:10:19.000 --> 00:10:23.096
(Applause)

00:10:23.120 --> 00:10:26.720
Alright, now raise your hand
if your computer has ever crashed.

00:10:27.480 --> 00:10:28.736
(Laughter)

00:10:28.760 --> 00:10:30.416
Wow, that's a lot of hands.

00:10:30.440 --> 00:10:32.616
Well, then you'll appreciate
this principle

00:10:32.640 --> 00:10:35.776
that we should invest much more
in AI safety research,

00:10:35.800 --> 00:10:39.456
because as we put AI in charge
of even more decisions and infrastructure,

00:10:39.480 --> 00:10:43.096
we need to figure out how to transform
today's buggy and hackable computers

00:10:43.120 --> 00:10:45.536
into robust AI systems
that we can really trust,

00:10:45.560 --> 00:10:46.776
because otherwise,

00:10:46.800 --> 00:10:49.616
all this awesome new technology
can malfunction and harm us,

00:10:49.640 --> 00:10:51.616
or get hacked and be turned against us.

00:10:51.640 --> 00:10:57.336
And this AI safety work
has to include work on AI value alignment,

00:10:57.360 --> 00:11:00.176
because the real threat
from AGI isn't malice,

00:11:00.200 --> 00:11:01.856
like in silly Hollywood movies,

00:11:01.880 --> 00:11:03.616
but competence --

00:11:03.640 --> 00:11:07.056
AGI accomplishing goals
that just aren't aligned with ours.

00:11:07.080 --> 00:11:11.816
For example, when we humans drove
the West African black rhino extinct,

00:11:11.840 --> 00:11:15.736
we didn't do it because we were a bunch
of evil rhinoceros haters, did we?

00:11:15.760 --> 00:11:17.816
We did it because
we were smarter than them

00:11:17.840 --> 00:11:20.416
and our goals weren't aligned with theirs.

00:11:20.440 --> 00:11:23.096
But AGI is by definition smarter than us,

00:11:23.120 --> 00:11:26.696
so to make sure that we don't put
ourselves in the position of those rhinos

00:11:26.720 --> 00:11:28.696
if we create AGI,

00:11:28.720 --> 00:11:32.896
we need to figure out how
to make machines understand our goals,

00:11:32.920 --> 00:11:36.080
adopt our goals and retain our goals.

00:11:37.320 --> 00:11:40.176
And whose goals should these be, anyway?

00:11:40.200 --> 00:11:42.096
Which goals should they be?

00:11:42.120 --> 00:11:45.680
This brings us to the third part
of our rocket metaphor: the destination.

00:11:47.160 --> 00:11:49.016
We're making AI more powerful,

00:11:49.040 --> 00:11:50.856
trying to figure out how to steer it,

00:11:50.880 --> 00:11:52.560
but where do we want to go with it?

00:11:53.760 --> 00:11:57.416
This is the elephant in the room
that almost nobody talks about --

00:11:57.440 --> 00:11:59.296
not even here at TED --

00:11:59.320 --> 00:12:03.400
because we're so fixated
on short-term AI challenges.

00:12:04.080 --> 00:12:08.736
Look, our species is trying to build AGI,

00:12:08.760 --> 00:12:12.256
motivated by curiosity and economics,

00:12:12.280 --> 00:12:15.960
but what sort of future society
are we hoping for if we succeed?

00:12:16.680 --> 00:12:18.616
We did an opinion poll on this recently,

00:12:18.640 --> 00:12:19.856
and I was struck to see

00:12:19.880 --> 00:12:22.776
that most people actually
want us to build superintelligence:

00:12:22.800 --> 00:12:25.960
AI that's vastly smarter
than us in all ways.

00:12:27.120 --> 00:12:30.536
What there was the greatest agreement on
was that we should be ambitious

00:12:30.560 --> 00:12:32.576
and help life spread into the cosmos,

00:12:32.600 --> 00:12:37.096
but there was much less agreement
about who or what should be in charge.

00:12:37.120 --> 00:12:38.856
And I was actually quite amused

00:12:38.880 --> 00:12:42.336
to see that there's some some people
who want it to be just machines.

00:12:42.360 --> 00:12:44.056
(Laughter)

00:12:44.080 --> 00:12:47.936
And there was total disagreement
about what the role of humans should be,

00:12:47.960 --> 00:12:49.936
even at the most basic level,

00:12:49.960 --> 00:12:52.776
so let's take a closer look
at possible futures

00:12:52.800 --> 00:12:55.536
that we might choose
to steer toward, alright?

00:12:55.560 --> 00:12:56.896
So don't get me wrong here.

00:12:56.920 --> 00:12:58.976
I'm not talking about space travel,

00:12:59.000 --> 00:13:02.200
merely about humanity's
metaphorical journey into the future.

00:13:02.920 --> 00:13:06.416
So one option that some
of my AI colleagues like

00:13:06.440 --> 00:13:10.056
is to build superintelligence
and keep it under human control,

00:13:10.080 --> 00:13:11.816
like an enslaved god,

00:13:11.840 --> 00:13:13.416
disconnected from the internet

00:13:13.440 --> 00:13:16.696
and used to create unimaginable
technology and wealth

00:13:16.720 --> 00:13:17.960
for whoever controls it.

00:13:18.800 --> 00:13:20.256
But Lord Acton warned us

00:13:20.280 --> 00:13:23.896
that power corrupts,
and absolute power corrupts absolutely,

00:13:23.920 --> 00:13:27.976
so you might worry that maybe
we humans just aren't smart enough,

00:13:28.000 --> 00:13:29.536
or wise enough rather,

00:13:29.560 --> 00:13:30.800
to handle this much power.

00:13:31.640 --> 00:13:34.176
Also, aside from any
moral qualms you might have

00:13:34.200 --> 00:13:36.496
about enslaving superior minds,

00:13:36.520 --> 00:13:40.496
you might worry that maybe
the superintelligence could outsmart us,

00:13:40.520 --> 00:13:42.760
break out and take over.

00:13:43.560 --> 00:13:46.976
But I also have colleagues
who are fine with AI taking over

00:13:47.000 --> 00:13:49.296
and even causing human extinction,

00:13:49.320 --> 00:13:52.896
as long as we feel the the AIs
are our worthy descendants,

00:13:52.920 --> 00:13:54.656
like our children.

00:13:54.680 --> 00:14:00.296
But how would we know that the AIs
have adopted our best values

00:14:00.320 --> 00:14:04.696
and aren't just unconscious zombies
tricking us into anthropomorphizing them?

00:14:04.720 --> 00:14:07.576
Also, shouldn't those people
who don't want human extinction

00:14:07.600 --> 00:14:09.040
have a say in the matter, too?

00:14:10.200 --> 00:14:13.576
Now, if you didn't like either
of those two high-tech options,

00:14:13.600 --> 00:14:16.776
it's important to remember
that low-tech is suicide

00:14:16.800 --> 00:14:18.056
from a cosmic perspective,

00:14:18.080 --> 00:14:20.576
because if we don't go far
beyond today's technology,

00:14:20.600 --> 00:14:23.416
the question isn't whether humanity
is going to go extinct,

00:14:23.440 --> 00:14:25.456
merely whether
we're going to get taken out

00:14:25.480 --> 00:14:27.616
by the next killer asteroid, supervolcano

00:14:27.640 --> 00:14:30.736
or some other problem
that better technology could have solved.

00:14:30.760 --> 00:14:34.336
So, how about having
our cake and eating it ...

00:14:34.360 --> 00:14:36.200
with AGI that's not enslaved

00:14:37.120 --> 00:14:40.296
but treats us well because its values
are aligned with ours?

00:14:40.320 --> 00:14:44.496
This is the gist of what Eliezer Yudkowsky
has called "friendly AI,"

00:14:44.520 --> 00:14:47.200
and if we can do this,
it could be awesome.

00:14:47.840 --> 00:14:52.656
It could not only eliminate negative
experiences like disease, poverty,

00:14:52.680 --> 00:14:54.136
crime and other suffering,

00:14:54.160 --> 00:14:56.976
but it could also give us
the freedom to choose

00:14:57.000 --> 00:15:01.056
from a fantastic new diversity
of positive experiences --

00:15:01.080 --> 00:15:04.240
basically making us
the masters of our own destiny.

00:15:06.280 --> 00:15:07.656
So in summary,

00:15:07.680 --> 00:15:10.776
our situation with technology
is complicated,

00:15:10.800 --> 00:15:13.216
but the big picture is rather simple.

00:15:13.240 --> 00:15:16.696
Most AI researchers
expect AGI within decades,

00:15:16.720 --> 00:15:19.856
and if we just bumble
into this unprepared,

00:15:19.880 --> 00:15:23.216
it will probably be
the biggest mistake in human history --

00:15:23.240 --> 00:15:24.656
let's face it.

00:15:24.680 --> 00:15:27.256
It could enable brutal,
global dictatorship

00:15:27.280 --> 00:15:30.816
with unprecedented inequality,
surveillance and suffering,

00:15:30.840 --> 00:15:32.816
and maybe even human extinction.

00:15:32.840 --> 00:15:35.160
But if we steer carefully,

00:15:36.040 --> 00:15:39.936
we could end up in a fantastic future
where everybody's better off:

00:15:39.960 --> 00:15:42.336
the poor are richer, the rich are richer,

00:15:42.360 --> 00:15:46.320
everybody is healthy
and free to live out their dreams.

00:15:47.000 --> 00:15:48.536
Now, hang on.

00:15:48.560 --> 00:15:53.136
Do you folks want the future
that's politically right or left?

00:15:53.160 --> 00:15:56.016
Do you want the pious society
with strict moral rules,

00:15:56.040 --> 00:15:57.856
or do you an hedonistic free-for-all,

00:15:57.880 --> 00:16:00.096
more like Burning Man 24/7?

00:16:00.120 --> 00:16:02.536
Do you want beautiful beaches,
forests and lakes,

00:16:02.560 --> 00:16:05.976
or would you prefer to rearrange
some of those atoms with the computers,

00:16:06.000 --> 00:16:07.715
enabling virtual experiences?

00:16:07.739 --> 00:16:10.896
With friendly AI, we could simply
build all of these societies

00:16:10.920 --> 00:16:14.136
and give people the freedom
to choose which one they want to live in

00:16:14.160 --> 00:16:17.256
because we would no longer
be limited by our intelligence,

00:16:17.280 --> 00:16:18.736
merely by the laws of physics.

00:16:18.760 --> 00:16:23.376
So the resources and space
for this would be astronomical --

00:16:23.400 --> 00:16:24.720
literally.

00:16:25.320 --> 00:16:26.520
So here's our choice.

00:16:27.880 --> 00:16:30.200
We can either be complacent
about our future,

00:16:31.440 --> 00:16:34.096
taking as an article of blind faith

00:16:34.120 --> 00:16:38.136
that any new technology
is guaranteed to be beneficial,

00:16:38.160 --> 00:16:42.296
and just repeat that to ourselves
as a mantra over and over and over again

00:16:42.320 --> 00:16:46.000
as we drift like a rudderless ship
towards our own obsolescence.

00:16:46.920 --> 00:16:48.800
Or we can be ambitious --

00:16:49.840 --> 00:16:52.296
thinking hard about how
to steer our technology

00:16:52.320 --> 00:16:54.256
and where we want to go with it

00:16:54.280 --> 00:16:56.040
to create the age of amazement.

00:16:57.000 --> 00:16:59.856
We're all here to celebrate
the age of amazement,

00:16:59.880 --> 00:17:04.320
and I feel that its essence should lie
in becoming not overpowered

00:17:05.240 --> 00:17:07.856
but empowered by our technology.

00:17:07.880 --> 00:17:09.256
Thank you.

00:17:09.280 --> 00:17:12.360
(Applause)

