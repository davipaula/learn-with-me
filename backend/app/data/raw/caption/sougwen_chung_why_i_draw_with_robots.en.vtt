WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:07.000
Translator: Ivana Korom
Reviewer: Camille MartÃ­nez

00:00:12.937 --> 00:00:16.102
Many of us here use technology
in our day-to-day.

00:00:16.126 --> 00:00:19.373
And some of us rely
on technology to do our jobs.

00:00:19.397 --> 00:00:23.347
For a while, I thought of machines
and the technologies that drive them

00:00:23.371 --> 00:00:27.876
as perfect tools that could make my work
more efficient and more productive.

00:00:28.403 --> 00:00:31.657
But with the rise of automation
across so many different industries,

00:00:31.681 --> 00:00:33.053
it led me to wonder:

00:00:33.077 --> 00:00:35.418
If machines are starting
to be able to do the work

00:00:35.442 --> 00:00:37.109
traditionally done by humans,

00:00:37.133 --> 00:00:39.466
what will become of the human hand?

00:00:40.133 --> 00:00:44.226
How does our desire for perfection,
precision and automation

00:00:44.250 --> 00:00:46.172
affect our ability to be creative?

00:00:46.553 --> 00:00:50.640
In my work as an artist and researcher,
I explore AI and robotics

00:00:50.664 --> 00:00:53.669
to develop new processes
for human creativity.

00:00:54.077 --> 00:00:55.363
For the past few years,

00:00:55.387 --> 00:00:59.763
I've made work alongside machines,
data and emerging technologies.

00:01:00.143 --> 00:01:02.004
It's part of a lifelong fascination

00:01:02.028 --> 00:01:04.763
about the dynamics
of individuals and systems

00:01:04.787 --> 00:01:07.168
and all the messiness that that entails.

00:01:07.192 --> 00:01:12.000
It's how I'm exploring questions about
where AI ends and we begin

00:01:12.024 --> 00:01:13.666
and where I'm developing processes

00:01:13.690 --> 00:01:17.016
that investigate potential
sensory mixes of the future.

00:01:17.675 --> 00:01:20.532
I think it's where philosophy
and technology intersect.

00:01:20.992 --> 00:01:23.231
Doing this work
has taught me a few things.

00:01:23.642 --> 00:01:26.466
It's taught me how embracing imperfection

00:01:26.490 --> 00:01:28.979
can actually teach us
something about ourselves.

00:01:29.428 --> 00:01:31.764
It's taught me that exploring art

00:01:31.788 --> 00:01:34.719
can actually help shape
the technology that shapes us.

00:01:35.148 --> 00:01:38.409
And it's taught me
that combining AI and robotics

00:01:38.433 --> 00:01:41.965
with traditional forms of creativity --
visual arts in my case --

00:01:41.989 --> 00:01:44.291
can help us think a little bit more deeply

00:01:44.315 --> 00:01:47.212
about what is human
and what is the machine.

00:01:47.942 --> 00:01:49.649
And it's led me to the realization

00:01:49.673 --> 00:01:52.728
that collaboration is the key
to creating the space for both

00:01:52.752 --> 00:01:54.019
as we move forward.

00:01:54.387 --> 00:01:57.133
It all started with a simple
experiment with machines,

00:01:57.157 --> 00:01:59.983
called "Drawing Operations
Unit: Generation 1."

00:02:00.434 --> 00:02:02.950
I call the machine "D.O.U.G." for short.

00:02:02.974 --> 00:02:04.300
Before I built D.O.U.G,

00:02:04.324 --> 00:02:06.689
I didn't know anything
about building robots.

00:02:07.220 --> 00:02:10.117
I took some open-source
robotic arm designs,

00:02:10.141 --> 00:02:13.482
I hacked together a system
where the robot would match my gestures

00:02:13.506 --> 00:02:15.145
and follow [them] in real time.

00:02:15.169 --> 00:02:16.617
The premise was simple:

00:02:16.641 --> 00:02:18.841
I would lead, and it would follow.

00:02:19.403 --> 00:02:22.339
I would draw a line,
and it would mimic my line.

00:02:22.363 --> 00:02:26.061
So back in 2015, there we were,
drawing for the first time,

00:02:26.085 --> 00:02:28.704
in front of a small audience
in New York City.

00:02:28.728 --> 00:02:31.283
The process was pretty sparse --

00:02:31.307 --> 00:02:34.794
no lights, no sounds,
nothing to hide behind.

00:02:35.241 --> 00:02:38.636
Just my palms sweating
and the robot's new servos heating up.

00:02:38.950 --> 00:02:41.391
(Laughs) Clearly, we were
not built for this.

00:02:41.820 --> 00:02:45.053
But something interesting happened,
something I didn't anticipate.

00:02:45.077 --> 00:02:49.879
See, D.O.U.G., in its primitive form,
wasn't tracking my line perfectly.

00:02:49.903 --> 00:02:52.236
While in the simulation
that happened onscreen

00:02:52.260 --> 00:02:53.617
it was pixel-perfect,

00:02:53.641 --> 00:02:56.172
in physical reality,
it was a different story.

00:02:56.196 --> 00:02:59.013
It would slip and slide
and punctuate and falter,

00:02:59.037 --> 00:03:01.105
and I would be forced to respond.

00:03:01.525 --> 00:03:03.303
There was nothing pristine about it.

00:03:03.327 --> 00:03:06.565
And yet, somehow, the mistakes
made the work more interesting.

00:03:06.589 --> 00:03:09.343
The machine was interpreting
my line but not perfectly.

00:03:09.367 --> 00:03:10.739
And I was forced to respond.

00:03:10.763 --> 00:03:13.472
We were adapting
to each other in real time.

00:03:13.496 --> 00:03:15.433
And seeing this taught me a few things.

00:03:15.457 --> 00:03:20.337
It showed me that our mistakes
actually made the work more interesting.

00:03:20.663 --> 00:03:24.912
And I realized that, you know,
through the imperfection of the machine,

00:03:24.936 --> 00:03:28.641
our imperfections became
what was beautiful about the interaction.

00:03:29.650 --> 00:03:32.737
And I was excited,
because it led me to the realization

00:03:32.761 --> 00:03:36.411
that maybe part of the beauty
of human and machine systems

00:03:36.435 --> 00:03:39.173
is their shared inherent fallibility.

00:03:39.197 --> 00:03:41.017
For the second generation of D.O.U.G.,

00:03:41.041 --> 00:03:43.348
I knew I wanted to explore this idea.

00:03:43.372 --> 00:03:47.790
But instead of an accident produced
by pushing a robotic arm to its limits,

00:03:47.814 --> 00:03:50.711
I wanted to design a system
that would respond to my drawings

00:03:50.735 --> 00:03:52.568
in ways that I didn't expect.

00:03:52.592 --> 00:03:56.441
So, I used a visual algorithm
to extract visual information

00:03:56.465 --> 00:03:59.443
from decades of my digital
and analog drawings.

00:03:59.467 --> 00:04:01.522
I trained a neural net on these drawings

00:04:01.546 --> 00:04:04.411
in order to generate
recurring patterns in the work

00:04:04.435 --> 00:04:07.911
that were then fed through custom software
back into the machine.

00:04:07.935 --> 00:04:12.321
I painstakingly collected
as many of my drawings as I could find --

00:04:12.345 --> 00:04:16.560
finished works, unfinished experiments
and random sketches --

00:04:16.584 --> 00:04:18.583
and tagged them for the AI system.

00:04:18.607 --> 00:04:22.291
And since I'm an artist,
I've been making work for over 20 years.

00:04:22.315 --> 00:04:24.339
Collecting that many drawings took months,

00:04:24.363 --> 00:04:25.752
it was a whole thing.

00:04:25.776 --> 00:04:28.371
And here's the thing
about training AI systems:

00:04:28.395 --> 00:04:30.595
it's actually a lot of hard work.

00:04:31.022 --> 00:04:33.213
A lot of work goes on behind the scenes.

00:04:33.237 --> 00:04:35.918
But in doing the work,
I realized a little bit more

00:04:35.942 --> 00:04:39.363
about how the architecture
of an AI is constructed.

00:04:39.387 --> 00:04:42.334
And I realized it's not just made
of models and classifiers

00:04:42.358 --> 00:04:43.680
for the neural network.

00:04:43.704 --> 00:04:47.236
But it's a fundamentally
malleable and shapable system,

00:04:47.260 --> 00:04:50.371
one in which the human hand
is always present.

00:04:50.395 --> 00:04:54.395
It's far from the omnipotent AI
we've been told to believe in.

00:04:54.419 --> 00:04:56.934
So I collected these drawings
for the neural net.

00:04:56.958 --> 00:05:00.887
And we realized something
that wasn't previously possible.

00:05:00.911 --> 00:05:05.002
My robot D.O.U.G. became
a real-time interactive reflection

00:05:05.026 --> 00:05:07.653
of the work I'd done
through the course of my life.

00:05:07.677 --> 00:05:11.542
The data was personal,
but the results were powerful.

00:05:11.566 --> 00:05:13.050
And I got really excited,

00:05:13.074 --> 00:05:17.656
because I started thinking maybe
machines don't need to be just tools,

00:05:17.680 --> 00:05:21.100
but they can function
as nonhuman collaborators.

00:05:21.537 --> 00:05:23.084
And even more than that,

00:05:23.108 --> 00:05:25.537
I thought maybe
the future of human creativity

00:05:25.561 --> 00:05:27.085
isn't in what it makes

00:05:27.109 --> 00:05:30.545
but how it comes together
to explore new ways of making.

00:05:31.101 --> 00:05:33.291
So if D.O.U.G._1 was the muscle,

00:05:33.315 --> 00:05:35.077
and D.O.U.G._2 was the brain,

00:05:35.101 --> 00:05:38.029
then I like to think
of D.O.U.G._3 as the family.

00:05:38.482 --> 00:05:43.275
I knew I wanted to explore this idea
of human-nonhuman collaboration at scale.

00:05:43.299 --> 00:05:44.672
So over the past few months,

00:05:44.696 --> 00:05:47.831
I worked with my team
to develop 20 custom robots

00:05:47.855 --> 00:05:49.815
that could work with me as a collective.

00:05:49.839 --> 00:05:51.132
They would work as a group,

00:05:51.156 --> 00:05:54.045
and together, we would collaborate
with all of New York City.

00:05:54.069 --> 00:05:57.013
I was really inspired
by Stanford researcher Fei-Fei Li,

00:05:57.037 --> 00:05:59.552
who said, "if we want to teach
machines how to think,

00:05:59.576 --> 00:06:01.560
we need to first teach them how to see."

00:06:01.584 --> 00:06:04.369
It made me think of the past decade
of my life in New York,

00:06:04.393 --> 00:06:08.386
and how I'd been all watched over by these
surveillance cameras around the city.

00:06:08.410 --> 00:06:10.466
And I thought it would be
really interesting

00:06:10.490 --> 00:06:12.895
if I could use them
to teach my robots to see.

00:06:12.919 --> 00:06:14.807
So with this project,

00:06:14.831 --> 00:06:16.798
I thought about the gaze of the machine,

00:06:16.822 --> 00:06:20.048
and I began to think about vision
as multidimensional,

00:06:20.072 --> 00:06:21.672
as views from somewhere.

00:06:22.151 --> 00:06:23.985
We collected video

00:06:24.009 --> 00:06:27.072
from publicly available
camera feeds on the internet

00:06:27.096 --> 00:06:28.786
of people walking on the sidewalks,

00:06:28.810 --> 00:06:30.522
cars and taxis on the road,

00:06:30.546 --> 00:06:32.363
all kinds of urban movement.

00:06:33.188 --> 00:06:35.791
We trained a vision algorithm
on those feeds

00:06:35.815 --> 00:06:38.101
based on a technique
called "optical flow,"

00:06:38.125 --> 00:06:40.102
to analyze the collective density,

00:06:40.126 --> 00:06:43.763
direction, dwell and velocity states
of urban movement.

00:06:44.178 --> 00:06:48.447
Our system extracted those states
from the feeds as positional data

00:06:48.471 --> 00:06:51.844
and became pads for my
robotic units to draw on.

00:06:51.868 --> 00:06:54.402
Instead of a collaboration of one-to-one,

00:06:54.426 --> 00:06:57.450
we made a collaboration of many-to-many.

00:06:57.474 --> 00:07:01.061
By combining the vision of human
and machine in the city,

00:07:01.085 --> 00:07:03.879
we reimagined what
a landscape painting could be.

00:07:03.903 --> 00:07:06.121
Throughout all of my
experiments with D.O.U.G.,

00:07:06.145 --> 00:07:08.862
no two performances
have ever been the same.

00:07:08.886 --> 00:07:10.268
And through collaboration,

00:07:10.292 --> 00:07:13.156
we create something that neither of us
could have done alone:

00:07:13.180 --> 00:07:15.791
we explore the boundaries
of our creativity,

00:07:15.815 --> 00:07:18.707
human and nonhuman working in parallel.

00:07:19.823 --> 00:07:22.157
I think this is just the beginning.

00:07:22.569 --> 00:07:24.752
This year, I've launched Scilicet,

00:07:24.776 --> 00:07:29.021
my new lab exploring human
and interhuman collaboration.

00:07:29.339 --> 00:07:31.459
We're really interested
in the feedback loop

00:07:31.483 --> 00:07:35.713
between individual, artificial
and ecological systems.

00:07:36.276 --> 00:07:38.545
We're connecting human and machine output

00:07:38.569 --> 00:07:41.553
to biometrics and other kinds
of environmental data.

00:07:41.577 --> 00:07:45.656
We're inviting anyone who's interested
in the future of work, systems

00:07:45.680 --> 00:07:47.275
and interhuman collaboration

00:07:47.299 --> 00:07:48.849
to explore with us.

00:07:48.873 --> 00:07:52.278
We know it's not just technologists
that have to do this work

00:07:52.302 --> 00:07:54.405
and that we all have a role to play.

00:07:54.429 --> 00:07:56.672
We believe that by teaching machines

00:07:56.696 --> 00:07:59.426
how to do the work
traditionally done by humans,

00:07:59.450 --> 00:08:02.403
we can explore and evolve our criteria

00:08:02.427 --> 00:08:04.870
of what's made possible by the human hand.

00:08:04.894 --> 00:08:08.387
And part of that journey
is embracing the imperfections

00:08:08.411 --> 00:08:12.101
and recognizing the fallibility
of both human and machine,

00:08:12.125 --> 00:08:14.530
in order to expand the potential of both.

00:08:14.919 --> 00:08:17.220
Today, I'm still in pursuit
of finding the beauty

00:08:17.244 --> 00:08:19.520
in human and nonhuman creativity.

00:08:19.865 --> 00:08:22.694
In the future, I have no idea
what that will look like,

00:08:23.627 --> 00:08:25.651
but I'm pretty curious to find out.

00:08:25.675 --> 00:08:26.826
Thank you.

00:08:26.850 --> 00:08:28.734
(Applause)

