WEBVTT
Kind: captions
Language: en

00:00:13.131 --> 00:00:15.539
Chris Anderson:
What worries you right now?

00:00:15.563 --> 00:00:18.416
You've been very open
about lots of issues on Twitter.

00:00:18.440 --> 00:00:20.739
What would be your top worry

00:00:20.763 --> 00:00:22.812
about where things are right now?

00:00:23.447 --> 00:00:26.376
Jack Dorsey: Right now,
the health of the conversation.

00:00:26.400 --> 00:00:30.060
So, our purpose is to serve
the public conversation,

00:00:30.084 --> 00:00:35.140
and we have seen
a number of attacks on it.

00:00:35.164 --> 00:00:37.589
We've seen abuse, we've seen harassment,

00:00:37.613 --> 00:00:40.835
we've seen manipulation,

00:00:40.859 --> 00:00:45.124
automation, human coordination,
misinformation.

00:00:46.134 --> 00:00:50.168
So these are all dynamics
that we were not expecting

00:00:50.192 --> 00:00:53.910
13 years ago when we were
starting the company.

00:00:53.934 --> 00:00:56.598
But we do now see them at scale,

00:00:56.622 --> 00:01:01.900
and what worries me most
is just our ability to address it

00:01:01.924 --> 00:01:05.032
in a systemic way that is scalable,

00:01:05.056 --> 00:01:12.032
that has a rigorous understanding
of how we're taking action,

00:01:12.056 --> 00:01:15.161
a transparent understanding
of how we're taking action

00:01:15.185 --> 00:01:18.286
and a rigorous appeals process
for when we're wrong,

00:01:18.310 --> 00:01:20.479
because we will be wrong.

00:01:20.503 --> 00:01:22.900
Whitney Pennington Rodgers:
I'm really glad to hear

00:01:22.924 --> 00:01:24.852
that that's something that concerns you,

00:01:24.876 --> 00:01:27.506
because I think there's been
a lot written about people

00:01:27.530 --> 00:01:30.007
who feel they've been abused
and harassed on Twitter,

00:01:30.031 --> 00:01:34.133
and I think no one more so
than women and women of color

00:01:34.157 --> 00:01:35.327
and black women.

00:01:35.351 --> 00:01:37.264
And there's been data that's come out --

00:01:37.288 --> 00:01:40.197
Amnesty International put out
a report a few months ago

00:01:40.221 --> 00:01:44.701
where they showed that a subset
of active black female Twitter users

00:01:44.725 --> 00:01:48.181
were receiving, on average,
one in 10 of their tweets

00:01:48.205 --> 00:01:50.304
were some form of harassment.

00:01:50.328 --> 00:01:54.235
And so when you think about health
for the community on Twitter,

00:01:54.259 --> 00:01:58.283
I'm interested to hear,
"health for everyone,"

00:01:58.307 --> 00:02:01.432
but specifically: How are you looking
to make Twitter a safe space

00:02:01.456 --> 00:02:05.620
for that subset, for women,
for women of color and black women?

00:02:05.644 --> 00:02:06.808
JD: Yeah.

00:02:06.832 --> 00:02:09.475
So it's a pretty terrible situation

00:02:09.499 --> 00:02:11.118
when you're coming to a service

00:02:11.142 --> 00:02:15.463
that, ideally, you want to learn
something about the world,

00:02:15.487 --> 00:02:20.930
and you spend the majority of your time
reporting abuse, receiving abuse,

00:02:20.954 --> 00:02:22.758
receiving harassment.

00:02:23.373 --> 00:02:29.694
So what we're looking most deeply at
is just the incentives

00:02:29.718 --> 00:02:33.541
that the platform naturally provides
and the service provides.

00:02:34.262 --> 00:02:38.839
Right now, the dynamic of the system
makes it super-easy to harass

00:02:38.863 --> 00:02:42.527
and to abuse others through the service,

00:02:42.551 --> 00:02:45.813
and unfortunately, the majority
of our system in the past

00:02:45.837 --> 00:02:51.433
worked entirely based on people
reporting harassment and abuse.

00:02:51.457 --> 00:02:56.532
So about midway last year,
we decided that we were going to apply

00:02:56.556 --> 00:03:00.538
a lot more machine learning,
a lot more deep learning to the problem,

00:03:00.562 --> 00:03:05.100
and try to be a lot more proactive
around where abuse is happening,

00:03:05.124 --> 00:03:09.084
so that we can take the burden
off the victim completely.

00:03:09.108 --> 00:03:11.543
And we've made some progress recently.

00:03:11.567 --> 00:03:18.256
About 38 percent of abusive tweets
are now proactively identified

00:03:18.280 --> 00:03:19.995
by machine learning algorithms

00:03:20.019 --> 00:03:22.353
so that people don't actually
have to report them.

00:03:22.377 --> 00:03:25.682
But those that are identified
are still reviewed by humans,

00:03:25.706 --> 00:03:31.090
so we do not take down content or accounts
without a human actually reviewing it.

00:03:31.114 --> 00:03:33.873
But that was from zero percent
just a year ago.

00:03:33.897 --> 00:03:35.828
So that meant, at that zero percent,

00:03:35.852 --> 00:03:39.502
every single person who received abuse
had to actually report it,

00:03:39.526 --> 00:03:43.105
which was a lot of work for them,
a lot of work for us

00:03:43.129 --> 00:03:45.147
and just ultimately unfair.

00:03:46.528 --> 00:03:50.308
The other thing that we're doing
is making sure that we, as a company,

00:03:50.332 --> 00:03:53.665
have representation of all the communities
that we're trying to serve.

00:03:53.689 --> 00:03:55.848
We can't build a business
that is successful

00:03:55.872 --> 00:03:59.172
unless we have a diversity
of perspective inside of our walls

00:03:59.196 --> 00:04:02.928
that actually feel these issues
every single day.

00:04:02.952 --> 00:04:06.690
And that's not just with the team
that's doing the work,

00:04:06.714 --> 00:04:08.810
it's also within our leadership as well.

00:04:08.834 --> 00:04:14.591
So we need to continue to build empathy
for what people are experiencing

00:04:14.615 --> 00:04:17.931
and give them better tools to act on it

00:04:17.955 --> 00:04:22.207
and also give our customers
a much better and easier approach

00:04:22.231 --> 00:04:24.613
to handle some of the things
that they're seeing.

00:04:24.637 --> 00:04:27.903
So a lot of what we're doing
is around technology,

00:04:27.927 --> 00:04:32.235
but we're also looking at
the incentives on the service:

00:04:32.259 --> 00:04:37.442
What does Twitter incentivize you to do
when you first open it up?

00:04:37.466 --> 00:04:38.760
And in the past,

00:04:40.670 --> 00:04:46.214
it's incented a lot of outrage,
it's incented a lot of mob behavior,

00:04:46.238 --> 00:04:48.697
it's incented a lot of group harassment.

00:04:48.721 --> 00:04:52.369
And we have to look a lot deeper
at some of the fundamentals

00:04:52.393 --> 00:04:55.351
of what the service is doing
to make the bigger shifts.

00:04:55.375 --> 00:04:59.406
We can make a bunch of small shifts
around technology, as I just described,

00:04:59.430 --> 00:05:03.816
but ultimately, we have to look deeply
at the dynamics in the network itself,

00:05:03.840 --> 00:05:05.208
and that's what we're doing.

00:05:05.232 --> 00:05:07.292
CA: But what's your sense --

00:05:07.316 --> 00:05:11.279
what is the kind of thing
that you might be able to change

00:05:11.303 --> 00:05:14.052
that would actually
fundamentally shift behavior?

00:05:15.386 --> 00:05:16.866
JD: Well, one of the things --

00:05:16.890 --> 00:05:22.230
we started the service
with this concept of following an account,

00:05:22.254 --> 00:05:23.979
as an example,

00:05:24.003 --> 00:05:28.352
and I don't believe that's why
people actually come to Twitter.

00:05:28.376 --> 00:05:33.233
I believe Twitter is best
as an interest-based network.

00:05:33.257 --> 00:05:36.710
People come with a particular interest.

00:05:36.734 --> 00:05:40.221
They have to do a ton of work
to find and follow the related accounts

00:05:40.245 --> 00:05:41.650
around those interests.

00:05:42.217 --> 00:05:45.614
What we could do instead
is allow you to follow an interest,

00:05:45.638 --> 00:05:47.741
follow a hashtag, follow a trend,

00:05:47.765 --> 00:05:49.519
follow a community,

00:05:49.543 --> 00:05:54.180
which gives us the opportunity
to show all of the accounts,

00:05:54.204 --> 00:05:57.527
all the topics, all the moments,
all the hashtags

00:05:57.551 --> 00:06:01.543
that are associated with that
particular topic and interest,

00:06:01.567 --> 00:06:06.167
which really opens up
the perspective that you see.

00:06:06.191 --> 00:06:08.348
But that is a huge fundamental shift

00:06:08.372 --> 00:06:12.164
to bias the entire network
away from just an account bias

00:06:12.188 --> 00:06:14.775
towards a topics and interest bias.

00:06:15.283 --> 00:06:18.431
CA: Because isn't it the case

00:06:19.375 --> 00:06:22.916
that one reason why you have
so much content on there

00:06:22.940 --> 00:06:26.531
is a result of putting millions
of people around the world

00:06:26.555 --> 00:06:29.697
in this kind of gladiatorial
contest with each other

00:06:29.721 --> 00:06:31.811
for followers, for attention?

00:06:31.835 --> 00:06:35.952
Like, from the point of view
of people who just read Twitter,

00:06:35.976 --> 00:06:37.131
that's not an issue,

00:06:37.155 --> 00:06:40.505
but for the people who actually create it,
everyone's out there saying,

00:06:40.529 --> 00:06:43.765
"You know, I wish I had
a few more 'likes,' followers, retweets."

00:06:43.789 --> 00:06:45.937
And so they're constantly experimenting,

00:06:45.961 --> 00:06:47.922
trying to find the path to do that.

00:06:47.946 --> 00:06:52.072
And what we've all discovered
is that the number one path to do that

00:06:52.096 --> 00:06:55.502
is to be some form of provocative,

00:06:55.526 --> 00:06:58.506
obnoxious, eloquently obnoxious,

00:06:58.530 --> 00:07:02.046
like, eloquent insults
are a dream on Twitter,

00:07:02.070 --> 00:07:04.673
where you rapidly pile up --

00:07:04.697 --> 00:07:09.305
and it becomes this self-fueling
process of driving outrage.

00:07:09.329 --> 00:07:11.680
How do you defuse that?

00:07:12.624 --> 00:07:15.571
JD: Yeah, I mean, I think you're spot on,

00:07:15.595 --> 00:07:17.481
but that goes back to the incentives.

00:07:17.505 --> 00:07:20.137
Like, one of the choices
we made in the early days was

00:07:20.161 --> 00:07:24.862
we had this number that showed
how many people follow you.

00:07:24.886 --> 00:07:27.845
We decided that number
should be big and bold,

00:07:27.869 --> 00:07:31.609
and anything that's on the page
that's big and bold has importance,

00:07:31.633 --> 00:07:33.911
and those are the things
that you want to drive.

00:07:33.935 --> 00:07:35.842
Was that the right decision at the time?

00:07:35.866 --> 00:07:37.019
Probably not.

00:07:37.043 --> 00:07:38.848
If I had to start the service again,

00:07:38.872 --> 00:07:41.270
I would not emphasize
the follower count as much.

00:07:41.294 --> 00:07:43.589
I would not emphasize
the "like" count as much.

00:07:43.613 --> 00:07:46.733
I don't think I would even
create "like" in the first place,

00:07:46.757 --> 00:07:50.024
because it doesn't actually push

00:07:50.048 --> 00:07:53.227
what we believe now
to be the most important thing,

00:07:53.251 --> 00:07:56.290
which is healthy contribution
back to the network

00:07:56.314 --> 00:07:58.966
and conversation to the network,

00:07:58.990 --> 00:08:01.062
participation within conversation,

00:08:01.086 --> 00:08:03.579
learning something from the conversation.

00:08:03.603 --> 00:08:06.427
Those are not things
that we thought of 13 years ago,

00:08:06.451 --> 00:08:08.890
and we believe are extremely
important right now.

00:08:08.914 --> 00:08:11.937
So we have to look at
how we display the follower count,

00:08:11.961 --> 00:08:14.326
how we display retweet count,

00:08:14.350 --> 00:08:15.751
how we display "likes,"

00:08:15.775 --> 00:08:18.029
and just ask the deep question:

00:08:18.053 --> 00:08:21.101
Is this really the number
that we want people to drive up?

00:08:21.125 --> 00:08:23.670
Is this the thing that,
when you open Twitter,

00:08:23.694 --> 00:08:26.210
you see, "That's the thing
I need to increase?"

00:08:26.234 --> 00:08:28.378
And I don't believe
that's the case right now.

00:08:28.402 --> 00:08:30.505
(Applause)

00:08:30.529 --> 00:08:32.881
WPR: I think we should look at
some of the tweets

00:08:32.905 --> 00:08:35.074
that are coming
in from the audience as well.

00:08:35.868 --> 00:08:38.304
CA: Let's see what you guys are asking.

00:08:38.328 --> 00:08:41.622
I mean, this is -- generally, one
of the amazing things about Twitter

00:08:41.646 --> 00:08:43.940
is how you can use it for crowd wisdom,

00:08:43.964 --> 00:08:48.804
you know, that more knowledge,
more questions, more points of view

00:08:48.828 --> 00:08:50.066
than you can imagine,

00:08:50.090 --> 00:08:53.779
and sometimes, many of them
are really healthy.

00:08:53.803 --> 00:08:56.703
WPR: I think one I saw that 
passed already quickly down here,

00:08:56.717 --> 00:09:00.241
"What's Twitter's plan to combat
foreign meddling in the 2020 US election?"

00:09:00.265 --> 00:09:02.836
I think that's something
that's an issue we're seeing

00:09:02.860 --> 00:09:04.761
on the internet in general,

00:09:04.785 --> 00:09:08.452
that we have a lot of malicious
automated activity happening.

00:09:08.476 --> 00:09:13.849
And on Twitter, for example,
in fact, we have some work

00:09:13.873 --> 00:09:16.631
that's come from our friends
at Zignal Labs,

00:09:16.655 --> 00:09:19.311
and maybe we can even see that
to give us an example

00:09:19.335 --> 00:09:21.262
of what exactly I'm talking about,

00:09:21.286 --> 00:09:24.490
where you have these bots, if you will,

00:09:24.514 --> 00:09:29.064
or coordinated automated
malicious account activity,

00:09:29.088 --> 00:09:31.852
that is being used to influence
things like elections.

00:09:31.876 --> 00:09:35.719
And in this example we have
from Zignal which they've shared with us

00:09:35.743 --> 00:09:37.941
using the data that
they have from Twitter,

00:09:37.965 --> 00:09:40.406
you actually see that in this case,

00:09:40.430 --> 00:09:44.800
white represents the humans --
human accounts, each dot is an account.

00:09:44.824 --> 00:09:46.183
The pinker it is,

00:09:46.207 --> 00:09:47.947
the more automated the activity is.

00:09:47.971 --> 00:09:53.941
And you can see how you have
a few humans interacting with bots.

00:09:53.965 --> 00:09:58.384
In this case, it's related
to the election in Israel

00:09:58.408 --> 00:10:01.241
and spreading misinformation
about Benny Gantz,

00:10:01.265 --> 00:10:03.927
and as we know, in the end,
that was an election

00:10:03.951 --> 00:10:07.675
that Netanyahu won by a slim margin,

00:10:07.699 --> 00:10:10.541
and that may have been
in some case influenced by this.

00:10:10.565 --> 00:10:13.180
And when you think about
that happening on Twitter,

00:10:13.204 --> 00:10:15.660
what are the things
that you're doing, specifically,

00:10:15.684 --> 00:10:19.386
to ensure you don't have misinformation
like this spreading in this way,

00:10:19.410 --> 00:10:23.591
influencing people in ways
that could affect democracy?

00:10:23.615 --> 00:10:25.386
JD: Just to back up a bit,

00:10:25.410 --> 00:10:28.385
we asked ourselves a question:

00:10:28.409 --> 00:10:32.225
Can we actually measure
the health of a conversation,

00:10:32.249 --> 00:10:33.537
and what does that mean?

00:10:33.561 --> 00:10:36.943
And in the same way
that you have indicators

00:10:36.967 --> 00:10:40.434
and we have indicators as humans
in terms of are we healthy or not,

00:10:40.458 --> 00:10:45.116
such as temperature,
the flushness of your face,

00:10:45.140 --> 00:10:49.700
we believe that we could find
the indicators of conversational health.

00:10:49.724 --> 00:10:53.567
And we worked with a lab
called Cortico at MIT

00:10:54.479 --> 00:11:00.570
to propose four starter indicators

00:11:00.594 --> 00:11:04.264
that we believe we could ultimately
measure on the system.

00:11:05.249 --> 00:11:10.853
And the first one is
what we're calling shared attention.

00:11:10.877 --> 00:11:14.458
It's a measure of how much
of the conversation is attentive

00:11:14.482 --> 00:11:17.112
on the same topic versus disparate.

00:11:17.739 --> 00:11:20.522
The second one is called shared reality,

00:11:21.217 --> 00:11:23.476
and this is what percentage
of the conversation

00:11:23.500 --> 00:11:25.505
shares the same facts --

00:11:25.529 --> 00:11:28.642
not whether those facts
are truthful or not,

00:11:28.666 --> 00:11:31.675
but are we sharing
the same facts as we converse?

00:11:32.235 --> 00:11:34.588
The third is receptivity:

00:11:34.612 --> 00:11:38.571
How much of the conversation
is receptive or civil

00:11:38.595 --> 00:11:41.539
or the inverse, toxic?

00:11:42.213 --> 00:11:45.435
And then the fourth
is variety of perspective.

00:11:45.459 --> 00:11:48.604
So, are we seeing filter bubbles
or echo chambers,

00:11:48.628 --> 00:11:51.685
or are we actually getting
a variety of opinions

00:11:51.709 --> 00:11:53.344
within the conversation?

00:11:53.368 --> 00:11:57.386
And implicit in all four of these
is the understanding that,

00:11:57.410 --> 00:12:00.800
as they increase, the conversation
gets healthier and healthier.

00:12:00.824 --> 00:12:05.693
So our first step is to see
if we can measure these online,

00:12:05.717 --> 00:12:07.025
which we believe we can.

00:12:07.049 --> 00:12:10.216
We have the most momentum
around receptivity.

00:12:10.240 --> 00:12:14.557
We have a toxicity score,
a toxicity model, on our system

00:12:14.581 --> 00:12:18.705
that can actually measure
whether you are likely to walk away

00:12:18.729 --> 00:12:21.042
from a conversation
that you're having on Twitter

00:12:21.066 --> 00:12:22.699
because you feel it's toxic,

00:12:22.723 --> 00:12:25.235
with some pretty high degree.

00:12:26.369 --> 00:12:28.568
We're working to measure the rest,

00:12:28.592 --> 00:12:30.556
and the next step is,

00:12:30.580 --> 00:12:33.939
as we build up solutions,

00:12:33.963 --> 00:12:37.454
to watch how these measurements
trend over time

00:12:37.478 --> 00:12:39.351
and continue to experiment.

00:12:39.375 --> 00:12:43.416
And our goal is to make sure
that these are balanced,

00:12:43.440 --> 00:12:46.506
because if you increase one,
you might decrease another.

00:12:46.530 --> 00:12:48.677
If you increase variety of perspective,

00:12:48.701 --> 00:12:51.792
you might actually decrease
shared reality.

00:12:51.816 --> 00:12:56.805
CA: Just picking up on some
of the questions flooding in here.

00:12:56.829 --> 00:12:58.100
JD: Constant questioning.

00:12:58.996 --> 00:13:02.616
CA: A lot of people are puzzled why,

00:13:02.640 --> 00:13:06.887
like, how hard is it to get rid
of Nazis from Twitter?

00:13:08.309 --> 00:13:09.631
JD: (Laughs)

00:13:09.655 --> 00:13:16.650
So we have policies
around violent extremist groups,

00:13:16.674 --> 00:13:21.100
and the majority of our work
and our terms of service

00:13:21.124 --> 00:13:24.853
works on conduct, not content.

00:13:24.877 --> 00:13:27.428
So we're actually looking for conduct.

00:13:27.452 --> 00:13:30.466
Conduct being using the service

00:13:30.490 --> 00:13:34.357
to repeatedly or episodically
harass someone,

00:13:34.381 --> 00:13:36.874
using hateful imagery

00:13:36.898 --> 00:13:39.004
that might be associated with the KKK

00:13:39.028 --> 00:13:42.309
or the American Nazi Party.

00:13:42.333 --> 00:13:46.489
Those are all things
that we act on immediately.

00:13:47.002 --> 00:13:52.454
We're in a situation right now
where that term is used fairly loosely,

00:13:52.478 --> 00:13:57.791
and we just cannot take
any one mention of that word

00:13:57.815 --> 00:13:59.932
accusing someone else

00:13:59.956 --> 00:14:03.711
as a factual indication that they
should be removed from the platform.

00:14:03.735 --> 00:14:06.362
So a lot of our models
are based around, number one:

00:14:06.386 --> 00:14:09.526
Is this account associated
with a violent extremist group?

00:14:09.550 --> 00:14:11.533
And if so, we can take action.

00:14:11.557 --> 00:14:15.409
And we have done so on the KKK
and the American Nazi Party and others.

00:14:15.433 --> 00:14:19.616
And number two: Are they using
imagery or conduct

00:14:19.640 --> 00:14:22.012
that would associate them as such as well?

00:14:22.416 --> 00:14:25.348
CA: How many people do you have
working on content moderation

00:14:25.372 --> 00:14:26.622
to look at this?

00:14:26.646 --> 00:14:28.142
JD: It varies.

00:14:28.166 --> 00:14:29.761
We want to be flexible on this,

00:14:29.785 --> 00:14:32.431
because we want to make sure
that we're, number one,

00:14:32.455 --> 00:14:36.879
building algorithms instead of just
hiring massive amounts of people,

00:14:36.903 --> 00:14:39.727
because we need to make sure
that this is scalable,

00:14:39.751 --> 00:14:43.205
and there are no amount of people
that can actually scale this.

00:14:43.229 --> 00:14:49.858
So this is why we've done so much work
around proactive detection of abuse

00:14:49.882 --> 00:14:51.273
that humans can then review.

00:14:51.297 --> 00:14:54.158
We want to have a situation

00:14:54.182 --> 00:14:57.923
where algorithms are constantly
scouring every single tweet

00:14:57.947 --> 00:15:00.289
and bringing the most
interesting ones to the top

00:15:00.313 --> 00:15:04.215
so that humans can bring their judgment
to whether we should take action or not,

00:15:04.239 --> 00:15:05.763
based on our terms of service.

00:15:05.787 --> 00:15:08.590
WPR: But there's not an amount
of people that are scalable,

00:15:08.614 --> 00:15:12.111
but how many people do you currently have
monitoring these accounts,

00:15:12.135 --> 00:15:14.681
and how do you figure out what's enough?

00:15:14.705 --> 00:15:16.977
JD: They're completely flexible.

00:15:17.001 --> 00:15:19.942
Sometimes we associate folks with spam.

00:15:19.966 --> 00:15:23.811
Sometimes we associate folks
with abuse and harassment.

00:15:23.835 --> 00:15:26.897
We're going to make sure that
we have flexibility in our people

00:15:26.921 --> 00:15:29.271
so that we can direct them
at what is most needed.

00:15:29.295 --> 00:15:30.499
Sometimes, the elections.

00:15:30.523 --> 00:15:35.450
We've had a string of elections
in Mexico, one coming up in India,

00:15:35.474 --> 00:15:39.921
obviously, the election last year,
the midterm election,

00:15:39.945 --> 00:15:42.417
so we just want to be flexible
with our resources.

00:15:42.441 --> 00:15:44.570
So when people --

00:15:44.594 --> 00:15:50.983
just as an example, if you go
to our current terms of service

00:15:51.007 --> 00:15:52.648
and you bring the page up,

00:15:52.672 --> 00:15:56.354
and you're wondering about abuse
and harassment that you just received

00:15:56.378 --> 00:16:00.012
and whether it was against
our terms of service to report it,

00:16:00.036 --> 00:16:02.595
the first thing you see
when you open that page

00:16:02.619 --> 00:16:05.707
is around intellectual
property protection.

00:16:06.504 --> 00:16:11.827
You scroll down and you get to
abuse, harassment

00:16:11.851 --> 00:16:14.233
and everything else
that you might be experiencing.

00:16:14.257 --> 00:16:17.452
So I don't know how that happened
over the company's history,

00:16:17.476 --> 00:16:22.273
but we put that above
the thing that people want

00:16:24.146 --> 00:16:27.368
the most information on
and to actually act on.

00:16:27.392 --> 00:16:32.633
And just our ordering shows the world
what we believed was important.

00:16:32.657 --> 00:16:35.538
So we're changing all that.

00:16:35.562 --> 00:16:37.125
We're ordering it the right way,

00:16:37.149 --> 00:16:40.600
but we're also simplifying the rules
so that they're human-readable

00:16:40.624 --> 00:16:44.691
so that people can actually
understand themselves

00:16:44.715 --> 00:16:48.163
when something is against our terms
and when something is not.

00:16:48.187 --> 00:16:50.348
And then we're making --

00:16:50.372 --> 00:16:55.572
again, our big focus is on removing
the burden of work from the victims.

00:16:55.596 --> 00:16:59.330
So that means push more
towards technology,

00:16:59.354 --> 00:17:01.227
rather than humans doing the work --

00:17:01.251 --> 00:17:03.664
that means the humans receiving the abuse

00:17:03.688 --> 00:17:06.714
and also the humans
having to review that work.

00:17:06.738 --> 00:17:08.411
So we want to make sure

00:17:08.435 --> 00:17:11.276
that we're not just encouraging more work

00:17:11.300 --> 00:17:13.929
around something
that's super, super negative,

00:17:13.953 --> 00:17:16.627
and we want to have a good balance
between the technology

00:17:16.651 --> 00:17:19.503
and where humans can actually be creative,

00:17:19.527 --> 00:17:22.617
which is the judgment of the rules,

00:17:22.641 --> 00:17:25.908
and not just all the mechanical stuff
of finding and reporting them.

00:17:25.932 --> 00:17:27.462
So that's how we think about it.

00:17:27.486 --> 00:17:29.892
CA: I'm curious to dig in more
about what you said.

00:17:29.916 --> 00:17:32.521
I mean, I love that you said
you are looking for ways

00:17:32.545 --> 00:17:36.007
to re-tweak the fundamental
design of the system

00:17:36.031 --> 00:17:40.906
to discourage some of the reactive
behavior, and perhaps --

00:17:40.930 --> 00:17:43.635
to use Tristan Harris-type language --

00:17:43.659 --> 00:17:47.947
engage people's more reflective thinking.

00:17:47.971 --> 00:17:49.825
How far advanced is that?

00:17:49.849 --> 00:17:54.154
What would alternatives
to that "like" button be?

00:17:55.518 --> 00:17:59.093
JD: Well, first and foremost,

00:17:59.117 --> 00:18:04.870
my personal goal with the service
is that I believe fundamentally

00:18:04.894 --> 00:18:07.596
that public conversation is critical.

00:18:07.620 --> 00:18:10.267
There are existential problems
facing the world

00:18:10.291 --> 00:18:14.454
that are facing the entire world,
not any one particular nation-state,

00:18:14.478 --> 00:18:17.127
that global public conversation benefits.

00:18:17.151 --> 00:18:19.523
And that is one of the unique
dynamics of Twitter,

00:18:19.547 --> 00:18:21.361
that it is completely open,

00:18:21.385 --> 00:18:22.981
it is completely public,

00:18:23.005 --> 00:18:24.404
it is completely fluid,

00:18:24.428 --> 00:18:28.466
and anyone can see any other conversation
and participate in it.

00:18:28.490 --> 00:18:30.696
So there are conversations
like climate change.

00:18:30.720 --> 00:18:33.402
There are conversations
like the displacement in the work

00:18:33.426 --> 00:18:35.426
through artificial intelligence.

00:18:35.450 --> 00:18:38.456
There are conversations
like economic disparity.

00:18:38.480 --> 00:18:41.245
No matter what any one nation-state does,

00:18:41.269 --> 00:18:43.690
they will not be able
to solve the problem alone.

00:18:43.714 --> 00:18:46.357
It takes coordination around the world,

00:18:46.381 --> 00:18:49.428
and that's where I think
Twitter can play a part.

00:18:49.452 --> 00:18:55.094
The second thing is that Twitter,
right now, when you go to it,

00:18:55.118 --> 00:18:58.864
you don't necessarily walk away
feeling like you learned something.

00:18:58.888 --> 00:19:00.164
Some people do.

00:19:00.188 --> 00:19:03.295
Some people have
a very, very rich network,

00:19:03.319 --> 00:19:06.436
a very rich community
that they learn from every single day.

00:19:06.460 --> 00:19:10.151
But it takes a lot of work
and a lot of time to build up to that.

00:19:10.175 --> 00:19:13.623
So we want to get people
to those topics and those interests

00:19:13.647 --> 00:19:15.226
much, much faster

00:19:15.250 --> 00:19:17.816
and make sure that
they're finding something that,

00:19:18.728 --> 00:19:21.088
no matter how much time
they spend on Twitter --

00:19:21.112 --> 00:19:23.470
and I don't want to maximize
the time on Twitter,

00:19:23.494 --> 00:19:26.404
I want to maximize
what they actually take away from it

00:19:26.428 --> 00:19:28.458
and what they learn from it, and --

00:19:29.598 --> 00:19:30.926
CA: Well, do you, though?

00:19:30.950 --> 00:19:34.194
Because that's the core question
that a lot of people want to know.

00:19:34.218 --> 00:19:37.856
Surely, Jack, you're constrained,
to a huge extent,

00:19:37.880 --> 00:19:39.887
by the fact that you're a public company,

00:19:39.911 --> 00:19:41.685
you've got investors pressing on you,

00:19:41.709 --> 00:19:45.268
the number one way you make your money
is from advertising --

00:19:45.292 --> 00:19:48.064
that depends on user engagement.

00:19:48.088 --> 00:19:52.788
Are you willing to sacrifice
user time, if need be,

00:19:52.812 --> 00:19:56.541
to go for a more reflective conversation?

00:19:56.565 --> 00:19:59.676
JD: Yeah; more relevance means
less time on the service,

00:19:59.700 --> 00:20:01.637
and that's perfectly fine,

00:20:01.661 --> 00:20:04.760
because we want to make sure
that, like, you're coming to Twitter,

00:20:04.784 --> 00:20:09.304
and you see something immediately
that you learn from and that you push.

00:20:09.328 --> 00:20:12.748
We can still serve an ad against that.

00:20:12.772 --> 00:20:15.693
That doesn't mean you need to spend
any more time to see more.

00:20:15.717 --> 00:20:17.450
The second thing we're looking at --

00:20:17.474 --> 00:20:20.172
CA: But just -- on that goal,
daily active usage,

00:20:20.196 --> 00:20:23.441
if you're measuring that,
that doesn't necessarily mean things

00:20:23.465 --> 00:20:25.203
that people value every day.

00:20:25.227 --> 00:20:26.388
It may well mean

00:20:26.412 --> 00:20:29.718
things that people are drawn to
like a moth to the flame, every day.

00:20:29.742 --> 00:20:32.764
We are addicted, because we see
something that pisses us off,

00:20:32.788 --> 00:20:35.966
so we go in and add fuel to the fire,

00:20:35.990 --> 00:20:37.917
and the daily active usage goes up,

00:20:37.941 --> 00:20:39.656
and there's more ad revenue there,

00:20:39.680 --> 00:20:42.432
but we all get angrier with each other.

00:20:42.456 --> 00:20:44.965
How do you define ...

00:20:44.989 --> 00:20:49.115
"Daily active usage" seems like a really
dangerous term to be optimizing.

00:20:49.139 --> 00:20:54.196
(Applause)

00:20:54.220 --> 00:20:55.488
JD: Taken alone, it is,

00:20:55.512 --> 00:20:57.858
but you didn't let me
finish the other metric,

00:20:57.882 --> 00:21:01.609
which is, we're watching for conversations

00:21:01.633 --> 00:21:03.762
and conversation chains.

00:21:03.786 --> 00:21:08.862
So we want to incentivize
healthy contribution back to the network,

00:21:08.886 --> 00:21:13.067
and what we believe that is
is actually participating in conversation

00:21:13.091 --> 00:21:14.288
that is healthy,

00:21:14.312 --> 00:21:19.349
as defined by those four indicators
I articulated earlier.

00:21:19.373 --> 00:21:22.030
So you can't just optimize
around one metric.

00:21:22.054 --> 00:21:24.806
You have to balance and look constantly

00:21:24.830 --> 00:21:28.913
at what is actually going to create
a healthy contribution to the network

00:21:28.937 --> 00:21:31.278
and a healthy experience for people.

00:21:31.302 --> 00:21:33.168
Ultimately, we want to get to a metric

00:21:33.192 --> 00:21:36.949
where people can tell us,
"Hey, I learned something from Twitter,

00:21:36.973 --> 00:21:39.140
and I'm walking away
with something valuable."

00:21:39.164 --> 00:21:41.207
That is our goal ultimately over time,

00:21:41.231 --> 00:21:43.040
but that's going to take some time.

00:21:43.064 --> 00:21:48.346
CA: You come over to many,
I think to me, as this enigma.

00:21:48.370 --> 00:21:52.766
This is possibly unfair,
but I woke up the other night

00:21:52.790 --> 00:21:56.669
with this picture of how I found I was
thinking about you and the situation,

00:21:56.693 --> 00:22:03.596
that we're on this great voyage with you
on this ship called the "Twittanic" --

00:22:03.620 --> 00:22:04.901
(Laughter)

00:22:04.925 --> 00:22:09.282
and there are people on board in steerage

00:22:09.306 --> 00:22:11.509
who are expressing discomfort,

00:22:11.533 --> 00:22:14.076
and you, unlike many other captains,

00:22:14.100 --> 00:22:17.531
are saying, "Well, tell me, talk to me,
listen to me, I want to hear."

00:22:17.555 --> 00:22:21.174
And they talk to you, and they say,
"We're worried about the iceberg ahead."

00:22:21.198 --> 00:22:23.440
And you go, "You know,
that is a powerful point,

00:22:23.464 --> 00:22:25.894
and our ship, frankly,
hasn't been built properly

00:22:25.918 --> 00:22:27.587
for steering as well as it might."

00:22:27.611 --> 00:22:29.269
And we say, "Please do something."

00:22:29.293 --> 00:22:30.704
And you go to the bridge,

00:22:30.728 --> 00:22:33.023
and we're waiting,

00:22:33.047 --> 00:22:37.595
and we look, and then you're showing
this extraordinary calm,

00:22:37.619 --> 00:22:41.502
but we're all standing outside,
saying, "Jack, turn the fucking wheel!"

00:22:41.526 --> 00:22:42.677
You know?

00:22:42.701 --> 00:22:44.036
(Laughter)

00:22:44.060 --> 00:22:46.441
(Applause)

00:22:46.465 --> 00:22:47.631
I mean --

00:22:47.655 --> 00:22:49.389
(Applause)

00:22:49.413 --> 00:22:54.007
It's democracy at stake.

00:22:54.031 --> 00:22:56.852
It's our culture at stake.
It's our world at stake.

00:22:56.876 --> 00:23:01.582
And Twitter is amazing and shapes so much.

00:23:01.606 --> 00:23:03.839
It's not as big as some
of the other platforms,

00:23:03.863 --> 00:23:06.667
but the people of influence use it
to set the agenda,

00:23:06.691 --> 00:23:13.478
and it's just hard to imagine a more
important role in the world than to ...

00:23:13.502 --> 00:23:17.286
I mean, you're doing a brilliant job
of listening, Jack, and hearing people,

00:23:17.310 --> 00:23:21.755
but to actually dial up the urgency
and move on this stuff --

00:23:21.779 --> 00:23:23.980
will you do that?

00:23:24.750 --> 00:23:28.565
JD: Yes, and we have been
moving substantially.

00:23:28.589 --> 00:23:31.814
I mean, there's been
a few dynamics in Twitter's history.

00:23:31.838 --> 00:23:33.921
One, when I came back to the company,

00:23:35.477 --> 00:23:41.733
we were in a pretty dire state
in terms of our future,

00:23:41.757 --> 00:23:46.391
and not just from how people
were using the platform,

00:23:46.415 --> 00:23:48.462
but from a corporate narrative as well.

00:23:48.486 --> 00:23:51.690
So we had to fix
a bunch of the foundation,

00:23:51.714 --> 00:23:53.683
turn the company around,

00:23:53.707 --> 00:23:56.818
go through two crazy layoffs,

00:23:56.842 --> 00:24:00.635
because we just got too big
for what we were doing,

00:24:00.659 --> 00:24:02.719
and we focused all of our energy

00:24:02.743 --> 00:24:06.251
on this concept of serving
the public conversation.

00:24:06.275 --> 00:24:07.726
And that took some work.

00:24:07.750 --> 00:24:10.358
And as we dived into that,

00:24:10.382 --> 00:24:13.374
we realized some of the issues
with the fundamentals.

00:24:14.120 --> 00:24:18.776
We could do a bunch of superficial things
to address what you're talking about,

00:24:18.800 --> 00:24:20.590
but we need the changes to last,

00:24:20.614 --> 00:24:23.073
and that means going really, really deep

00:24:23.097 --> 00:24:27.447
and paying attention
to what we started 13 years ago

00:24:27.471 --> 00:24:29.732
and really questioning

00:24:29.756 --> 00:24:32.322
how the system works
and how the framework works

00:24:32.346 --> 00:24:36.179
and what is needed for the world today,

00:24:36.203 --> 00:24:40.227
given how quickly everything is moving
and how people are using it.

00:24:40.251 --> 00:24:46.795
So we are working as quickly as we can,
but quickness will not get the job done.

00:24:46.819 --> 00:24:49.430
It's focus, it's prioritization,

00:24:49.454 --> 00:24:52.400
it's understanding
the fundamentals of the network

00:24:52.424 --> 00:24:55.266
and building a framework that scales

00:24:55.290 --> 00:24:57.641
and that is resilient to change,

00:24:57.665 --> 00:25:03.094
and being open about where we are
and being transparent about where are

00:25:03.118 --> 00:25:05.297
so that we can continue to earn trust.

00:25:06.141 --> 00:25:09.472
So I'm proud of all the frameworks
that we've put in place.

00:25:09.496 --> 00:25:12.384
I'm proud of our direction.

00:25:12.915 --> 00:25:15.633
We obviously can move faster,

00:25:15.657 --> 00:25:20.376
but that required just stopping a bunch
of stupid stuff we were doing in the past.

00:25:21.067 --> 00:25:22.231
CA: All right.

00:25:22.255 --> 00:25:26.322
Well, I suspect there are many people here
who, if given the chance,

00:25:26.346 --> 00:25:30.335
would love to help you
on this change-making agenda you're on,

00:25:30.359 --> 00:25:31.901
and I don't know if Whitney --

00:25:31.925 --> 00:25:34.686
Jack, thank you for coming here
and speaking so openly.

00:25:34.710 --> 00:25:36.237
It took courage.

00:25:36.261 --> 00:25:39.645
I really appreciate what you said,
and good luck with your mission.

00:25:39.669 --> 00:25:41.764
JD: Thank you so much.
Thanks for having me.

00:25:41.788 --> 00:25:45.110
(Applause)

00:25:45.134 --> 00:25:46.293
Thank you.

